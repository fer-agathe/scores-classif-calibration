[
  {
    "objectID": "book_ojeda_gamsel.html",
    "href": "book_ojeda_gamsel.html",
    "title": "11  Generalized Additive Model Selection",
    "section": "",
    "text": "11.1 Data\nWe generate data using the first 12 scenarios from Ojeda et al. (2023) and an additional set of 4 scenarios in which the true probability does not depend on the predictors in a linear way (see Chapter 4).\nsource(\"functions/data-ojeda.R\")\nlibrary(ks)\nsource(\"functions/subsample_target_distribution.R\")\nWhen we simulate a dataset, we draw the following number of observations:\nnb_obs &lt;- 10000\nDefinition of the 16 scenarios\n# Coefficients beta\ncoefficients &lt;- list(\n  # First category (baseline, 2 covariates)\n  c(0.5, 1),  # scenario 1, 0 noise variable\n  c(0.5, 1),  # scenario 2, 10 noise variables\n  c(0.5, 1),  # scenario 3, 50 noise variables\n  c(0.5, 1),  # scenario 4, 100 noise variables\n  # Second category (same as baseline, with lower number of 1s)\n  c(0.5, 1),  # scenario 5, 0 noise variable\n  c(0.5, 1),  # scenario 6, 10 noise variables\n  c(0.5, 1),  # scenario 7, 50 noise variables\n  c(0.5, 1),  # scenario 8, 100 noise variables\n  # Third category (same as baseline but with 5 num. and 5 categ. covariates)\n  c(0.1, 0.2, 0.3, 0.4, 0.5, 0.01, 0.02, 0.03, 0.04, 0.05),\n  c(0.1, 0.2, 0.3, 0.4, 0.5, 0.01, 0.02, 0.03, 0.04, 0.05),\n  c(0.1, 0.2, 0.3, 0.4, 0.5, 0.01, 0.02, 0.03, 0.04, 0.05),\n  c(0.1, 0.2, 0.3, 0.4, 0.5, 0.01, 0.02, 0.03, 0.04, 0.05),\n  # Fourth category (nonlinear predictor, 3 covariates)\n  c(0.5, 1, .3),  # scenario 5, 0 noise variable\n  c(0.5, 1, .3),  # scenario 6, 10 noise variables\n  c(0.5, 1, .3),  # scenario 7, 50 noise variables\n  c(0.5, 1, .3)  # scenario 8, 100 noise variables\n)\n\n# Mean parameter for the normal distribution to draw from to draw num covariates\nmean_num &lt;- list(\n  # First category (baseline, 2 covariates)\n  rep(0, 2),  # scenario 1, 0 noise variable\n  rep(0, 2),  # scenario 2, 10 noise variables\n  rep(0, 2),  # scenario 3, 50 noise variables\n  rep(0, 2),  # scenario 4, 100 noise variables\n  # Second category (same as baseline, with lower number of 1s)\n  rep(0, 2),  # scenario 5, 0 noise variable\n  rep(0, 2),  # scenario 6, 10 noise variables\n  rep(0, 2),  # scenario 7, 50 noise variables\n  rep(0, 2),  # scenario 8, 100 noise variables\n  # Third category (same as baseline but with 5 num. and 5 categ. covariates)\n  rep(0, 5),\n  rep(0, 5),\n  rep(0, 5),\n  rep(0, 5),\n  # Fourth category (nonlinear predictor, 3 covariates)\n  rep(0, 3),\n  rep(0, 3),\n  rep(0, 3),\n  rep(0, 3)\n)\n# Sd parameter for the normal distribution to draw from to draw num covariates\nsd_num &lt;- list(\n  # First category (baseline, 2 covariates)\n  rep(1, 2),  # scenario 1, 0 noise variable\n  rep(1, 2),  # scenario 2, 10 noise variables\n  rep(1, 2),  # scenario 3, 50 noise variables\n  rep(1, 2),  # scenario 4, 100 noise variables\n  # Second category (same as baseline, with lower number of 1s)\n  rep(1, 2),  # scenario 5, 0 noise variable\n  rep(1, 2),  # scenario 6, 10 noise variables\n  rep(1, 2),  # scenario 7, 50 noise variables\n  rep(1, 2),  # scenario 8, 100 noise variables\n  # Third category (same as baseline but with 5 num. and 5 categ. covariates)\n  rep(1, 5),\n  rep(1, 5),\n  rep(1, 5),\n  rep(1, 5),\n  # Fourth category (nonlinear predictor, 3 covariates)\n  rep(1, 3),\n  rep(1, 3),\n  rep(1, 3),\n  rep(1, 3)\n)\n\nparams_df &lt;- tibble(\n  scenario = 1:16,\n  coefficients = coefficients,\n  n_num = c(rep(2, 8), rep(5, 4), rep(3, 4)),\n  add_categ = c(rep(FALSE, 8), rep(TRUE, 4), rep(FALSE, 4)),\n  n_noise = rep(c(0, 10, 50, 100), 4),\n  mean_num = mean_num,\n  sd_num = sd_num,\n  size_train = rep(nb_obs, 16),\n  size_test = rep(nb_obs, 16),\n  transform_probs = c(rep(FALSE, 4), rep(TRUE, 4), rep(FALSE, 4), rep(FALSE, 4)),\n  linear_predictor = c(rep(TRUE, 12), rep(FALSE, 4)),\n  seed = 202105\n)\nrm(coefficients, mean_num, sd_num)",
    "crumbs": [
      "Simulated Data",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Generalized Additive Model Selection</span>"
    ]
  },
  {
    "objectID": "book_ojeda_gamsel.html#metrics",
    "href": "book_ojeda_gamsel.html#metrics",
    "title": "11  Generalized Additive Model Selection",
    "section": "11.2 Metrics",
    "text": "11.2 Metrics\nWe load the functions from Chapter 3 to compute performance, calibration and divergence metrics.\n\nsource(\"functions/metrics.R\")",
    "crumbs": [
      "Simulated Data",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Generalized Additive Model Selection</span>"
    ]
  },
  {
    "objectID": "book_ojeda_gamsel.html#simulations-setup",
    "href": "book_ojeda_gamsel.html#simulations-setup",
    "title": "11  Generalized Additive Model Selection",
    "section": "11.3 Simulations Setup",
    "text": "11.3 Simulations Setup\nAs in previous chapters, we define a function to run replications of the simulations for each scenario. This function is called simul_gamsel(). It uses multiple helper functions also defined here.\n\n\nHelper Functions\n#' Counts the number of scores in each of the 20 equal-sized bins over [0,1]\n#'\n#' @param scores_train vector of scores on the train test\n#' @param scores_test vector of scores on the test test\nget_histogram &lt;- function(scores_train, scores_test) {\n  breaks &lt;- seq(0, 1, by = .05)\n  scores_train_hist &lt;- hist(scores_train, breaks = breaks, plot = FALSE)\n  scores_test_hist &lt;- hist(scores_test, breaks = breaks, plot = FALSE)\n  scores_hist &lt;- list(\n    train = scores_train_hist,\n    test = scores_test_hist\n  )\n  scores_hist\n}\n\n#' Get KL divergence metrics for estimated scores and true probabilities\n#' \n#' @param scores_train vector of scores on the train test\n#' @param scores_test vector of scores on the test test\n#' @param true_prob list of true probabilities on train and test set\nget_disp_metrics &lt;- function(scores_train, scores_test, true_prob) {\n  disp_train &lt;- dispersion_metrics(\n    true_probas = true_prob$train, scores = scores_train\n  ) |&gt; mutate(sample = \"train\")\n  disp_test &lt;- dispersion_metrics(\n    true_probas = true_prob$test, scores = scores_test\n  ) |&gt; mutate(sample = \"test\")\n  \n  tb_disp_metrics &lt;- disp_train |&gt;\n    bind_rows(disp_test)\n  tb_disp_metrics\n}\n\n#' Get the performance and calibration metrics for estimated scores\n#' \n#' @param scores_train vector of scores on the train test\n#' @param scores_test vector of scores on the test test\n#' @param tb_train train set\n#' @param tb_test test set\n#' @param true_prob list of true probabilities on train and test set\nget_perf_metrics &lt;- function(scores_train, \n                             scores_test,\n                             tb_train,\n                             tb_test,\n                             true_prob) {\n  # We add very small noise to predicted scores\n  # otherwise the local regression may crash\n  scores_train_noise &lt;- scores_train +\n    runif(n = length(scores_train), min = 0, max = 0.01)\n  scores_train_noise[scores_train_noise &gt; 1] &lt;- 1\n  metrics_train &lt;- compute_metrics(\n    obs = tb_train$d, scores = scores_train_noise, true_probas = true_prob$train\n  ) |&gt; mutate(sample = \"train\")\n  \n  scores_test_noise &lt;- scores_test +\n    runif(n = length(scores_test), min = 0, max = 0.01)\n  scores_test_noise[scores_test_noise &gt; 1] &lt;- 1\n  metrics_test &lt;- compute_metrics(\n    obs = tb_test$d, scores = scores_test_noise, true_probas = true_prob$test\n  ) |&gt; mutate(sample = \"test\")\n  \n  tb_metrics &lt;- metrics_train |&gt;\n    bind_rows(metrics_test)\n  tb_metrics\n}\n\n#' Estimation of P(q1 &lt; score &lt; q2)\n#' \n#' @param scores_train vector of scores on the train test\n#' @param scores_test vector of scores on the test test\n#' @param q1 vector of desired values for q1 (q2 = 1-q1)\nestim_prop &lt;- function(scores_train, \n                       scores_test, \n                       q1 = c(.1, .2, .3, .4)) {\n  proq_scores_train &lt;- map(\n    q1,\n    ~prop_btw_quantiles(s = scores_train, q1 = .x)\n  ) |&gt;\n    list_rbind() |&gt;\n    mutate(sample = \"train\")\n  proq_scores_test &lt;- map(\n    q1,\n    ~prop_btw_quantiles(s = scores_test, q1 = .x)\n  ) |&gt;\n    list_rbind() |&gt;\n    mutate(sample = \"test\")\n  \n  proq_scores_train |&gt; \n    bind_rows(proq_scores_test)\n}\n\n\n\n\nThe simul_gamsel() function\n#' Run a single replication of the simulations of a scenario\n#' Fits a GAMSEL model to the data.\n#' \n#' @param scenario ID of the scenario\n#' @param params_df tibble with the parameters used to generate data\n#' @param repn replication ID number\nsimul_gamsel &lt;- function(scenario, params_df, repn) {\n  # Generate Data----\n  simu_data &lt;- simulate_data_wrapper(\n    scenario = scenario,\n    params_df = params_df,\n    repn = repn\n  )\n\n  tb_train &lt;- simu_data$data$train |&gt; rename(d = y)\n  tb_test &lt;- simu_data$data$test |&gt; rename(d = y)\n\n  true_prob &lt;-\n    list(\n      train = simu_data$data$probs_train,\n      test = simu_data$data$probs_test\n    )\n\n  # Format Data----\n\n  if (scenario %in% 9:12) {\n    # Factor variables\n    tb_train &lt;- tb_train |&gt; mutate(across(x6:x10, as.factor))\n    tb_test &lt;- tb_test |&gt; mutate(across(x6:x10, as.factor))\n  }\n\n  X_train &lt;- tb_train |&gt; select(-d)\n  X_test &lt;- tb_test |&gt; select(-d)\n  y_train &lt;- tb_train |&gt; pull(d)\n  ## One hot encoding\n  dmy_train &lt;- dummyVars(\n    \" ~ -1+.\", data = X_train, fullRank = TRUE, contrasts = TRUE, sep = \".\"\n  )\n  X_dmy_train &lt;- data.frame(predict(dmy_train, newdata = X_train))\n  # using same encoder for test set\n  X_dmy_test &lt;- data.frame(predict(dmy_train, newdata = X_test))\n  categ_names &lt;- map2(\n    .x = dmy_train$facVars, .y = dmy_train$lvls, .f = ~str_c(.x, \".\", .y)\n  ) |&gt;\n    unlist()\n\n  # Estimation----\n  ## Degrees\n  deg &lt;- ifelse(!colnames(X_dmy_train) %in% categ_names, 5, 1)\n  gamsel_cv &lt;- cv.gamsel(\n    x = X_dmy_train, y = y_train, family=\"binomial\", degrees = deg\n  )\n  gamsel_out &lt;- gamsel(\n    x = X_dmy_train, y = y_train, family = \"binomial\", degrees = deg,\n    lambda = gamsel_cv$lambda.min\n  )\n\n  # Predicted scores----\n  scores_train &lt;- predict(\n    gamsel_out, newdata = X_dmy_train, type = \"response\")[, 1]\n  scores_test &lt;- predict(\n    gamsel_out, newdata = X_dmy_test, type = \"response\")[, 1]\n\n  # Histogram of scores----\n  scores_hist &lt;- get_histogram(scores_train, scores_test)\n\n  # Performance and Calibration Metrics----\n  tb_metrics &lt;- get_perf_metrics(\n    scores_train = scores_train,\n    scores_test = scores_test,\n    tb_train = tb_train,\n    tb_test = tb_test,\n    true_prob = true_prob) |&gt;\n    mutate(\n      scenario = simu_data$scenario,\n      repn = simu_data$repn\n    )\n\n  # Dispersion Metrics----\n  tb_disp_metrics &lt;- get_disp_metrics(\n    scores_train = scores_train,\n    scores_test = scores_test,\n    true_prob = true_prob\n  ) |&gt;\n    mutate(\n      scenario = simu_data$scenario,\n      repn = simu_data$repn\n    )\n\n  metrics &lt;- suppressMessages(\n    left_join(tb_metrics, tb_disp_metrics)\n  )\n\n  # Estimation of P(q1 &lt; score &lt; q2)----\n  tb_prop_scores &lt;- estim_prop(scores_train, scores_test) |&gt;\n    mutate(\n      scenario = simu_data$scenario,\n      repn = simu_data$repn\n    )\n\n  list(\n    scenario = simu_data$scenario,   # data scenario\n    repn = simu_data$repn,           # data replication ID\n    metrics = metrics,            # table with performance/calib/divergence\n    tb_prop_scores = tb_prop_scores, # table with P(q1 &lt; score &lt; q2)\n    scores_hist = scores_hist       # histogram of scores\n  )\n}\n\n\nThe desired number of replications for each scenario needs to be set:\n\nrepns_vector &lt;- 1:100",
    "crumbs": [
      "Simulated Data",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Generalized Additive Model Selection</span>"
    ]
  },
  {
    "objectID": "book_ojeda_gamsel.html#estimations",
    "href": "book_ojeda_gamsel.html#estimations",
    "title": "11  Generalized Additive Model Selection",
    "section": "11.4 Estimations",
    "text": "11.4 Estimations\nWe loop over the 16 scenarios and run the 100 replications in parallel.\n\n\nEstimation codes\nlibrary(pbapply)\nlibrary(parallel)\nncl &lt;- detectCores()-1\n(cl &lt;- makeCluster(ncl))\n\nclusterEvalQ(cl, {\n  library(tidyverse)\n  library(ks)\n  library(gamsel)\n  library(philentropy)\n  library(caret)\n}) |&gt;\n  invisible()\n\nclusterExport(\n  cl, c(\n    # Functions\n    \"brier_score\",\n    \"compute_metrics\",\n    \"dispersion_metrics\",\n    \"prop_btw_quantiles\",\n    \"subset_target\",\n    \"simulate_data\",\n    \"simulate_data_wrapper\",\n    \"simul_gamsel\",\n    \"get_histogram\",\n    \"get_disp_metrics\",\n    \"get_perf_metrics\",\n    \"estim_prop\",\n    # Objects\n    \"grid\",\n    \"params_df\",\n    \"repns_vector\"\n    )\n  )\n\nfor (i_scenario in 1:16) {\n  scenario &lt;- i_scenario\n  print(str_c(\"Scenario \", scenario, \"/\", nrow(params_df)))\n  clusterExport(cl, c(\"scenario\"))\n  resul_gamsel_scenario &lt;-\n    pblapply(\n      1:length(repns_vector), function(i) simul_gamsel(\n        scenario = scenario, params_df = params_df, repn = i\n      ),\n      cl = cl\n    )\n  save(\n    resul_gamsel_scenario,\n    file = str_c(\"output/simul/dgp-ojeda/resul_gamsel_scenario_\", scenario, \".rda\")\n  )\n}\nstopCluster(cl)\n\n\nThe results can be loaded as follows:\n\nfiles &lt;- str_c(\n  \"output/simul/dgp-ojeda/resul_gamsel_scenario_\", 1:16, \".rda\"\n)\nresul_gamsel &lt;- map(files[file.exists(files)], ~{load(.x) ; resul_gamsel_scenario})\n\nThe resul_gamsel object is of length 16: each element contains the simulations for a scenario. For each scenario, the elements are a list of length max(repns_vector), i.e., the number of replications. Each replication gives, in a list, the following elements:\n\nscenario: the number of the scenario\nrepn: the replication number\nmetrics: the metrics (AUC, Calibration, KL Divergence , etc.) for each model from the grid search, for all boosting iterations.\ntb_prop_scores: the estimations of \\(\\mathbb{P}(q_1 &lt; \\hat{s}(\\mathbb{x})&lt; q_2)\\), for \\(q_1 =\\{ .1, .2, .3, .4\\}\\).\nscores_hist: the counts on bins defined on estimated scores (on both train and test sets).",
    "crumbs": [
      "Simulated Data",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Generalized Additive Model Selection</span>"
    ]
  },
  {
    "objectID": "book_ojeda_gamsel.html#results",
    "href": "book_ojeda_gamsel.html#results",
    "title": "11  Generalized Additive Model Selection",
    "section": "11.5 Results",
    "text": "11.5 Results\nWe can now extract some information from the results. Let us begin with the different metrics computed for each of the replications for each scenario.\n\nmetrics_gamsel_all &lt;- map(\n  resul_gamsel, \n  function(resul_gamsel_sc) map(resul_gamsel_sc, \"metrics\") |&gt; list_rbind()\n) |&gt; \n  list_rbind()\n\nWe can then show boxplots of the metrics for each scenario.\n\n\nCodes to create the boxplots\ndf_plot &lt;- metrics_gamsel_all |&gt; \n  select(scenario, sample, AUC, mse, ici, KL_20_true_probas) |&gt; \n  pivot_longer(cols = -c(scenario, sample), names_to = \"metric\") |&gt; \n  mutate(\n    scenario = factor(scenario),\n    sample = factor(\n      sample,\n      levels = c(\"train\", \"test\"),\n      labels = c(\"Train\", \"Test\")\n    ),\n    metric = factor(\n      metric,\n      levels = c(\"AUC\", \"mse\", \"ici\", \"KL_20_true_probas\"),\n      labels = c(\"AUC\", \"MSE\", \"ICI\", \"KL Divergence\")\n    )\n  )\n\nggplot(\n  data = df_plot,\n  mapping = aes(x = scenario, y = value, fill = sample)\n) +\n  geom_boxplot() +\n  facet_wrap(~metric, scales = \"free\") +\n  scale_fill_manual(\"Sample\", values = colour_samples) +\n  labs(x = \"Scenario\", y = NULL) +\n  theme_paper()\n\n\n\n\n\nFigure 11.1: Metrics for the GAMSEL model computed on 100 replications of the simulations for each scenario.\n\n\n\n\n\n\n\n\n\n11.5.1 Distribution of Scores\nThen, we can have a look at the distribution of scores on the train set and on the test set for each scenario.\n\nscores_hist_all &lt;- \n  map(\n    resul_gamsel,\n    function(resul_gamsel_sc) map(resul_gamsel_sc, \"scores_hist\")\n  )\n\nWe can focus on the first replication of each of the scenarios:\n\nrepn &lt;- 1\n\nWe define a function, plot_hist() to plot the distribution of scores also showing some metrics (AUC, ICI and KL divergence) for a particular replication of one scenario. We also define a second function, plot_hist_dgp() to plot the distributions of true probabilities and that of a replication, for multiple scenarios within a DGP.\n\n\nFunctions plot_hist() and plot_hist_dgp()\nplot_hist &lt;- function(metrics_interest, scores_hist_interest) {\n  subtitle &lt;- str_c(\n    \"AUC = \", round(metrics_interest$AUC, 2), \", \",\n    \"Brier = \", round(metrics_interest$ici, 2), \", \\n\",\n    \"ICI = \", round(metrics_interest$ici, 2), \", \",\n    \"KL = \", round(metrics_interest$KL_20_true_probas, 2)\n  )\n  plot(\n    # main = \"Test Set\",\n    main = \"\",\n    scores_hist_interest$test,\n    xlab = latex2exp::TeX(\"$\\\\hat{s}(x)$\"),\n    ylab = \"\"\n  )\n  mtext(side = 3, line = -0.25, adj = .5, subtitle, cex = .6)\n}\n\nplot_hist_dgp &lt;- function(repn) {\n  layout(\n    matrix(c(1:5, (1:20)+5), ncol = 5, byrow = TRUE), \n    heights = c(.3, rep(3, 4))\n  )\n  par(mar = c(0, 4.1, 0, 2.1))\n  col_titles &lt;- c(\"True Probabilities\", str_c(c(0, 10, 50, 100), \" noise variables\"))\n  for (i in 1:5) {\n    plot(c(0, 1), c(0, 1), ann = F, bty = 'n', type = 'n', xaxt = 'n', yaxt = 'n')\n    text(x = 0.5, y = 0.5, col_titles[i], cex = 1.6, col = \"black\")\n  }\n  \n  par(mar = c(4.1, 4.1, 1.6, 2.1))\n  for (dgp in 1:4) {\n    scenarios &lt;- (1:4) + 4*(dgp-1)\n    # True Probabilities\n    simu_data &lt;- simulate_data_wrapper(\n      scenario = scenarios[1],\n      params_df = params_df,\n      repn = repn # only one replication here\n    )\n    true_prob &lt;- simu_data$data$probs_train\n    hist(\n      true_prob,\n      breaks = seq(0, 1, by = .05),\n      xlab = \"p\", ylab = \"\",\n      main = \"\",\n      xlim = c(0, 1)\n    )\n    mtext(\n      side = 2, str_c(\"DGP \", dgp), line = 2.5, cex = 1, \n      font.lab = 2\n    )\n    \n    for (i_scenario in scenarios) {\n      metrics_interest &lt;- metrics_gamsel_all |&gt; \n        filter(scenario == !!i_scenario, repn == !!repn, sample == \"test\")\n      scores_hist_interest &lt;- scores_hist_all[[i_scenario]][[repn]]\n      plot_hist(\n        metrics_interest = metrics_interest,\n        scores_hist_interest = scores_hist_interest\n      )\n    }\n  }\n}\n\n\n\n\nCode\nplot_hist_dgp(repn = 1)\n\n\n\n\n\nFigure 11.2: Distribution of true probabilities and estimated scores on test set for the GAMSEL\n\n\n\n\n\n\n\n\n\n\n\n\nChouldechova, Alexandra, and Trevor Hastie. 2015. “Generalized Additive Model Selection.” https://arxiv.org/abs/1506.03850.\n\n\nOjeda, Francisco M., Max L. Jansen, Alexandre Thiéry, Stefan Blankenberg, Christian Weimar, Matthias Schmid, and Andreas Ziegler. 2023. “Calibrating Machine Learning Approaches for Probability Estimation: A Comprehensive Comparison.” Statistics in Medicine 42 (29): 5451–78. https://doi.org/10.1002/sim.9921.",
    "crumbs": [
      "Simulated Data",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Generalized Additive Model Selection</span>"
    ]
  },
  {
    "objectID": "book_ojeda_comparison.html",
    "href": "book_ojeda_comparison.html",
    "title": "12  Comparison of Models",
    "section": "",
    "text": "13 Load Previous Results",
    "crumbs": [
      "Simulated Data",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Comparison of Models</span>"
    ]
  },
  {
    "objectID": "book_ojeda_comparison.html#trees",
    "href": "book_ojeda_comparison.html#trees",
    "title": "12  Comparison of Models",
    "section": "13.1 Trees",
    "text": "13.1 Trees\nThe trees estimated in Chapter 5.\n\nfiles &lt;- str_c(\n  \"output/simul/dgp-ojeda/resul_trees_scenario_\", 1:16, \".rda\"\n)\nresul_trees &lt;- map(files[file.exists(files)], ~{load(.x) ; resul_trees_scenario})\n\nWe can merge the metrics tables computed for each scenario and replications for these scenarios into a single tibble.\n\nmetrics_trees_all &lt;- map(\n  resul_trees,\n  function(resul_trees_sc) map(resul_trees_sc, \"metrics_all\") |&gt; list_rbind()\n) |&gt;\n  list_rbind() |&gt;\n  mutate(\n    sample = factor(\n      sample,\n      levels = c(\"train\", \"valid\", \"test\"),\n      labels = c(\"Train\", \"Validation\", \"Test\")\n    )\n  )\n\nWe extract the metrics from the trees of interest:\n\nsmallest: tree with the smallest number of leaves\nlargest: tree with the highest number of leaves\nlargest_auc: tree with the highest AUC on validation set\nlowest_mse: tree with the lowest MSE on validation set\nlowest_brier: tree with the lowest Brier on validation set\nlowest_ici: tree with the lowest ICI on validation set\nlowest_kl: tree with the lowest KL Divergence on validation set\n\n\n\nCode to identify trees of interest\n# Identify the smallest tree\nsmallest_tree &lt;-\n  metrics_trees_all |&gt;\n  filter(sample == \"Validation\") |&gt;\n  group_by(scenario, repn) |&gt;\n  arrange(nb_leaves) |&gt;\n  slice_head(n = 1) |&gt;\n  select(scenario, repn, ind, nb_leaves) |&gt;\n  mutate(result_type = \"smallest\") |&gt;\n  ungroup()\n\n# Identify the largest tree\nlargest_tree &lt;-\n  metrics_trees_all |&gt;\n  filter(sample == \"Test\") |&gt;\n  group_by(scenario, repn) |&gt; \n  arrange(desc(nb_leaves)) |&gt;\n  slice_head(n = 1) |&gt;\n  select(scenario, repn, ind, nb_leaves) |&gt;\n  mutate(result_type = \"largest\") |&gt; \n  ungroup()\n\n# Identify tree with highest AUC on test set\nhighest_auc_tree &lt;-\n  metrics_trees_all |&gt;\n  filter(sample == \"Validation\") |&gt;\n  group_by(scenario, repn) |&gt;\n  arrange(desc(AUC)) |&gt;\n  slice_head(n = 1) |&gt;\n  select(scenario, repn, ind, nb_leaves) |&gt;\n  mutate(result_type = \"largest_auc\") |&gt;\n  ungroup()\n\n# Identify tree with lowest MSE\nlowest_mse_tree &lt;-\n  metrics_trees_all |&gt;\n  filter(sample == \"Validation\") |&gt;\n  group_by(scenario, repn) |&gt;\n  arrange(mse) |&gt;\n  slice_head(n = 1) |&gt;\n  select(scenario, repn, ind, nb_leaves) |&gt;\n  mutate(result_type = \"lowest_mse\") |&gt;\n  ungroup()\n\n# Identify tree with lowest ICI\nlowest_ici_tree &lt;-\n  metrics_trees_all |&gt;\n  filter(sample == \"Validation\") |&gt;\n  group_by(scenario, repn) |&gt;\n  arrange(ici) |&gt;\n  slice_head(n = 1) |&gt;\n  select(scenario, repn, ind, nb_leaves) |&gt;\n  mutate(result_type = \"lowest_ici\") |&gt;\n  ungroup()\n\n# Identify tree with lowest Brier's score\nlowest_brier_tree &lt;-\n  metrics_trees_all |&gt;\n  filter(sample == \"Validation\") |&gt;\n  group_by(scenario, repn) |&gt;\n  arrange(brier) |&gt;\n  slice_head(n = 1) |&gt;\n  select(scenario, repn, ind, nb_leaves) |&gt;\n  mutate(result_type = \"lowest_brier\") |&gt;\n  ungroup()\n\n# Identify tree with lowest KL\nlowest_kl_tree &lt;-\n  metrics_trees_all |&gt;\n  filter(sample == \"Validation\") |&gt;\n  group_by(scenario, repn) |&gt;\n  arrange(KL_20_true_probas) |&gt;\n  slice_head(n = 1) |&gt;\n  select(scenario, repn, ind, nb_leaves) |&gt;\n  mutate(result_type = \"lowest_kl\") |&gt;\n  ungroup()\n\n\n# Merge these\ntrees_of_interest_tree &lt;-\n  smallest_tree |&gt;\n  bind_rows(largest_tree) |&gt;\n  bind_rows(highest_auc_tree) |&gt;\n  bind_rows(lowest_mse_tree) |&gt;\n  bind_rows(lowest_ici_tree) |&gt;\n  bind_rows(lowest_brier_tree) |&gt;\n  bind_rows(lowest_kl_tree)\n\n# Add metrics now\ntrees_of_interest_metrics_tree &lt;-\n  trees_of_interest_tree |&gt;\n  left_join(\n    metrics_trees_all, \n    by = c(\"scenario\", \"repn\", \"ind\", \"nb_leaves\"),\n    relationship = \"many-to-many\" # (train, valid, test)\n  ) |&gt; \n  mutate(\n    result_type = factor(\n      result_type,\n      levels = c(\n        \"smallest\", \"largest\", \"lowest_mse\", \"largest_auc\",\n        \"lowest_brier\", \"lowest_ici\", \"lowest_kl\"),\n      labels = c(\n        \"Smallest\", \"Largest\", \"MSE*\", \"AUC*\", \n        \"Brier*\", \"ICI*\", \"KL*\"\n      )\n    )\n  )\n\n# Sanity check\n# trees_of_interest_metrics_tree |&gt; count(scenario, sample, result_type)\n\n\nWe ran 100 replications of the simulations for each scenario. Let us compute the average AUC, ICI and KL Divergence over these 100 replications, both on the train and on the validation set.\n\nmodels_interest_trees &lt;- \n  trees_of_interest_metrics_tree |&gt; \n  group_by(scenario, sample, result_type) |&gt; \n  summarise(\n    AUC_lower = quantile(AUC, probs = 2.5/100),\n    AUC_upper = quantile(AUC, probs = 97.5/100),\n    AUC_sd = sd(AUC),\n    AUC = mean(AUC),\n    brier_lower = quantile(brier, probs = 2.5/100),\n    brier_upper = quantile(brier, probs = 97.5/100),\n    brier_sd = sd(brier),\n    brier = mean(brier),\n    ici_lower = quantile(ici, probs = 2.5/100),\n    ici_upper = quantile(ici, probs = 97.5/100),\n    ici_sd = sd(ici),\n    ici = mean(ici),\n    KL_20_true_probas_lower = quantile(KL_20_true_probas, probs = 2.5/100),\n    KL_20_true_probas_upper = quantile(KL_20_true_probas, probs = 97.5/100),\n    KL_20_true_probas_sd = sd(KL_20_true_probas),\n    KL_20_true_probas = mean(KL_20_true_probas),\n    quant_ratio_sd = sd(inter_quantile_10_90),\n    quant_ratio = mean(inter_quantile_10_90),\n    .groups = \"drop\"\n  ) |&gt; \n  mutate(model = \"tree\")",
    "crumbs": [
      "Simulated Data",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Comparison of Models</span>"
    ]
  },
  {
    "objectID": "book_ojeda_comparison.html#random-forests",
    "href": "book_ojeda_comparison.html#random-forests",
    "title": "12  Comparison of Models",
    "section": "13.2 Random Forests",
    "text": "13.2 Random Forests\nWe load the estimated random forests from Chapter 6.\n\nfiles &lt;- str_c(\n  \"output/simul/dgp-ojeda/resul_rf_scenario_\", 1:16, \".rda\"\n)\nresul_rf &lt;- map(files[file.exists(files)], ~{load(.x) ; resul_rf_scenario})\n\nLet us merge in a single tibble the metrics computed over the replications of the scenarios.\n\nmetrics_rf_all &lt;- map(\n  resul_rf,\n  function(resul_rf_sc) map(resul_rf_sc, \"metrics_all\") |&gt; list_rbind()\n) |&gt;\n  list_rbind() |&gt;\n  mutate(\n    sample = factor(\n      sample,\n      levels = c(\"train\", \"test\", \"valid\"),\n      labels = c(\"Train\", \"Test\", \"Validation\")\n    )\n  )\n\nWe extract the metrics from the trees of interest:\n\nsmallest: forest with the smallest average number of leaves in the trees\nlargest: forest with the highest average number of leaves in the trees\nlargest_auc: forest with the highest AUC on validation set\nlowest_mse: forest with the lowest MSE on validation set\nlowest_ici: forest with the lowest ICI on validation set\nlowest_brier: forest with the lowest Brier score on validation set\nlowest_kl: forest with the lowest KL Divergence on validation set\n\n\n\nCode\n# Identify the model with the smallest number of leaves on average on\n# validation set\nsmallest_rf &lt;-\n  metrics_rf_all |&gt;\n  filter(sample == \"Validation\") |&gt;\n  group_by(scenario, repn) |&gt;\n  arrange(nb_leaves) |&gt;\n  slice_head(n = 1) |&gt;\n  select(scenario, repn, ind, nb_leaves) |&gt;\n  mutate(result_type = \"smallest\") |&gt;\n  ungroup()\n\n# Identify the largest tree\nlargest_rf &lt;-\n  metrics_rf_all |&gt;\n  filter(sample == \"Validation\") |&gt;\n  group_by(scenario, repn) |&gt;\n  arrange(desc(nb_leaves)) |&gt;\n  slice_head(n = 1) |&gt;\n  select(scenario, repn, ind, nb_leaves) |&gt;\n  mutate(result_type = \"largest\") |&gt;\n  ungroup()\n\n# Identify tree with highest AUC on test set\nhighest_auc_rf &lt;-\n  metrics_rf_all |&gt;\n  filter(sample == \"Validation\") |&gt;\n  group_by(scenario, repn) |&gt;\n  arrange(desc(AUC)) |&gt;\n  slice_head(n = 1) |&gt;\n  select(scenario, repn, ind, nb_leaves) |&gt;\n  mutate(result_type = \"largest_auc\") |&gt;\n  ungroup()\n\n# Identify tree with lowest MSE\nlowest_mse_rf &lt;-\n  metrics_rf_all |&gt;\n  filter(sample == \"Validation\") |&gt;\n  group_by(scenario, repn) |&gt;\n  arrange(mse) |&gt;\n  slice_head(n = 1) |&gt;\n  select(scenario, repn, ind, nb_leaves) |&gt;\n  mutate(result_type = \"lowest_mse\") |&gt;\n  ungroup()\n\n# Identify tree with lowest Brier\nlowest_brier_rf &lt;-\n  metrics_rf_all |&gt;\n  filter(sample == \"Validation\") |&gt;\n  group_by(scenario, repn) |&gt;\n  arrange(brier) |&gt;\n  slice_head(n = 1) |&gt;\n  select(scenario, repn, ind, nb_leaves) |&gt;\n  mutate(result_type = \"lowest_brier\") |&gt;\n  ungroup()\n\n# Identify tree with lowest ICI\nlowest_ici_rf &lt;-\n  metrics_rf_all |&gt;\n  filter(sample == \"Validation\") |&gt;\n  group_by(scenario, repn) |&gt;\n  arrange(ici) |&gt;\n  slice_head(n = 1) |&gt;\n  select(scenario, repn, ind, nb_leaves) |&gt;\n  mutate(result_type = \"lowest_ici\") |&gt;\n  ungroup()\n\n# Identify tree with lowest KL\nlowest_kl_rf &lt;-\n  metrics_rf_all |&gt;\n  filter(sample == \"Validation\") |&gt;\n  group_by(scenario, repn) |&gt;\n  arrange(KL_20_true_probas) |&gt;\n  slice_head(n = 1) |&gt;\n  select(scenario, repn, ind, nb_leaves) |&gt;\n  mutate(result_type = \"lowest_kl\") |&gt;\n  ungroup()\n\n# Merge these\nrf_of_interest &lt;-\n  smallest_rf |&gt;\n  bind_rows(largest_rf) |&gt;\n  bind_rows(highest_auc_rf) |&gt;\n  bind_rows(lowest_mse_rf) |&gt;\n  bind_rows(lowest_brier_rf) |&gt;\n  bind_rows(lowest_ici_rf) |&gt;\n  bind_rows(lowest_kl_rf)\n\n# Add metrics now\nrf_of_interest &lt;-\n  rf_of_interest |&gt;\n  left_join(\n    metrics_rf_all,\n    by = c(\"scenario\", \"repn\", \"ind\", \"nb_leaves\"),\n    relationship = \"many-to-many\" # (train, valid, test)\n  ) |&gt;\n  mutate(\n    result_type = factor(\n      result_type,\n      levels = c(\n        \"smallest\", \"largest\", \"lowest_mse\", \"largest_auc\",\n        \"lowest_brier\", \"lowest_ici\", \"lowest_kl\"),\n      labels = c(\n        \"Smallest\", \"Largest\", \"MSE*\", \"AUC*\",\n        \"Brier*\", \"ICI*\", \"KL*\"\n      )\n    )\n  )\n\n# Sanity check\n# trees_of_interest_metrics_rf |&gt; count(scenario, sample, result_type)\n\n\nWe ran 100 replications of the simulations for each scenario and each set of hyperparameters. Let us compute the average AUC, ICI and KL Divergence over these replications, both on the train and on the validation set.\n\nmodels_interest_rf &lt;- rf_of_interest |&gt; \n  group_by(scenario, sample, result_type) |&gt; \n  summarise(\n    AUC_lower = quantile(AUC, probs = 2.5/100),\n    AUC_upper = quantile(AUC, probs = 97.5/100),\n    AUC_sd = sd(AUC),\n    AUC = mean(AUC),\n    brier_lower = quantile(brier, probs = 2.5/100),\n    brier_upper = quantile(brier, probs = 97.5/100),\n    brier_sd = sd(brier),\n    brier = mean(brier),\n    ici_lower = quantile(ici, probs = 2.5/100),\n    ici_upper = quantile(ici, probs = 97.5/100),\n    ici_sd = sd(ici),\n    ici = mean(ici),\n    KL_20_true_probas_lower = quantile(KL_20_true_probas, probs = 2.5/100),\n    KL_20_true_probas_upper = quantile(KL_20_true_probas, probs = 97.5/100),\n    KL_20_true_probas_sd = sd(KL_20_true_probas),\n    KL_20_true_probas = mean(KL_20_true_probas),\n    quant_ratio_sd = sd(inter_quantile_10_90),\n    quant_ratio = mean(inter_quantile_10_90),\n    .groups = \"drop\"\n  ) |&gt; \n  mutate(model = \"rf\")",
    "crumbs": [
      "Simulated Data",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Comparison of Models</span>"
    ]
  },
  {
    "objectID": "book_ojeda_comparison.html#extreme-gradient-boosting",
    "href": "book_ojeda_comparison.html#extreme-gradient-boosting",
    "title": "12  Comparison of Models",
    "section": "13.3 Extreme Gradient Boosting",
    "text": "13.3 Extreme Gradient Boosting\n\nscenarios &lt;- 1:16\n\nLet us load the estimated models from ?sec-simul-sgb.\n\nfiles &lt;- str_c(\n  \"output/simul/dgp-ojeda/resul_xgb_scenario_\", scenarios, \".rda\"\n)\nresul_xgb &lt;- map(files[file.exists(files)], ~{load(.x) ; resul_xgb_scenario})\n\nLet us merge in a single tibble the metrics computed over the replications of the scenarios.\n\nmetrics_xgb_all &lt;- map(\n  resul_xgb,\n  function(resul_xgb_sc) map(resul_xgb_sc, \"metrics_simul\") |&gt; list_rbind()\n) |&gt;\n  list_rbind() |&gt;\n  mutate(\n    sample = factor(\n      sample,\n      levels = c(\"train\", \"valid\", \"test\"),\n      labels = c(\"Train\",\"Validation\" ,\"Test\")\n    )\n  )\n\nFor each replication, we made some hyperparameters vary. Let us identify some models of interest:\n\nsmallest: model with the lowest number of boosting iteration\nlargest: model with the highest number of boosting iteration\nlargest_auc: model with the highest AUC on validation set\nlowest_mse: model with the lowest MSE on validation set\nlowest_brier: model with the lowest Brier score on validation set\nlowest_ici: model with the lowest ICI on validation set\nlowest_kl: model with the lowest KL Divergence on validation set\n\n\n\nCode\n# Identify the model with the smallest number of boosting iterations\nsmallest_xgb &lt;-\n  metrics_xgb_all |&gt;\n  filter(sample == \"Validation\") |&gt;\n  group_by(scenario, repn) |&gt;\n  arrange(nb_iter) |&gt;\n  slice_head(n = 1) |&gt;\n  select(scenario, repn, ind, nb_iter) |&gt;\n  mutate(result_type = \"smallest\") |&gt;\n  ungroup()\n\n# Identify the largest tree\nlargest_xgb &lt;-\n  metrics_xgb_all |&gt;\n  filter(sample == \"Validation\") |&gt;\n  group_by(scenario, repn) |&gt;\n  arrange(desc(nb_iter)) |&gt;\n  slice_head(n = 1) |&gt;\n  select(scenario, repn, ind, nb_iter) |&gt;\n  mutate(result_type = \"largest\") |&gt;\n  ungroup()\n\n# Identify tree with highest AUC on test set\nhighest_auc_xgb &lt;-\n  metrics_xgb_all |&gt;\n  filter(sample == \"Validation\") |&gt;\n  group_by(scenario, repn) |&gt;\n  arrange(desc(AUC)) |&gt;\n  slice_head(n = 1) |&gt;\n  select(scenario, repn, ind, nb_iter) |&gt;\n  mutate(result_type = \"largest_auc\") |&gt;\n  ungroup()\n\n# Identify tree with lowest MSE\nlowest_mse_xgb &lt;-\n  metrics_xgb_all |&gt;\n  filter(sample == \"Validation\") |&gt;\n  group_by(scenario, repn) |&gt;\n  arrange(mse) |&gt;\n  slice_head(n = 1) |&gt;\n  select(scenario, repn, ind, nb_iter) |&gt;\n  mutate(result_type = \"lowest_mse\") |&gt;\n  ungroup()\n\n# Identify tree with lowest brier\nlowest_brier_xgb &lt;-\n  metrics_xgb_all |&gt;\n  filter(sample == \"Validation\") |&gt;\n  group_by(scenario, repn) |&gt;\n  arrange(brier) |&gt;\n  slice_head(n = 1) |&gt;\n  select(scenario, repn, ind, nb_iter) |&gt;\n  mutate(result_type = \"lowest_brier\") |&gt;\n  ungroup()\n\n# Identify tree with lowest ICI\nlowest_ici_xgb &lt;-\n  metrics_xgb_all |&gt;\n  filter(sample == \"Validation\") |&gt;\n  group_by(scenario, repn) |&gt;\n  arrange(ici) |&gt;\n  slice_head(n = 1) |&gt;\n  select(scenario, repn, ind, nb_iter) |&gt;\n  mutate(result_type = \"lowest_ici\") |&gt;\n  ungroup()\n\n# Identify tree with lowest KL\nlowest_kl_xgb &lt;-\n  metrics_xgb_all |&gt;\n  filter(sample == \"Validation\") |&gt;\n  group_by(scenario, repn) |&gt;\n  arrange(KL_20_true_probas) |&gt;\n  slice_head(n = 1) |&gt;\n  select(scenario, repn, ind, nb_iter) |&gt;\n  mutate(result_type = \"lowest_kl\") |&gt;\n  ungroup()\n\n# Merge these\nmodels_of_interest_xgb &lt;-\n  smallest_xgb |&gt;\n  bind_rows(largest_xgb) |&gt;\n  bind_rows(highest_auc_xgb) |&gt;\n  bind_rows(lowest_mse_xgb) |&gt;\n  bind_rows(lowest_brier_xgb) |&gt;\n  bind_rows(lowest_ici_xgb) |&gt;\n  bind_rows(lowest_kl_xgb)\n\n# Add metrics now\nmodels_of_interest_metrics &lt;-\n  models_of_interest_xgb |&gt;\n  left_join(\n    metrics_xgb_all,\n    by = c(\"scenario\", \"repn\", \"ind\", \"nb_iter\"),\n    relationship = \"many-to-many\" # (train, valid, test)\n  ) |&gt; \n  mutate(\n    result_type = factor(\n      result_type,\n      levels = c(\n        \"smallest\", \"largest\", \"lowest_mse\", \"largest_auc\",\n        \"lowest_brier\", \"lowest_ici\", \"lowest_kl\"),\n      labels = c(\n        \"Smallest\", \"Largest\", \"MSE*\", \"AUC*\", \n        \"Brier*\", \"ICI*\", \"KL*\"\n      )\n    )\n  )\n\n# Sanity check\n# models_of_interest_metrics |&gt; count(scenario, sample, result_type)\n\n\nThen, we compute the average values of the AUC, the ICI and the KL divergence for these models of interest over the 100 replications, for each scenario, both on the train and the validation set.\n\nmodels_interest_xgb &lt;- models_of_interest_metrics |&gt; \n  group_by(scenario, sample, result_type) |&gt; \n  summarise(\n    AUC_lower = quantile(AUC, probs = 2.5/100),\n    AUC_upper = quantile(AUC, probs = 97.5/100),\n    AUC_sd = sd(AUC),\n    AUC = mean(AUC),\n    brier_lower = quantile(brier, probs = 2.5/100),\n    brier_upper = quantile(brier, probs = 97.5/100),\n    brier_sd = sd(brier),\n    brier = mean(brier),\n    ici_lower = quantile(ici, probs = 2.5/100),\n    ici_upper = quantile(ici, probs = 97.5/100),\n    ici_sd = sd(ici),\n    ici = mean(ici),\n    KL_20_true_probas_lower = quantile(KL_20_true_probas, probs = 2.5/100),\n    KL_20_true_probas_upper = quantile(KL_20_true_probas, probs = 97.5/100),\n    KL_20_true_probas_sd = sd(KL_20_true_probas),\n    KL_20_true_probas = mean(KL_20_true_probas),\n    quant_ratio_sd = sd(inter_quantile_10_90),\n    quant_ratio = mean(inter_quantile_10_90),\n    .groups = \"drop\"\n  ) |&gt; \n  mutate(\n    model = \"xgb\",\n    sample = str_to_lower(as.character(sample))\n  )\n\n# Sanity check\n# metrics_xgb_all |&gt; count(scenario, ind, sample, nb_iter) |&gt;\n#   filter(n != max(repns_vector))",
    "crumbs": [
      "Simulated Data",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Comparison of Models</span>"
    ]
  },
  {
    "objectID": "book_ojeda_comparison.html#generalized-linear-models",
    "href": "book_ojeda_comparison.html#generalized-linear-models",
    "title": "12  Comparison of Models",
    "section": "13.4 Generalized Linear Models",
    "text": "13.4 Generalized Linear Models\nLet us load the results from Chapter 9.\n\nfiles &lt;- str_c(\n  \"output/simul/dgp-ojeda/resul_glm_scenario_\", 1:16, \".rda\"\n)\nresul_glm &lt;- map(files[file.exists(files)], ~{load(.x) ; resul_glm_scenario})\n\nWe extract the computed metrics:\n\nmetrics_glm_all &lt;- map(\n  resul_glm,\n  function(resul_glm_sc) map(resul_glm_sc, \"metrics\") |&gt; list_rbind()\n) |&gt;\n  list_rbind() |&gt;\n  mutate(\n    sample = factor(\n      sample,\n      levels = c(\"train\", \"test\"), labels = c(\"Train\", \"Test\")\n    )\n  )\n\nThen, for each scenario, we compute the average of the AUC, the ICI and the KL divergence over the 100 replications.\n\nmodels_interest_glm &lt;- \n  metrics_glm_all |&gt; \n  group_by(scenario, sample) |&gt; \n  summarise(\n    AUC_lower = quantile(AUC, probs = 2.5/100),\n    AUC_upper = quantile(AUC, probs = 97.5/100),\n    AUC_sd = sd(AUC),\n    AUC = mean(AUC),\n    brier_lower = quantile(brier, probs = 2.5/100),\n    brier_upper = quantile(brier, probs = 97.5/100),\n    brier_sd = sd(brier),\n    brier = mean(brier),\n    ici_lower = quantile(ici, probs = 2.5/100),\n    ici_upper = quantile(ici, probs = 97.5/100),\n    ici_sd = sd(ici),\n    ici = mean(ici),\n    KL_20_true_probas_lower = quantile(KL_20_true_probas, probs = 2.5/100),\n    KL_20_true_probas_upper = quantile(KL_20_true_probas, probs = 97.5/100),\n    KL_20_true_probas_sd = sd(KL_20_true_probas),\n    KL_20_true_probas = mean(KL_20_true_probas),\n    quant_ratio_sd = sd(inter_quantile_10_90),\n    quant_ratio = mean(inter_quantile_10_90),\n    .groups = \"drop\"\n  ) |&gt; \n  mutate(\n    model = \"glm\",\n    sample = str_to_lower(as.character(sample))\n  ) |&gt; \n  mutate(result_type = \"None\")",
    "crumbs": [
      "Simulated Data",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Comparison of Models</span>"
    ]
  },
  {
    "objectID": "book_ojeda_comparison.html#generalized-additive-models",
    "href": "book_ojeda_comparison.html#generalized-additive-models",
    "title": "12  Comparison of Models",
    "section": "13.5 Generalized Additive Models",
    "text": "13.5 Generalized Additive Models\nLet us load the results from Chapter 10.\n\nfiles &lt;- str_c(\n  \"output/simul/dgp-ojeda/resul_gam_scenario_\", 1:16, \".rda\"\n)\nresul_gam &lt;- map(files[file.exists(files)], ~{load(.x) ; resul_gam_scenario})\n\nWe extract the computed metrics:\n\nmetrics_gam_all &lt;- map(\n  resul_gam,\n  function(resul_gam_sc) map(resul_gam_sc, \"metrics\") |&gt; list_rbind()\n) |&gt;\n  list_rbind() |&gt;\n  mutate(\n    sample = factor(\n      sample,\n      levels = c(\"train\", \"test\"), labels = c(\"Train\", \"Test\")\n    )\n  )\n\nThen, for each scenario, we compute the average of the AUC, the ICI and the KL divergence over the 100 replications.\n\nmodels_interest_gam &lt;- \n  metrics_gam_all |&gt; \n  group_by(scenario, sample) |&gt; \n  summarise(\n    AUC_lower = quantile(AUC, probs = 2.5/100),\n    AUC_upper = quantile(AUC, probs = 97.5/100),\n    AUC_sd = sd(AUC),\n    AUC = mean(AUC),\n    brier_lower = quantile(brier, probs = 2.5/100),\n    brier_upper = quantile(brier, probs = 97.5/100),\n    brier_sd = sd(brier),\n    brier = mean(brier),\n    ici_lower = quantile(ici, probs = 2.5/100),\n    ici_upper = quantile(ici, probs = 97.5/100),\n    ici_sd = sd(ici),\n    ici = mean(ici),\n    KL_20_true_probas_lower = quantile(KL_20_true_probas, probs = 2.5/100),\n    KL_20_true_probas_upper = quantile(KL_20_true_probas, probs = 97.5/100),\n    KL_20_true_probas_sd = sd(KL_20_true_probas),\n    KL_20_true_probas = mean(KL_20_true_probas),\n    quant_ratio_sd = sd(inter_quantile_10_90),\n    quant_ratio = mean(inter_quantile_10_90),\n    .groups = \"drop\"\n  ) |&gt; \n  mutate(\n    model = \"gam\",\n    sample = str_to_lower(as.character(sample))\n  ) |&gt; \n  mutate(result_type = \"None\")",
    "crumbs": [
      "Simulated Data",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Comparison of Models</span>"
    ]
  },
  {
    "objectID": "book_ojeda_comparison.html#generalized-additive-models-selection",
    "href": "book_ojeda_comparison.html#generalized-additive-models-selection",
    "title": "12  Comparison of Models",
    "section": "13.6 Generalized Additive Models Selection",
    "text": "13.6 Generalized Additive Models Selection\nLet us load the results from Chapter 11.\n\nfiles &lt;- str_c(\n  \"output/simul/dgp-ojeda/resul_gamsel_scenario_\", 1:16, \".rda\"\n)\nresul_gamsel &lt;- map(files[file.exists(files)], ~{load(.x) ; resul_gamsel_scenario})\n\nWe extract the computed metrics:\n\nmetrics_gamsel_all &lt;- map(\n  resul_gamsel,\n  function(resul_gamsel_sc) map(resul_gamsel_sc, \"metrics\") |&gt; list_rbind()\n) |&gt;\n  list_rbind() |&gt;\n  mutate(\n    sample = factor(\n      sample,\n      levels = c(\"train\", \"test\"), labels = c(\"Train\", \"Test\")\n    )\n  )\n\nThen, for each scenario, we compute the average of the AUC, the ICI and the KL divergence over the 100 replications.\n\nmodels_interest_gamsel &lt;- \n  metrics_gamsel_all |&gt; \n  group_by(scenario, sample) |&gt; \n  summarise(\n    AUC_lower = quantile(AUC, probs = 2.5/100),\n    AUC_upper = quantile(AUC, probs = 97.5/100),\n    AUC_sd = sd(AUC),\n    AUC = mean(AUC),\n    brier_lower = quantile(brier, probs = 2.5/100),\n    brier_upper = quantile(brier, probs = 97.5/100),\n    brier_sd = sd(brier),\n    brier = mean(brier),\n    ici_lower = quantile(ici, probs = 2.5/100),\n    ici_upper = quantile(ici, probs = 97.5/100),\n    ici_sd = sd(ici),\n    ici = mean(ici),\n    KL_20_true_probas_lower = quantile(KL_20_true_probas, probs = 2.5/100),\n    KL_20_true_probas_upper = quantile(KL_20_true_probas, probs = 97.5/100),\n    KL_20_true_probas_sd = sd(KL_20_true_probas),\n    KL_20_true_probas = mean(KL_20_true_probas),\n    quant_ratio_sd = sd(inter_quantile_10_90),\n    quant_ratio = mean(inter_quantile_10_90),\n    .groups = \"drop\"\n  ) |&gt; \n  mutate(\n    model = \"gamsel\",\n    sample = str_to_lower(as.character(sample))\n  ) |&gt; \n  mutate(result_type = \"None\")",
    "crumbs": [
      "Simulated Data",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Comparison of Models</span>"
    ]
  },
  {
    "objectID": "book_ojeda_comparison.html#tables",
    "href": "book_ojeda_comparison.html#tables",
    "title": "12  Comparison of Models",
    "section": "14.1 Tables",
    "text": "14.1 Tables\nWe also visualize the results in tables. For each model within a given scenario, we report the average AUC, Brier Score, ICI, and KL divergence over 100 replications for the ‘best’ model. For ensemble tree models, the ‘best’ model is identified either when maximizing the AUC (denoted \\(AUC^*\\)), when minimizing the Brier Score (denoted \\(Brier^*\\)), the ICI (denoted \\(ICI^*\\)), or the KL divergence (denoted \\(KL^*\\)). When the best model is selected based anything but the AUC, we compute the variation in the metric as the difference between the metric obtained when minimizing either the Brier score, the ICI, or the KL divergence and the metric obtained when maximizing AUC. Thus, negative values indicate a decrease in the metric compared to when the best model is selected by optimizing AUC. For general linear models, the only metrics reported are the AUC, Brier, ICI, and KL divergence.\n\n\nDisplay the codes to create the summary table.\ntable_models_interest_mean &lt;- \n  models_interest |&gt; \n  filter(sample == \"Test\") |&gt; \n  select(\n    scenario, sample, model, result_type, \n    AUC, brier, ici, kl = KL_20_true_probas, quant_ratio\n  ) |&gt; \n  filter(\n    result_type %in% c(\"AUC*\", \"Brier*\", \"ICI*\", \"KL*\", \"None\")\n  ) |&gt; \n  mutate(result_type = fct_recode(result_type, \"KL*\" = \"None\")) |&gt; \n  mutate(value_type = \"mean\")\n\ntable_models_interest_sd &lt;- \n  models_interest |&gt; \n  filter(sample == \"Test\") |&gt; \n  select(\n    scenario, sample, model, result_type, \n    AUC = AUC_sd, brier = brier_sd, ici = ici_sd, kl = KL_20_true_probas_sd, quant_ratio = quant_ratio_sd\n  ) |&gt; \n  filter(\n    result_type %in% c(\"AUC*\", \"Brier*\", \"ICI*\", \"KL*\", \"None\")\n  ) |&gt; \n  mutate(result_type = fct_recode(result_type, \"KL*\" = \"None\")) |&gt; \n  mutate(value_type = \"sd\")\n\n\nred_colours &lt;- c(\n  \"#FFD6D6\", \"#FFCCCC\", \"#FFC2C2\", \"#FFB8B8\", \"#FFADAD\", \n  \"#FFA3A3\", \"#FF9999\", \"#FF8F8F\", \"#FF8585\", \"#FF7A7A\"\n)\nred_colours_txt &lt;- c(\n  \"#333333\", \"#333333\", \"#2B2B2B\", \"#2B2B2B\", \"#232323\", \n  \"#1F1F1F\", \"#1A1A1A\", \"#141414\", \"#101010\", \"#0A0A0A\"\n)\ngreen_colours &lt;- c(\n  \"#E9F6E9\", \"#D4F2D4\", \"#BFEFBF\", \"#AADCA9\", \"#96C996\",\n  \"#81B781\", \"#6CA56C\", \"#578252\", \"#426F42\", \"#2F5D2F\"\n)\ngreen_colours_txt &lt;- c(\n  \"#1A1A1A\", \"#1A1A1A\", \"#1A1A1A\", \"#1A1A1A\", \"#1A1A1A\",\n  \"#E6E6E6\", \"#E6E6E6\", \"#E6E6E6\", \"#E6E6E6\", \"#E6E6E6\"\n)\n\naccuracy_digits &lt;- 0.01\n\ntable_kb &lt;- \n  table_models_interest_mean |&gt; \n  bind_rows(table_models_interest_sd) |&gt; \n  pivot_wider(\n    names_from = \"result_type\", \n    values_from = c(\"AUC\", \"brier\", \"ici\", \"kl\", \"quant_ratio\")\n  ) |&gt; \n  mutate(\n    value_type = factor(value_type, levels = c(\"mean\", \"sd\")),\n    scenario = factor(scenario)\n  ) |&gt; \n  select(\n    scenario, model, value_type,\n    # # columns for GLM/GAM/GAMSEL\n    # AUC_None, ici_None, kl_None, \n    # columns for ML models selected based on AUC\n    `AUC_AUC*`, `brier_AUC*`, `ici_AUC*`, `kl_AUC*`, `quant_ratio_AUC*`,\n    # columns for ML models selected based on Brier score\n    `AUC_Brier*`,  `brier_Brier*`, `ici_Brier*`, `kl_Brier*`, `quant_ratio_Brier*`,\n    # columns for ML models selected based on ICI\n    `AUC_ICI*`, `brier_ICI*`, `ici_ICI*`, `kl_ICI*`, `quant_ratio_ICI*`,\n    # columns for ML models selected based on KL dist\n    `AUC_KL*`, `brier_KL*`, `ici_KL*`, `kl_KL*`, `quant_ratio_KL*`\n  ) |&gt; \n  arrange(scenario, model, value_type) |&gt; \n  mutate(\n    # Difference in metrics computed when minnimizing Brier wrt when maximizing AUC\n    diff_AUC_Brier = `AUC_Brier*` - `AUC_AUC*`,\n    diff_brier_Brier = `brier_Brier*` - `brier_AUC*`,\n    diff_ICI_Brier = `ici_Brier*` - `ici_AUC*`,\n    diff_KL_Brier = `kl_Brier*` - `kl_AUC*`,\n    diff_quant_ratio_Brier = `quant_ratio_Brier*` - `quant_ratio_AUC*`,\n    # Difference in metrics computed when minnimizing ICI wrt when maximizing AUC\n    diff_AUC_ICI = `AUC_ICI*` - `AUC_AUC*`,\n    diff_brier_ICI = `brier_ICI*` - `brier_AUC*`,\n    diff_ICI_ICI = `ici_ICI*` - `ici_AUC*`,\n    diff_KL_ICI = `kl_ICI*` - `kl_AUC*`,\n    diff_quant_ratio_ICI = `quant_ratio_ICI*` - `quant_ratio_AUC*`,\n    # Difference in metrics computed when minnimizing KL wrt when maximizing AUC\n    diff_AUC_KL = `AUC_KL*` - `AUC_AUC*`,\n    diff_brier_KL = `brier_KL*` - `brier_AUC*`,\n    diff_ICI_KL = `ici_KL*` - `ici_AUC*`,\n    diff_KL_KL = `kl_KL*` - `kl_AUC*`,\n    diff_quant_ratio_KL = `quant_ratio_KL*` - `quant_ratio_AUC*`\n  ) |&gt; \n  ungroup()\n\nget_range_for_colours &lt;- function(variable_name) {\n  value &lt;- table_kb |&gt; \n    filter(value_type == \"mean\") |&gt; \n    pull(!!variable_name) |&gt; \n    range(na.rm = TRUE) |&gt; abs() |&gt; max()\n  value * c(-1, 1)\n}\n\nget_colour &lt;- function(variable, value_type, min_or_max, colour_type) {\n  if (value_type == \"mean\") {\n    variable_string &lt;- deparse(substitute(variable))\n    if (colour_type == \"bg\") {\n      # background colour\n      if (min_or_max == \"min\") {\n        colours &lt;- rev(c(rev(red_colours), green_colours))\n      } else {\n        colours &lt;- c(rev(red_colours), rev(green_colours))\n      }\n    } else {\n      # text colour\n      if (min_or_max == \"min\") {\n        colours &lt;- rev(c(rev(red_colours_txt), green_colours_txt))\n      } else {\n        colours &lt;- c(rev(red_colours_txt), rev(green_colours_txt))\n      }\n    }\n    res &lt;- kableExtra::spec_color(\n      variable,\n      palette = colours,\n      scale_from = get_range_for_colours(variable_string),\n      na_color = \"white\"\n    )\n  } else {\n    res &lt;- \"white\"\n  }\n  res\n}\n\ntable_kb &lt;- \n  table_kb |&gt; \n  rowwise() |&gt; \n  mutate(\n    # Difference in metrics computed when minnimizing ICI wrt when maximizing AUC\n    diff_AUC_Brier_bgcol = get_colour(diff_AUC_Brier, value_type, \"max\", \"bg\"),\n    diff_AUC_Brier_txtcol = get_colour(diff_AUC_Brier, value_type, \"max\", \"txt\"),\n    diff_brier_Brier_bgcol = get_colour(diff_brier_Brier, value_type, \"min\", \"bg\"),\n    diff_brier_Brier_txtcol = get_colour(diff_brier_Brier, value_type, \"min\", \"txt\"),\n    diff_ICI_Brier_bgcol = get_colour(diff_ICI_Brier, value_type, \"min\", \"bg\"),\n    diff_ICI_Brier_txtcol = get_colour(diff_ICI_Brier, value_type, \"min\", \"txt\"),\n    diff_KL_Brier_bgcol = get_colour(diff_KL_Brier, value_type, \"min\", \"bg\"),\n    diff_KL_Brier_txtcol = get_colour(diff_KL_Brier, value_type, \"min\", \"txt\"),\n    diff_quant_ratio_Brier_bgcol = get_colour(diff_quant_ratio_Brier, value_type, \"min\", \"bg\"),\n    diff_quant_ratio_Brier_txtcol = get_colour(diff_quant_ratio_Brier, value_type, \"min\", \"txt\"),\n    # Difference in metrics computed when minnimizing ICI wrt when maximizing AUC\n    diff_AUC_ICI_bgcol = get_colour(diff_AUC_ICI, value_type, \"max\", \"bg\"),\n    diff_AUC_ICI_txtcol = get_colour(diff_AUC_ICI, value_type, \"max\", \"txt\"),\n    diff_brier_ICI_bgcol = get_colour(diff_brier_ICI, value_type, \"min\", \"bg\"),\n    diff_brier_ICI_txtcol = get_colour(diff_brier_ICI, value_type, \"min\", \"txt\"),\n    diff_ICI_ICI_bgcol = get_colour(diff_ICI_ICI, value_type, \"min\", \"bg\"),\n    diff_ICI_ICI_txtcol = get_colour(diff_ICI_ICI, value_type, \"min\", \"txt\"),\n    diff_KL_ICI_bgcol = get_colour(diff_KL_ICI, value_type, \"min\", \"bg\"),\n    diff_KL_ICI_txtcol = get_colour(diff_KL_ICI, value_type, \"min\", \"txt\"),\n    diff_quant_ratio_ICI_bgcol = get_colour(diff_quant_ratio_ICI, value_type, \"min\", \"bg\"),\n    diff_quant_ratio_ICI_txtcol = get_colour(diff_quant_ratio_ICI, value_type, \"min\", \"txt\"),\n    # Difference in metrics computed when minnimizing KL wrt when maximizing AUC\n    diff_AUC_KL_bgcol = get_colour(diff_AUC_KL, value_type, \"max\", \"bg\"),\n    diff_AUC_KL_txtcol = get_colour(diff_AUC_KL, value_type, \"max\", \"txt\"),\n    diff_brier_KL_bgcol = get_colour(diff_brier_KL, value_type, \"min\", \"bg\"),\n    diff_brier_KL_txtcol = get_colour(diff_brier_KL, value_type, \"min\", \"txt\"),\n    diff_ICI_KL_bgcol = get_colour(diff_ICI_KL, value_type, \"min\", \"bg\"),\n    diff_ICI_KL_txtcol = get_colour(diff_ICI_KL, value_type, \"min\", \"txt\"),\n    diff_KL_KL_bgcol = get_colour(diff_KL_KL, value_type, \"min\", \"bg\"),\n    diff_KL_KL_txtcol = get_colour(diff_KL_KL, value_type, \"min\", \"txt\"),\n    diff_quant_ratio_KL_bgcol = get_colour(diff_quant_ratio_KL, value_type, \"min\", \"bg\"),\n    diff_quant_ratio_KL_txtcol = get_colour(diff_quant_ratio_KL, value_type, \"min\", \"txt\")\n  ) |&gt; \n  mutate(\n    across(\n      where(is.numeric), \n      ~ifelse(value_type == \"mean\", \n              scales::number(.x, accuracy = accuracy_digits),\n              str_c(\"(\", scales::number(.x, accuracy = accuracy_digits), \")\")\n      )\n    )\n  )\n\n\nopts &lt;- options(knitr.kable.NA = \"\")\n\n\nprint_table &lt;- function(scenario) {\n  \n  table_kb &lt;- table_kb |&gt; filter(scenario == !!scenario) |&gt; \n    select(\n      scenario, model,\n      # Max AUC\n      `AUC_AUC*`, `brier_AUC*`, `ici_AUC*`, `kl_AUC*`, `quant_ratio_AUC*`,\n      # Min Brier\n      `AUC_Brier*`, `brier_Brier*`, `ici_Brier*`, `kl_Brier*`, `quant_ratio_Brier*`,\n      diff_AUC_Brier, diff_brier_Brier, diff_ICI_Brier, diff_KL_Brier, diff_quant_ratio_Brier,\n      # Min ICI\n      `AUC_ICI*`, `brier_ICI*`, `ici_ICI*`, `kl_ICI*`, `quant_ratio_ICI*`,\n      diff_AUC_ICI, diff_brier_ICI, diff_ICI_ICI, diff_KL_ICI, diff_quant_ratio_ICI,\n      # Min KL\n      `AUC_KL*`, `brier_KL*`, `ici_KL*`, `kl_KL*`, `quant_ratio_KL*`,\n      diff_AUC_KL, diff_brier_KL, diff_ICI_KL, diff_KL_KL, diff_quant_ratio_KL,\n      # colouring variables\n      diff_AUC_Brier_bgcol, diff_brier_Brier_bgcol, diff_ICI_Brier_bgcol, diff_KL_Brier_bgcol, diff_quant_ratio_Brier_bgcol,\n      #\n      diff_AUC_Brier_txtcol, diff_brier_Brier_txtcol, diff_ICI_Brier_txtcol, diff_KL_Brier_txtcol, diff_quant_ratio_Brier_txtcol,\n      #\n      diff_AUC_ICI_bgcol, diff_brier_ICI_bgcol, diff_ICI_ICI_bgcol, diff_KL_ICI_bgcol, diff_quant_ratio_ICI_bgcol,\n      #\n      diff_AUC_ICI_txtcol, diff_brier_ICI_txtcol, diff_ICI_ICI_txtcol, diff_KL_ICI_txtcol, diff_quant_ratio_ICI_txtcol,\n      #\n      diff_AUC_KL_bgcol, diff_brier_KL_bgcol, diff_ICI_KL_bgcol, diff_KL_KL_bgcol, diff_quant_ratio_KL_bgcol,\n      #\n      diff_AUC_KL_txtcol, diff_brier_KL_txtcol, diff_ICI_KL_txtcol, diff_KL_KL_txtcol, diff_quant_ratio_KL_txtcol\n    )\n  \n  knitr::kable(\n    table_kb |&gt; \n      select(\n        scenario, model,\n        # Max AUC\n        `AUC_AUC*`, `brier_AUC*`, `ici_AUC*`, `kl_AUC*`, `quant_ratio_AUC*`,\n        # Min Brier\n        `AUC_Brier*`, `brier_Brier*`, `ici_Brier*`, `kl_Brier*`, `quant_ratio_Brier*`,\n        diff_AUC_Brier, diff_brier_Brier, diff_ICI_Brier, diff_KL_Brier, diff_quant_ratio_Brier,\n        # Min ICI\n        `AUC_ICI*`, `brier_ICI*`, `ici_ICI*`, `kl_ICI*`, `quant_ratio_ICI*`, \n        diff_AUC_ICI, diff_brier_ICI, diff_ICI_ICI, diff_KL_ICI, diff_quant_ratio_ICI,\n        # Min KL\n        `AUC_KL*`, `brier_KL*`, `ici_KL*`, `kl_KL*`, `quant_ratio_KL*`,\n        diff_AUC_KL, diff_brier_KL, diff_ICI_KL, diff_KL_KL, diff_quant_ratio_KL\n      ),\n    col.names = c(\n      \"Scenario\", \"Model\",\n      # # columns for GLM/GAM/GAMSEL\n      # \"AUC\", \"ICI\", \"KL\", \n      # columns for ML models selected based on AUC\n      \"AUC\", \"Brier\", \"ICI\", \"KL\", \"Quant. Ratio\",\n      # columns for ML models selected based on Brier\n      \"AUC\", \"Brier\", \"ICI\", \"KL\", \"Quant. Ratio\", \"ΔAUC\", \"ΔBrier\", \"ΔICI\", \"ΔKL\", \"ΔQR\",\n      # columns for ML models selected based on ICI\n      \"AUC\", \"Brier\", \"ICI\", \"KL\", \"Quant. Ratio\", \"ΔAUC\", \"ΔBrier\", \"ΔICI\", \"ΔKL\", \"ΔQR\",\n      # columns for ML models selected based on KL dist\n      \"AUC\", \"Brier\", \"ICI\", \"KL\", \"Quant. Ratio\", \"ΔAUC\", \"ΔBrier\",\"ΔICI\", \"ΔKL\", \"ΔQR\"\n    ),\n    align = str_c(\"cl\", str_c(rep(\"c\", 5+10*3), collapse = \"\"), collapse = \"\"),\n    escape = FALSE, booktabs = T, digits = 3, format = \"markdown\") |&gt; \n    # Difference in metrics computed when minnimizing Brier wrt when maximizing AUC\n    kableExtra::column_spec(\n      which(colnames(table_kb) == \"diff_AUC_Brier\"),\n      background = table_kb$diff_AUC_Brier_bgcol,\n      color = table_kb$diff_AUC_Brier_txtcol\n    ) |&gt;\n    kableExtra::column_spec(\n      which(colnames(table_kb) == \"diff_brier_Brier\"),\n      background = table_kb$diff_brier_Brier_bgcol,\n      color = table_kb$diff_brier_Brier_txtcol\n    ) |&gt;\n    kableExtra::column_spec(\n      which(colnames(table_kb) == \"diff_ICI_Brier\"),\n      background = table_kb$diff_ICI_Brier_bgcol,\n      color = table_kb$diff_ICI_Brier_txtcol\n    ) |&gt;\n    kableExtra::column_spec(\n      which(colnames(table_kb) == \"diff_KL_Brier\"),\n      background = table_kb$diff_KL_Brier_bgcol,\n      color = table_kb$diff_KL_Brier_txtcol\n    ) |&gt;\n    kableExtra::column_spec(\n      which(colnames(table_kb) == \"diff_quant_ratio_Brier\"),\n      background = table_kb$diff_quant_ratio_Brier_bgcol,\n      color = table_kb$diff_quant_ratio_Brier_txtcol\n    ) |&gt;\n    # Difference in metrics computed when minnimizing ICI wrt when maximizing AUC\n    kableExtra::column_spec(\n      which(colnames(table_kb) == \"diff_AUC_ICI\"),\n      background = table_kb$diff_AUC_ICI_bgcol,\n      color = table_kb$diff_AUC_ICI_txtcol\n    ) |&gt;\n    kableExtra::column_spec(\n      which(colnames(table_kb) == \"diff_brier_ICI\"),\n      background = table_kb$diff_brier_ICI_bgcol,\n      color = table_kb$diff_brier_ICI_txtcol\n    ) |&gt;\n    kableExtra::column_spec(\n      which(colnames(table_kb) == \"diff_ICI_ICI\"),\n      background = table_kb$diff_ICI_ICI_bgcol,\n      color = table_kb$diff_ICI_ICI_txtcol\n    ) |&gt;\n    kableExtra::column_spec(\n      which(colnames(table_kb) == \"diff_KL_ICI\"),\n      background = table_kb$diff_KL_ICI_bgcol,\n      color = table_kb$diff_KL_ICI_txtcol\n    ) |&gt;\n    kableExtra::column_spec(\n      which(colnames(table_kb) == \"diff_quant_ratio_ICI\"),\n      background = table_kb$diff_quant_ratio_ICI_bgcol,\n      color = table_kb$diff_quant_ratio_ICI_txtcol\n    ) |&gt;\n    # Difference in metrics computed when minnimizing KL wrt when maximizing AUC\n    kableExtra::column_spec(\n      which(colnames(table_kb) == \"diff_AUC_KL\"),\n      background = table_kb$diff_AUC_KL_bgcol,\n      color = table_kb$diff_AUC_KL_txtcol\n    ) |&gt;\n    kableExtra::column_spec(\n      which(colnames(table_kb) == \"diff_brier_KL\"),\n      background = table_kb$diff_brier_KL_bgcol,\n      color = table_kb$diff_brier_KL_txtcol\n    ) |&gt;\n    kableExtra::column_spec(\n      which(colnames(table_kb) == \"diff_ICI_KL\"),\n      background = table_kb$diff_ICI_KL_bgcol,\n      color = table_kb$diff_ICI_KL_txtcol\n    ) |&gt;\n    kableExtra::column_spec(\n      which(colnames(table_kb) == \"diff_KL_KL\"),\n      background = table_kb$diff_KL_KL_bgcol,\n      color = table_kb$diff_KL_KL_txtcol\n    ) |&gt;\n     kableExtra::column_spec(\n      which(colnames(table_kb) == \"diff_quant_ratio_KL\"),\n      background = table_kb$diff_quant_ratio_KL_bgcol,\n      color = table_kb$diff_quant_ratio_KL_txtcol\n    ) |&gt;\n    kableExtra::collapse_rows(columns = 1:2, valign = \"top\") |&gt;\n    kableExtra::add_header_above(\n      c(\" \" = 2,\n        # \"Generalized Lin. Models\" = 3,\n        \"AUC*\" = 5,\n        \"Brier*\" = 10,\n        \"ICI*\" = 10,\n        \"KL*\" = 10\n      )\n    )\n}\n\n\n\nDGP 1DGP 2DGP 3DGP 4\n\n\n\n01050100\n\n\n\n\n\n\n\n\n\n\n\nAUC*\n\n\n\n\nBrier*\n\n\n\n\nICI*\n\n\n\n\nKL*\n\n\n\n\n\nScenario\n\n\nModel\n\n\nAUC\n\n\nBrier\n\n\nICI\n\n\nKL\n\n\nQuant. Ratio\n\n\nAUC\n\n\nBrier\n\n\nICI\n\n\nKL\n\n\nQuant. Ratio\n\n\nΔAUC\n\n\nΔBrier\n\n\nΔICI\n\n\nΔKL\n\n\nΔQR\n\n\nAUC\n\n\nBrier\n\n\nICI\n\n\nKL\n\n\nQuant. Ratio\n\n\nΔAUC\n\n\nΔBrier\n\n\nΔICI\n\n\nΔKL\n\n\nΔQR\n\n\nAUC\n\n\nBrier\n\n\nICI\n\n\nKL\n\n\nQuant. Ratio\n\n\nΔAUC\n\n\nΔBrier\n\n\nΔICI\n\n\nΔKL\n\n\nΔQR\n\n\n\n\n\n\n1\n\n\nTrees\n\n\n0.75\n\n\n0.20\n\n\n0.01\n\n\n0.28\n\n\n1.02\n\n\n0.75\n\n\n0.20\n\n\n0.01\n\n\n0.28\n\n\n1.02\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n0.73\n\n\n0.21\n\n\n0.01\n\n\n1.26\n\n\n0.98\n\n\n-0.02\n\n\n0.00\n\n\n0.00\n\n\n0.98\n\n\n-0.05\n\n\n0.74\n\n\n0.21\n\n\n0.03\n\n\n0.09\n\n\n1.08\n\n\n-0.01\n\n\n0.00\n\n\n0.01\n\n\n-0.20\n\n\n0.06\n\n\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.12)\n\n\n(0.05)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.13)\n\n\n(0.05)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.54)\n\n\n(0.08)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.43)\n\n\n(0.03)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.02)\n\n\n(0.04)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(-0.10)\n\n\n(-0.01)\n\n\n\n\nRandom Forests\n\n\n0.76\n\n\n0.20\n\n\n0.01\n\n\n0.06\n\n\n1.00\n\n\n0.76\n\n\n0.20\n\n\n0.01\n\n\n0.05\n\n\n1.00\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n0.59\n\n\n0.23\n\n\n0.01\n\n\n2.61\n\n\n0.32\n\n\n-0.17\n\n\n0.03\n\n\n0.00\n\n\n2.55\n\n\n-0.67\n\n\n0.75\n\n\n0.20\n\n\n0.02\n\n\n0.03\n\n\n1.04\n\n\n-0.01\n\n\n0.00\n\n\n0.01\n\n\n-0.03\n\n\n0.04\n\n\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.02)\n\n\n(0.03)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.02)\n\n\n(0.03)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.12)\n\n\n(0.02)\n\n\n(0.00)\n\n\n(1.75)\n\n\n(0.46)\n\n\n(0.11)\n\n\n(0.02)\n\n\n(0.00)\n\n\n(1.73)\n\n\n(0.43)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.01)\n\n\n(0.02)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(-0.01)\n\n\n(0.00)\n\n\n\n\nXGB\n\n\n0.76\n\n\n0.20\n\n\n0.01\n\n\n0.05\n\n\n0.96\n\n\n0.76\n\n\n0.20\n\n\n0.01\n\n\n0.04\n\n\n0.97\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n-0.01\n\n\n0.01\n\n\n0.75\n\n\n0.20\n\n\n0.01\n\n\n0.05\n\n\n0.97\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n-0.01\n\n\n0.01\n\n\n0.75\n\n\n0.20\n\n\n0.01\n\n\n0.02\n\n\n1.01\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n-0.03\n\n\n0.04\n\n\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.03)\n\n\n(0.03)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.02)\n\n\n(0.02)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(-0.01)\n\n\n(-0.01)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.02)\n\n\n(0.03)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(-0.01)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.02)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(-0.02)\n\n\n(-0.01)\n\n\n\n\nGLM\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0.76\n\n\n0.20\n\n\n0.01\n\n\n0.00\n\n\n1.00\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.02)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGAM\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0.76\n\n\n0.20\n\n\n0.01\n\n\n0.00\n\n\n1.00\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.02)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGAMSEL\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0.76\n\n\n0.20\n\n\n0.01\n\n\n0.01\n\n\n0.93\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.02)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComparison of metrics for models chosen based on AUC, on Brier Score, on ICI, or on KL divergence (DGP 1, 0 noise variables)\n\n\n\n\n\n\n\n\n\n\n\n\n\nAUC*\n\n\n\n\nBrier*\n\n\n\n\nICI*\n\n\n\n\nKL*\n\n\n\n\n\nScenario\n\n\nModel\n\n\nAUC\n\n\nBrier\n\n\nICI\n\n\nKL\n\n\nQuant. Ratio\n\n\nAUC\n\n\nBrier\n\n\nICI\n\n\nKL\n\n\nQuant. Ratio\n\n\nΔAUC\n\n\nΔBrier\n\n\nΔICI\n\n\nΔKL\n\n\nΔQR\n\n\nAUC\n\n\nBrier\n\n\nICI\n\n\nKL\n\n\nQuant. Ratio\n\n\nΔAUC\n\n\nΔBrier\n\n\nΔICI\n\n\nΔKL\n\n\nΔQR\n\n\nAUC\n\n\nBrier\n\n\nICI\n\n\nKL\n\n\nQuant. Ratio\n\n\nΔAUC\n\n\nΔBrier\n\n\nΔICI\n\n\nΔKL\n\n\nΔQR\n\n\n\n\n\n\n2\n\n\nTrees\n\n\n0.75\n\n\n0.20\n\n\n0.01\n\n\n0.27\n\n\n1.02\n\n\n0.75\n\n\n0.20\n\n\n0.01\n\n\n0.29\n\n\n1.02\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n0.01\n\n\n0.00\n\n\n0.73\n\n\n0.21\n\n\n0.01\n\n\n1.22\n\n\n0.98\n\n\n-0.02\n\n\n0.00\n\n\n0.00\n\n\n0.95\n\n\n-0.04\n\n\n0.74\n\n\n0.21\n\n\n0.03\n\n\n0.09\n\n\n1.08\n\n\n-0.01\n\n\n0.00\n\n\n0.01\n\n\n-0.19\n\n\n0.06\n\n\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.12)\n\n\n(0.05)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.13)\n\n\n(0.05)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.56)\n\n\n(0.08)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.44)\n\n\n(0.03)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.02)\n\n\n(0.04)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(-0.10)\n\n\n(-0.01)\n\n\n\n\nRandom Forests\n\n\n0.76\n\n\n0.20\n\n\n0.01\n\n\n0.06\n\n\n0.98\n\n\n0.75\n\n\n0.20\n\n\n0.01\n\n\n0.05\n\n\n0.98\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n-0.01\n\n\n0.00\n\n\n0.61\n\n\n0.23\n\n\n0.01\n\n\n2.26\n\n\n0.40\n\n\n-0.15\n\n\n0.03\n\n\n0.00\n\n\n2.21\n\n\n-0.58\n\n\n0.75\n\n\n0.20\n\n\n0.01\n\n\n0.01\n\n\n1.00\n\n\n-0.01\n\n\n0.00\n\n\n0.00\n\n\n-0.05\n\n\n0.02\n\n\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.03)\n\n\n(0.03)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.02)\n\n\n(0.03)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(-0.01)\n\n\n(-0.01)\n\n\n(0.12)\n\n\n(0.02)\n\n\n(0.00)\n\n\n(1.87)\n\n\n(0.48)\n\n\n(0.12)\n\n\n(0.02)\n\n\n(0.00)\n\n\n(1.84)\n\n\n(0.45)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.02)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(-0.02)\n\n\n(-0.01)\n\n\n\n\nXGB\n\n\n0.76\n\n\n0.20\n\n\n0.01\n\n\n0.07\n\n\n0.93\n\n\n0.76\n\n\n0.20\n\n\n0.01\n\n\n0.05\n\n\n0.95\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n-0.02\n\n\n0.01\n\n\n0.75\n\n\n0.20\n\n\n0.01\n\n\n0.03\n\n\n0.97\n\n\n-0.01\n\n\n0.00\n\n\n0.00\n\n\n-0.03\n\n\n0.03\n\n\n0.75\n\n\n0.20\n\n\n0.02\n\n\n0.01\n\n\n1.02\n\n\n-0.01\n\n\n0.00\n\n\n0.00\n\n\n-0.06\n\n\n0.08\n\n\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.05)\n\n\n(0.04)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.02)\n\n\n(0.02)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(-0.02)\n\n\n(-0.01)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.02)\n\n\n(0.03)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(-0.03)\n\n\n(-0.01)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.02)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(-0.04)\n\n\n(-0.02)\n\n\n\n\nGLM\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0.76\n\n\n0.20\n\n\n0.01\n\n\n0.00\n\n\n1.00\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.02)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGAM\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0.76\n\n\n0.20\n\n\n0.01\n\n\n0.00\n\n\n1.01\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.02)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGAMSEL\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0.76\n\n\n0.20\n\n\n0.01\n\n\n0.01\n\n\n0.93\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.02)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComparison of metrics for models chosen based on AUC, on Brier Score, on ICI, or on KL divergence (DGP 1, 10 noise variables)\n\n\n\n\n\n\n\n\n\n\n\n\n\nAUC*\n\n\n\n\nBrier*\n\n\n\n\nICI*\n\n\n\n\nKL*\n\n\n\n\n\nScenario\n\n\nModel\n\n\nAUC\n\n\nBrier\n\n\nICI\n\n\nKL\n\n\nQuant. Ratio\n\n\nAUC\n\n\nBrier\n\n\nICI\n\n\nKL\n\n\nQuant. Ratio\n\n\nΔAUC\n\n\nΔBrier\n\n\nΔICI\n\n\nΔKL\n\n\nΔQR\n\n\nAUC\n\n\nBrier\n\n\nICI\n\n\nKL\n\n\nQuant. Ratio\n\n\nΔAUC\n\n\nΔBrier\n\n\nΔICI\n\n\nΔKL\n\n\nΔQR\n\n\nAUC\n\n\nBrier\n\n\nICI\n\n\nKL\n\n\nQuant. Ratio\n\n\nΔAUC\n\n\nΔBrier\n\n\nΔICI\n\n\nΔKL\n\n\nΔQR\n\n\n\n\n\n\n3\n\n\nTrees\n\n\n0.75\n\n\n0.20\n\n\n0.01\n\n\n0.27\n\n\n1.02\n\n\n0.75\n\n\n0.20\n\n\n0.01\n\n\n0.28\n\n\n1.02\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n0.01\n\n\n0.00\n\n\n0.73\n\n\n0.21\n\n\n0.01\n\n\n1.23\n\n\n0.98\n\n\n-0.02\n\n\n0.00\n\n\n0.00\n\n\n0.96\n\n\n-0.05\n\n\n0.74\n\n\n0.21\n\n\n0.03\n\n\n0.09\n\n\n1.08\n\n\n-0.01\n\n\n0.00\n\n\n0.01\n\n\n-0.19\n\n\n0.06\n\n\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.12)\n\n\n(0.05)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.13)\n\n\n(0.05)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.55)\n\n\n(0.08)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.44)\n\n\n(0.03)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.02)\n\n\n(0.04)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(-0.10)\n\n\n(-0.01)\n\n\n\n\nRandom Forests\n\n\n0.75\n\n\n0.20\n\n\n0.04\n\n\n0.27\n\n\n0.76\n\n\n0.75\n\n\n0.20\n\n\n0.03\n\n\n0.20\n\n\n0.80\n\n\n0.00\n\n\n0.00\n\n\n-0.01\n\n\n-0.07\n\n\n0.04\n\n\n0.50\n\n\n0.25\n\n\n0.01\n\n\n3.82\n\n\n0.00\n\n\n-0.25\n\n\n0.05\n\n\n-0.03\n\n\n3.56\n\n\n-0.76\n\n\n0.75\n\n\n0.21\n\n\n0.03\n\n\n0.14\n\n\n0.81\n\n\n0.00\n\n\n0.00\n\n\n-0.01\n\n\n-0.12\n\n\n0.04\n\n\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.05)\n\n\n(0.03)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.03)\n\n\n(0.02)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(-0.02)\n\n\n(-0.01)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.05)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(-0.03)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.02)\n\n\n(0.02)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(-0.04)\n\n\n(-0.01)\n\n\n\n\nXGB\n\n\n0.76\n\n\n0.20\n\n\n0.02\n\n\n0.09\n\n\n0.91\n\n\n0.76\n\n\n0.20\n\n\n0.01\n\n\n0.06\n\n\n0.93\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n-0.02\n\n\n0.02\n\n\n0.75\n\n\n0.20\n\n\n0.01\n\n\n0.04\n\n\n0.96\n\n\n-0.01\n\n\n0.00\n\n\n-0.01\n\n\n-0.05\n\n\n0.05\n\n\n0.74\n\n\n0.21\n\n\n0.02\n\n\n0.01\n\n\n1.02\n\n\n-0.01\n\n\n0.00\n\n\n0.00\n\n\n-0.08\n\n\n0.10\n\n\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.05)\n\n\n(0.04)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.03)\n\n\n(0.02)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(-0.02)\n\n\n(-0.01)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.02)\n\n\n(0.03)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(-0.02)\n\n\n(-0.01)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.02)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(-0.04)\n\n\n(-0.02)\n\n\n\n\nGLM\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0.76\n\n\n0.20\n\n\n0.01\n\n\n0.00\n\n\n1.01\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.02)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGAM\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0.75\n\n\n0.20\n\n\n0.02\n\n\n0.01\n\n\n1.05\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.01)\n\n\n(0.02)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGAMSEL\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0.76\n\n\n0.20\n\n\n0.01\n\n\n0.01\n\n\n0.93\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.02)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComparison of metrics for models chosen based on AUC, on Brier Score, on ICI, or on KL divergence (DGP 1, 50 noise variables)\n\n\n\n\n\n\n\n\n\n\n\n\n\nAUC*\n\n\n\n\nBrier*\n\n\n\n\nICI*\n\n\n\n\nKL*\n\n\n\n\n\nScenario\n\n\nModel\n\n\nAUC\n\n\nBrier\n\n\nICI\n\n\nKL\n\n\nQuant. Ratio\n\n\nAUC\n\n\nBrier\n\n\nICI\n\n\nKL\n\n\nQuant. Ratio\n\n\nΔAUC\n\n\nΔBrier\n\n\nΔICI\n\n\nΔKL\n\n\nΔQR\n\n\nAUC\n\n\nBrier\n\n\nICI\n\n\nKL\n\n\nQuant. Ratio\n\n\nΔAUC\n\n\nΔBrier\n\n\nΔICI\n\n\nΔKL\n\n\nΔQR\n\n\nAUC\n\n\nBrier\n\n\nICI\n\n\nKL\n\n\nQuant. Ratio\n\n\nΔAUC\n\n\nΔBrier\n\n\nΔICI\n\n\nΔKL\n\n\nΔQR\n\n\n\n\n\n\n4\n\n\nTrees\n\n\n0.75\n\n\n0.20\n\n\n0.01\n\n\n0.28\n\n\n1.02\n\n\n0.75\n\n\n0.20\n\n\n0.01\n\n\n0.29\n\n\n1.02\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n0.01\n\n\n0.00\n\n\n0.73\n\n\n0.21\n\n\n0.01\n\n\n1.22\n\n\n0.98\n\n\n-0.02\n\n\n0.00\n\n\n0.00\n\n\n0.94\n\n\n-0.05\n\n\n0.74\n\n\n0.21\n\n\n0.03\n\n\n0.09\n\n\n1.08\n\n\n-0.01\n\n\n0.00\n\n\n0.01\n\n\n-0.19\n\n\n0.05\n\n\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.12)\n\n\n(0.05)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.13)\n\n\n(0.05)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.54)\n\n\n(0.08)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.42)\n\n\n(0.03)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.02)\n\n\n(0.04)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(-0.10)\n\n\n(-0.01)\n\n\n\n\nRandom Forests\n\n\n0.75\n\n\n0.21\n\n\n0.07\n\n\n0.59\n\n\n0.57\n\n\n0.74\n\n\n0.21\n\n\n0.06\n\n\n0.47\n\n\n0.61\n\n\n0.00\n\n\n0.00\n\n\n-0.01\n\n\n-0.11\n\n\n0.04\n\n\n0.50\n\n\n0.25\n\n\n0.01\n\n\n3.82\n\n\n0.00\n\n\n-0.24\n\n\n0.04\n\n\n-0.06\n\n\n3.23\n\n\n-0.57\n\n\n0.74\n\n\n0.21\n\n\n0.06\n\n\n0.45\n\n\n0.61\n\n\n0.00\n\n\n0.00\n\n\n-0.01\n\n\n-0.13\n\n\n0.04\n\n\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.08)\n\n\n(0.03)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.03)\n\n\n(0.02)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(-0.04)\n\n\n(-0.01)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.05)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(-0.02)\n\n\n(-0.03)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.03)\n\n\n(0.02)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(-0.05)\n\n\n(-0.01)\n\n\n\n\nXGB\n\n\n0.76\n\n\n0.20\n\n\n0.02\n\n\n0.09\n\n\n0.91\n\n\n0.76\n\n\n0.20\n\n\n0.01\n\n\n0.07\n\n\n0.92\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n-0.02\n\n\n0.01\n\n\n0.75\n\n\n0.20\n\n\n0.01\n\n\n0.04\n\n\n0.96\n\n\n-0.01\n\n\n0.00\n\n\n-0.01\n\n\n-0.05\n\n\n0.05\n\n\n0.74\n\n\n0.21\n\n\n0.02\n\n\n0.01\n\n\n1.02\n\n\n-0.01\n\n\n0.00\n\n\n0.00\n\n\n-0.08\n\n\n0.11\n\n\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.04)\n\n\n(0.03)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.03)\n\n\n(0.03)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(-0.01)\n\n\n(-0.01)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.02)\n\n\n(0.02)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(-0.02)\n\n\n(-0.01)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(-0.03)\n\n\n(-0.02)\n\n\n\n\nGLM\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0.75\n\n\n0.20\n\n\n0.01\n\n\n0.00\n\n\n1.02\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.02)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGAM\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0.74\n\n\n0.21\n\n\n0.04\n\n\n0.03\n\n\n1.10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.02)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGAMSEL\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0.76\n\n\n0.20\n\n\n0.01\n\n\n0.01\n\n\n0.93\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.02)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComparison of metrics for models chosen based on AUC, on Brier Score, on ICI, or on KL divergence (DGP 1, 100 noise variables)\n\n\n\n\n\n\n\n\n01050100\n\n\n\n\n\n\n\n\n\n\n\nAUC*\n\n\n\n\nBrier*\n\n\n\n\nICI*\n\n\n\n\nKL*\n\n\n\n\n\nScenario\n\n\nModel\n\n\nAUC\n\n\nBrier\n\n\nICI\n\n\nKL\n\n\nQuant. Ratio\n\n\nAUC\n\n\nBrier\n\n\nICI\n\n\nKL\n\n\nQuant. Ratio\n\n\nΔAUC\n\n\nΔBrier\n\n\nΔICI\n\n\nΔKL\n\n\nΔQR\n\n\nAUC\n\n\nBrier\n\n\nICI\n\n\nKL\n\n\nQuant. Ratio\n\n\nΔAUC\n\n\nΔBrier\n\n\nΔICI\n\n\nΔKL\n\n\nΔQR\n\n\nAUC\n\n\nBrier\n\n\nICI\n\n\nKL\n\n\nQuant. Ratio\n\n\nΔAUC\n\n\nΔBrier\n\n\nΔICI\n\n\nΔKL\n\n\nΔQR\n\n\n\n\n\n\n5\n\n\nTrees\n\n\n0.83\n\n\n0.12\n\n\n0.01\n\n\n0.24\n\n\n1.03\n\n\n0.83\n\n\n0.12\n\n\n0.01\n\n\n0.23\n\n\n1.03\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n-0.01\n\n\n0.00\n\n\n0.81\n\n\n0.13\n\n\n0.01\n\n\n0.84\n\n\n0.97\n\n\n-0.02\n\n\n0.01\n\n\n0.00\n\n\n0.60\n\n\n-0.06\n\n\n0.82\n\n\n0.13\n\n\n0.03\n\n\n0.06\n\n\n1.07\n\n\n-0.02\n\n\n0.00\n\n\n0.02\n\n\n-0.18\n\n\n0.04\n\n\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.10)\n\n\n(0.06)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.10)\n\n\n(0.05)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.02)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.39)\n\n\n(0.13)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.29)\n\n\n(0.07)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.02)\n\n\n(0.04)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(-0.08)\n\n\n(-0.02)\n\n\n\n\nRandom Forests\n\n\n0.84\n\n\n0.12\n\n\n0.01\n\n\n0.04\n\n\n1.01\n\n\n0.84\n\n\n0.12\n\n\n0.01\n\n\n0.04\n\n\n1.01\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n-0.01\n\n\n0.00\n\n\n0.62\n\n\n0.15\n\n\n0.01\n\n\n2.66\n\n\n0.34\n\n\n-0.22\n\n\n0.03\n\n\n0.00\n\n\n2.62\n\n\n-0.67\n\n\n0.83\n\n\n0.12\n\n\n0.01\n\n\n0.02\n\n\n1.03\n\n\n-0.01\n\n\n0.00\n\n\n0.00\n\n\n-0.03\n\n\n0.02\n\n\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.02)\n\n\n(0.03)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.03)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.15)\n\n\n(0.02)\n\n\n(0.00)\n\n\n(1.70)\n\n\n(0.45)\n\n\n(0.15)\n\n\n(0.02)\n\n\n(0.00)\n\n\n(1.68)\n\n\n(0.42)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.03)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(-0.01)\n\n\n(0.00)\n\n\n\n\nXGB\n\n\n0.84\n\n\n0.12\n\n\n0.01\n\n\n0.03\n\n\n0.99\n\n\n0.84\n\n\n0.12\n\n\n0.01\n\n\n0.03\n\n\n0.99\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n0.84\n\n\n0.12\n\n\n0.01\n\n\n0.02\n\n\n1.00\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n0.01\n\n\n0.84\n\n\n0.12\n\n\n0.01\n\n\n0.01\n\n\n1.01\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n-0.01\n\n\n0.02\n\n\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.03)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.03)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.03)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.03)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(-0.01)\n\n\n(0.00)\n\n\n\n\nGLM\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0.84\n\n\n0.12\n\n\n0.01\n\n\n0.02\n\n\n0.99\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.02)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGAM\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0.84\n\n\n0.12\n\n\n0.01\n\n\n0.01\n\n\n1.00\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.02)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGAMSEL\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0.84\n\n\n0.12\n\n\n0.02\n\n\n0.04\n\n\n0.92\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.02)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComparison of metrics for models chosen based on AUC, on Brier Score, on ICI, or on KL divergence (DGP 2, 0 noise variables)\n\n\n\n\n\n\n\n\n\n\n\n\n\nAUC*\n\n\n\n\nBrier*\n\n\n\n\nICI*\n\n\n\n\nKL*\n\n\n\n\n\nScenario\n\n\nModel\n\n\nAUC\n\n\nBrier\n\n\nICI\n\n\nKL\n\n\nQuant. Ratio\n\n\nAUC\n\n\nBrier\n\n\nICI\n\n\nKL\n\n\nQuant. Ratio\n\n\nΔAUC\n\n\nΔBrier\n\n\nΔICI\n\n\nΔKL\n\n\nΔQR\n\n\nAUC\n\n\nBrier\n\n\nICI\n\n\nKL\n\n\nQuant. Ratio\n\n\nΔAUC\n\n\nΔBrier\n\n\nΔICI\n\n\nΔKL\n\n\nΔQR\n\n\nAUC\n\n\nBrier\n\n\nICI\n\n\nKL\n\n\nQuant. Ratio\n\n\nΔAUC\n\n\nΔBrier\n\n\nΔICI\n\n\nΔKL\n\n\nΔQR\n\n\n\n\n\n\n6\n\n\nTrees\n\n\n0.83\n\n\n0.12\n\n\n0.01\n\n\n0.24\n\n\n1.03\n\n\n0.83\n\n\n0.12\n\n\n0.01\n\n\n0.23\n\n\n1.03\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n-0.02\n\n\n0.00\n\n\n0.81\n\n\n0.13\n\n\n0.01\n\n\n0.82\n\n\n0.97\n\n\n-0.02\n\n\n0.01\n\n\n0.00\n\n\n0.58\n\n\n-0.06\n\n\n0.82\n\n\n0.13\n\n\n0.03\n\n\n0.06\n\n\n1.07\n\n\n-0.02\n\n\n0.00\n\n\n0.02\n\n\n-0.18\n\n\n0.04\n\n\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.10)\n\n\n(0.06)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.10)\n\n\n(0.05)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.02)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.39)\n\n\n(0.13)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.30)\n\n\n(0.07)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.02)\n\n\n(0.04)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(-0.08)\n\n\n(-0.02)\n\n\n\n\nRandom Forests\n\n\n0.84\n\n\n0.12\n\n\n0.01\n\n\n0.04\n\n\n0.99\n\n\n0.84\n\n\n0.12\n\n\n0.01\n\n\n0.03\n\n\n1.00\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n-0.01\n\n\n0.00\n\n\n0.61\n\n\n0.15\n\n\n0.01\n\n\n2.65\n\n\n0.32\n\n\n-0.23\n\n\n0.03\n\n\n0.00\n\n\n2.61\n\n\n-0.68\n\n\n0.83\n\n\n0.12\n\n\n0.01\n\n\n0.01\n\n\n1.00\n\n\n-0.01\n\n\n0.00\n\n\n0.00\n\n\n-0.03\n\n\n0.01\n\n\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.03)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.03)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.15)\n\n\n(0.02)\n\n\n(0.00)\n\n\n(1.81)\n\n\n(0.46)\n\n\n(0.15)\n\n\n(0.02)\n\n\n(0.00)\n\n\n(1.80)\n\n\n(0.44)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.03)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(-0.01)\n\n\n(0.00)\n\n\n\n\nXGB\n\n\n0.84\n\n\n0.12\n\n\n0.01\n\n\n0.03\n\n\n0.96\n\n\n0.84\n\n\n0.12\n\n\n0.01\n\n\n0.03\n\n\n0.97\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n0.84\n\n\n0.12\n\n\n0.01\n\n\n0.01\n\n\n0.99\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n-0.02\n\n\n0.02\n\n\n0.83\n\n\n0.12\n\n\n0.01\n\n\n0.01\n\n\n1.00\n\n\n-0.01\n\n\n0.00\n\n\n0.00\n\n\n-0.02\n\n\n0.04\n\n\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.03)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.03)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.03)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(-0.01)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.02)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(-0.01)\n\n\n(-0.01)\n\n\n\n\nGLM\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0.84\n\n\n0.12\n\n\n0.01\n\n\n0.02\n\n\n0.99\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.02)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGAM\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0.84\n\n\n0.12\n\n\n0.01\n\n\n0.01\n\n\n1.00\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.02)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGAMSEL\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0.84\n\n\n0.12\n\n\n0.02\n\n\n0.04\n\n\n0.92\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.02)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComparison of metrics for models chosen based on AUC, on Brier Score, on ICI, or on KL divergence (DGP 2, 10 noise variables)\n\n\n\n\n\n\n\n\n\n\n\n\n\nAUC*\n\n\n\n\nBrier*\n\n\n\n\nICI*\n\n\n\n\nKL*\n\n\n\n\n\nScenario\n\n\nModel\n\n\nAUC\n\n\nBrier\n\n\nICI\n\n\nKL\n\n\nQuant. Ratio\n\n\nAUC\n\n\nBrier\n\n\nICI\n\n\nKL\n\n\nQuant. Ratio\n\n\nΔAUC\n\n\nΔBrier\n\n\nΔICI\n\n\nΔKL\n\n\nΔQR\n\n\nAUC\n\n\nBrier\n\n\nICI\n\n\nKL\n\n\nQuant. Ratio\n\n\nΔAUC\n\n\nΔBrier\n\n\nΔICI\n\n\nΔKL\n\n\nΔQR\n\n\nAUC\n\n\nBrier\n\n\nICI\n\n\nKL\n\n\nQuant. Ratio\n\n\nΔAUC\n\n\nΔBrier\n\n\nΔICI\n\n\nΔKL\n\n\nΔQR\n\n\n\n\n\n\n7\n\n\nTrees\n\n\n0.83\n\n\n0.12\n\n\n0.01\n\n\n0.24\n\n\n1.03\n\n\n0.83\n\n\n0.12\n\n\n0.01\n\n\n0.22\n\n\n1.03\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n-0.02\n\n\n-0.01\n\n\n0.81\n\n\n0.13\n\n\n0.01\n\n\n0.83\n\n\n0.97\n\n\n-0.02\n\n\n0.01\n\n\n0.00\n\n\n0.59\n\n\n-0.06\n\n\n0.82\n\n\n0.13\n\n\n0.03\n\n\n0.06\n\n\n1.07\n\n\n-0.02\n\n\n0.00\n\n\n0.02\n\n\n-0.18\n\n\n0.04\n\n\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.10)\n\n\n(0.06)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.10)\n\n\n(0.06)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.02)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.39)\n\n\n(0.13)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.30)\n\n\n(0.07)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.02)\n\n\n(0.04)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(-0.08)\n\n\n(-0.02)\n\n\n\n\nRandom Forests\n\n\n0.83\n\n\n0.12\n\n\n0.04\n\n\n0.37\n\n\n0.78\n\n\n0.83\n\n\n0.12\n\n\n0.03\n\n\n0.18\n\n\n0.85\n\n\n0.00\n\n\n0.00\n\n\n-0.01\n\n\n-0.19\n\n\n0.07\n\n\n0.50\n\n\n0.16\n\n\n0.01\n\n\n3.88\n\n\n0.00\n\n\n-0.33\n\n\n0.04\n\n\n-0.03\n\n\n3.51\n\n\n-0.78\n\n\n0.83\n\n\n0.12\n\n\n0.03\n\n\n0.12\n\n\n0.88\n\n\n-0.01\n\n\n0.00\n\n\n-0.01\n\n\n-0.26\n\n\n0.10\n\n\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.11)\n\n\n(0.04)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.04)\n\n\n(0.02)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(-0.07)\n\n\n(-0.02)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.14)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.03)\n\n\n(-0.04)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.02)\n\n\n(0.02)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(-0.09)\n\n\n(-0.02)\n\n\n\n\nXGB\n\n\n0.84\n\n\n0.12\n\n\n0.01\n\n\n0.04\n\n\n0.95\n\n\n0.84\n\n\n0.12\n\n\n0.01\n\n\n0.03\n\n\n0.95\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n-0.01\n\n\n0.01\n\n\n0.83\n\n\n0.12\n\n\n0.01\n\n\n0.01\n\n\n0.98\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n-0.03\n\n\n0.03\n\n\n0.83\n\n\n0.12\n\n\n0.01\n\n\n0.01\n\n\n1.00\n\n\n-0.01\n\n\n0.00\n\n\n0.00\n\n\n-0.04\n\n\n0.05\n\n\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.06)\n\n\n(0.03)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.03)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(-0.05)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.03)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(-0.05)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.02)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(-0.06)\n\n\n(-0.01)\n\n\n\n\nGLM\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0.84\n\n\n0.12\n\n\n0.01\n\n\n0.03\n\n\n1.00\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.02)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGAM\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0.83\n\n\n0.12\n\n\n0.02\n\n\n0.02\n\n\n1.04\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.02)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGAMSEL\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0.84\n\n\n0.12\n\n\n0.02\n\n\n0.04\n\n\n0.92\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.02)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComparison of metrics for models chosen based on AUC, on Brier Score, on ICI, or on KL divergence (DGP 2, 50 noise variables)\n\n\n\n\n\n\n\n\n\n\n\n\n\nAUC*\n\n\n\n\nBrier*\n\n\n\n\nICI*\n\n\n\n\nKL*\n\n\n\n\n\nScenario\n\n\nModel\n\n\nAUC\n\n\nBrier\n\n\nICI\n\n\nKL\n\n\nQuant. Ratio\n\n\nAUC\n\n\nBrier\n\n\nICI\n\n\nKL\n\n\nQuant. Ratio\n\n\nΔAUC\n\n\nΔBrier\n\n\nΔICI\n\n\nΔKL\n\n\nΔQR\n\n\nAUC\n\n\nBrier\n\n\nICI\n\n\nKL\n\n\nQuant. Ratio\n\n\nΔAUC\n\n\nΔBrier\n\n\nΔICI\n\n\nΔKL\n\n\nΔQR\n\n\nAUC\n\n\nBrier\n\n\nICI\n\n\nKL\n\n\nQuant. Ratio\n\n\nΔAUC\n\n\nΔBrier\n\n\nΔICI\n\n\nΔKL\n\n\nΔQR\n\n\n\n\n\n\n8\n\n\nTrees\n\n\n0.83\n\n\n0.12\n\n\n0.01\n\n\n0.24\n\n\n1.03\n\n\n0.83\n\n\n0.12\n\n\n0.01\n\n\n0.22\n\n\n1.03\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n-0.01\n\n\n0.00\n\n\n0.81\n\n\n0.13\n\n\n0.01\n\n\n0.82\n\n\n0.97\n\n\n-0.02\n\n\n0.01\n\n\n0.00\n\n\n0.58\n\n\n-0.06\n\n\n0.82\n\n\n0.13\n\n\n0.03\n\n\n0.06\n\n\n1.07\n\n\n-0.02\n\n\n0.00\n\n\n0.02\n\n\n-0.17\n\n\n0.04\n\n\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.09)\n\n\n(0.05)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.10)\n\n\n(0.05)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.02)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.40)\n\n\n(0.13)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.30)\n\n\n(0.07)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.02)\n\n\n(0.04)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(-0.08)\n\n\n(-0.01)\n\n\n\n\nRandom Forests\n\n\n0.82\n\n\n0.13\n\n\n0.07\n\n\n0.84\n\n\n0.60\n\n\n0.82\n\n\n0.13\n\n\n0.05\n\n\n0.53\n\n\n0.71\n\n\n0.00\n\n\n0.00\n\n\n-0.02\n\n\n-0.32\n\n\n0.11\n\n\n0.50\n\n\n0.16\n\n\n0.01\n\n\n3.88\n\n\n0.00\n\n\n-0.32\n\n\n0.03\n\n\n-0.06\n\n\n3.04\n\n\n-0.60\n\n\n0.82\n\n\n0.13\n\n\n0.05\n\n\n0.49\n\n\n0.72\n\n\n0.00\n\n\n0.00\n\n\n-0.02\n\n\n-0.35\n\n\n0.12\n\n\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.11)\n\n\n(0.05)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.06)\n\n\n(0.02)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(-0.05)\n\n\n(-0.03)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.14)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.03)\n\n\n(-0.05)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.04)\n\n\n(0.02)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(-0.07)\n\n\n(-0.03)\n\n\n\n\nXGB\n\n\n0.84\n\n\n0.12\n\n\n0.01\n\n\n0.04\n\n\n0.94\n\n\n0.84\n\n\n0.12\n\n\n0.01\n\n\n0.03\n\n\n0.95\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n-0.01\n\n\n0.01\n\n\n0.83\n\n\n0.12\n\n\n0.01\n\n\n0.01\n\n\n0.98\n\n\n-0.01\n\n\n0.00\n\n\n0.00\n\n\n-0.03\n\n\n0.04\n\n\n0.83\n\n\n0.12\n\n\n0.01\n\n\n0.01\n\n\n0.99\n\n\n-0.01\n\n\n0.00\n\n\n0.00\n\n\n-0.03\n\n\n0.06\n\n\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.02)\n\n\n(0.03)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.03)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(-0.01)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.03)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(-0.01)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.02)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(-0.01)\n\n\n(-0.01)\n\n\n\n\nGLM\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0.84\n\n\n0.12\n\n\n0.01\n\n\n0.03\n\n\n1.01\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.02)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGAM\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0.82\n\n\n0.13\n\n\n0.03\n\n\n0.04\n\n\n1.08\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.02)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGAMSEL\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0.84\n\n\n0.12\n\n\n0.02\n\n\n0.04\n\n\n0.92\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.02)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComparison of metrics for models chosen based on AUC, on Brier Score, on ICI, or on KL divergence (DGP 2, 100 noise variables)\n\n\n\n\n\n\n\n\n01050100\n\n\n\n\n\n\n\n\n\n\n\nAUC*\n\n\n\n\nBrier*\n\n\n\n\nICI*\n\n\n\n\nKL*\n\n\n\n\n\nScenario\n\n\nModel\n\n\nAUC\n\n\nBrier\n\n\nICI\n\n\nKL\n\n\nQuant. Ratio\n\n\nAUC\n\n\nBrier\n\n\nICI\n\n\nKL\n\n\nQuant. Ratio\n\n\nΔAUC\n\n\nΔBrier\n\n\nΔICI\n\n\nΔKL\n\n\nΔQR\n\n\nAUC\n\n\nBrier\n\n\nICI\n\n\nKL\n\n\nQuant. Ratio\n\n\nΔAUC\n\n\nΔBrier\n\n\nΔICI\n\n\nΔKL\n\n\nΔQR\n\n\nAUC\n\n\nBrier\n\n\nICI\n\n\nKL\n\n\nQuant. Ratio\n\n\nΔAUC\n\n\nΔBrier\n\n\nΔICI\n\n\nΔKL\n\n\nΔQR\n\n\n\n\n\n\n9\n\n\nTrees\n\n\n0.55\n\n\n0.24\n\n\n0.02\n\n\n1.23\n\n\n0.35\n\n\n0.55\n\n\n0.24\n\n\n0.01\n\n\n1.29\n\n\n0.36\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n0.05\n\n\n0.00\n\n\n0.55\n\n\n0.24\n\n\n0.01\n\n\n1.33\n\n\n0.35\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n0.10\n\n\n0.00\n\n\n0.52\n\n\n0.27\n\n\n0.13\n\n\n0.05\n\n\n1.00\n\n\n-0.03\n\n\n0.02\n\n\n0.11\n\n\n-1.18\n\n\n0.65\n\n\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.22)\n\n\n(0.06)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.23)\n\n\n(0.06)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.23)\n\n\n(0.06)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.02)\n\n\n(0.06)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(-0.20)\n\n\n(0.00)\n\n\n\n\nRandom Forests\n\n\n0.68\n\n\n0.22\n\n\n0.02\n\n\n0.12\n\n\n0.81\n\n\n0.68\n\n\n0.22\n\n\n0.01\n\n\n0.04\n\n\n0.92\n\n\n0.00\n\n\n0.00\n\n\n-0.01\n\n\n-0.08\n\n\n0.11\n\n\n0.58\n\n\n0.23\n\n\n0.01\n\n\n1.72\n\n\n0.43\n\n\n-0.10\n\n\n0.01\n\n\n-0.01\n\n\n1.60\n\n\n-0.38\n\n\n0.67\n\n\n0.22\n\n\n0.02\n\n\n0.01\n\n\n1.02\n\n\n-0.01\n\n\n0.00\n\n\n-0.01\n\n\n-0.11\n\n\n0.21\n\n\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.08)\n\n\n(0.10)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.02)\n\n\n(0.03)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(-0.01)\n\n\n(-0.06)\n\n\n(-0.06)\n\n\n(0.09)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(1.58)\n\n\n(0.46)\n\n\n(0.08)\n\n\n(0.01)\n\n\n(-0.01)\n\n\n(1.50)\n\n\n(0.36)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.02)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(-0.01)\n\n\n(-0.08)\n\n\n(-0.07)\n\n\n\n\nXGB\n\n\n0.68\n\n\n0.22\n\n\n0.01\n\n\n0.01\n\n\n0.96\n\n\n0.68\n\n\n0.22\n\n\n0.01\n\n\n0.01\n\n\n0.96\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n0.68\n\n\n0.22\n\n\n0.01\n\n\n0.03\n\n\n0.92\n\n\n-0.01\n\n\n0.00\n\n\n0.00\n\n\n0.02\n\n\n-0.04\n\n\n0.68\n\n\n0.22\n\n\n0.01\n\n\n0.01\n\n\n1.00\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n-0.01\n\n\n0.04\n\n\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.04)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.03)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(-0.01)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.02)\n\n\n(0.04)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.02)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.02)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(-0.01)\n\n\n(-0.02)\n\n\n\n\nGLM\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0.69\n\n\n0.22\n\n\n0.01\n\n\n0.00\n\n\n1.01\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.03)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGAM\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0.69\n\n\n0.22\n\n\n0.01\n\n\n0.01\n\n\n1.01\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.03)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGAMSEL\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0.69\n\n\n0.22\n\n\n0.01\n\n\n0.01\n\n\n0.93\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.03)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComparison of metrics for models chosen based on AUC, on Brier Score, on ICI, or on KL divergence (DGP 3, 0 noise variables)\n\n\n\n\n\n\n\n\n\n\n\n\n\nAUC*\n\n\n\n\nBrier*\n\n\n\n\nICI*\n\n\n\n\nKL*\n\n\n\n\n\nScenario\n\n\nModel\n\n\nAUC\n\n\nBrier\n\n\nICI\n\n\nKL\n\n\nQuant. Ratio\n\n\nAUC\n\n\nBrier\n\n\nICI\n\n\nKL\n\n\nQuant. Ratio\n\n\nΔAUC\n\n\nΔBrier\n\n\nΔICI\n\n\nΔKL\n\n\nΔQR\n\n\nAUC\n\n\nBrier\n\n\nICI\n\n\nKL\n\n\nQuant. Ratio\n\n\nΔAUC\n\n\nΔBrier\n\n\nΔICI\n\n\nΔKL\n\n\nΔQR\n\n\nAUC\n\n\nBrier\n\n\nICI\n\n\nKL\n\n\nQuant. Ratio\n\n\nΔAUC\n\n\nΔBrier\n\n\nΔICI\n\n\nΔKL\n\n\nΔQR\n\n\n\n\n\n\n10\n\n\nTrees\n\n\n0.55\n\n\n0.24\n\n\n0.02\n\n\n1.22\n\n\n0.36\n\n\n0.55\n\n\n0.24\n\n\n0.01\n\n\n1.27\n\n\n0.35\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n0.06\n\n\n-0.01\n\n\n0.55\n\n\n0.24\n\n\n0.01\n\n\n1.33\n\n\n0.35\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n0.11\n\n\n-0.01\n\n\n0.52\n\n\n0.27\n\n\n0.13\n\n\n0.05\n\n\n1.00\n\n\n-0.03\n\n\n0.02\n\n\n0.11\n\n\n-1.17\n\n\n0.64\n\n\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.21)\n\n\n(0.05)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.22)\n\n\n(0.06)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.23)\n\n\n(0.05)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.02)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.02)\n\n\n(0.06)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(-0.20)\n\n\n(0.00)\n\n\n\n\nRandom Forests\n\n\n0.68\n\n\n0.22\n\n\n0.03\n\n\n0.16\n\n\n0.77\n\n\n0.68\n\n\n0.22\n\n\n0.01\n\n\n0.04\n\n\n0.90\n\n\n0.00\n\n\n0.00\n\n\n-0.02\n\n\n-0.11\n\n\n0.14\n\n\n0.58\n\n\n0.23\n\n\n0.01\n\n\n1.74\n\n\n0.42\n\n\n-0.10\n\n\n0.01\n\n\n-0.02\n\n\n1.58\n\n\n-0.35\n\n\n0.67\n\n\n0.22\n\n\n0.01\n\n\n0.01\n\n\n0.96\n\n\n-0.01\n\n\n0.00\n\n\n-0.02\n\n\n-0.15\n\n\n0.19\n\n\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.09)\n\n\n(0.10)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.02)\n\n\n(0.03)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(-0.01)\n\n\n(-0.08)\n\n\n(-0.06)\n\n\n(0.09)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(1.59)\n\n\n(0.45)\n\n\n(0.08)\n\n\n(0.01)\n\n\n(-0.01)\n\n\n(1.49)\n\n\n(0.36)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.03)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(-0.01)\n\n\n(-0.09)\n\n\n(-0.07)\n\n\n\n\nXGB\n\n\n0.68\n\n\n0.22\n\n\n0.01\n\n\n0.03\n\n\n0.91\n\n\n0.68\n\n\n0.22\n\n\n0.01\n\n\n0.02\n\n\n0.92\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n-0.01\n\n\n0.01\n\n\n0.67\n\n\n0.22\n\n\n0.01\n\n\n0.04\n\n\n0.90\n\n\n-0.01\n\n\n0.00\n\n\n0.00\n\n\n0.01\n\n\n-0.01\n\n\n0.68\n\n\n0.22\n\n\n0.01\n\n\n0.00\n\n\n1.00\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n-0.02\n\n\n0.09\n\n\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.02)\n\n\n(0.05)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.03)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(-0.01)\n\n\n(-0.01)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.03)\n\n\n(0.04)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.02)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(-0.02)\n\n\n(-0.03)\n\n\n\n\nGLM\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0.69\n\n\n0.22\n\n\n0.01\n\n\n0.00\n\n\n1.01\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.03)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGAM\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0.69\n\n\n0.22\n\n\n0.01\n\n\n0.01\n\n\n1.03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.01)\n\n\n(0.03)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGAMSEL\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0.69\n\n\n0.22\n\n\n0.01\n\n\n0.01\n\n\n0.93\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.03)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComparison of metrics for models chosen based on AUC, on Brier Score, on ICI, or on KL divergence (DGP 3, 10 noise variables)\n\n\n\n\n\n\n\n\n\n\n\n\n\nAUC*\n\n\n\n\nBrier*\n\n\n\n\nICI*\n\n\n\n\nKL*\n\n\n\n\n\nScenario\n\n\nModel\n\n\nAUC\n\n\nBrier\n\n\nICI\n\n\nKL\n\n\nQuant. Ratio\n\n\nAUC\n\n\nBrier\n\n\nICI\n\n\nKL\n\n\nQuant. Ratio\n\n\nΔAUC\n\n\nΔBrier\n\n\nΔICI\n\n\nΔKL\n\n\nΔQR\n\n\nAUC\n\n\nBrier\n\n\nICI\n\n\nKL\n\n\nQuant. Ratio\n\n\nΔAUC\n\n\nΔBrier\n\n\nΔICI\n\n\nΔKL\n\n\nΔQR\n\n\nAUC\n\n\nBrier\n\n\nICI\n\n\nKL\n\n\nQuant. Ratio\n\n\nΔAUC\n\n\nΔBrier\n\n\nΔICI\n\n\nΔKL\n\n\nΔQR\n\n\n\n\n\n\n11\n\n\nTrees\n\n\n0.55\n\n\n0.24\n\n\n0.02\n\n\n1.22\n\n\n0.36\n\n\n0.55\n\n\n0.24\n\n\n0.01\n\n\n1.27\n\n\n0.36\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n0.06\n\n\n0.00\n\n\n0.55\n\n\n0.24\n\n\n0.01\n\n\n1.34\n\n\n0.35\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n0.12\n\n\n-0.01\n\n\n0.52\n\n\n0.27\n\n\n0.13\n\n\n0.05\n\n\n1.00\n\n\n-0.03\n\n\n0.02\n\n\n0.11\n\n\n-1.17\n\n\n0.64\n\n\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.20)\n\n\n(0.06)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.22)\n\n\n(0.06)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.24)\n\n\n(0.05)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.03)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.02)\n\n\n(0.06)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(-0.19)\n\n\n(0.00)\n\n\n\n\nRandom Forests\n\n\n0.68\n\n\n0.22\n\n\n0.04\n\n\n0.33\n\n\n0.63\n\n\n0.68\n\n\n0.22\n\n\n0.03\n\n\n0.22\n\n\n0.69\n\n\n0.00\n\n\n0.00\n\n\n-0.01\n\n\n-0.10\n\n\n0.06\n\n\n0.50\n\n\n0.24\n\n\n0.01\n\n\n3.20\n\n\n0.00\n\n\n-0.17\n\n\n0.02\n\n\n-0.03\n\n\n2.87\n\n\n-0.63\n\n\n0.67\n\n\n0.22\n\n\n0.03\n\n\n0.19\n\n\n0.71\n\n\n0.00\n\n\n0.00\n\n\n-0.01\n\n\n-0.14\n\n\n0.07\n\n\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.10)\n\n\n(0.06)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.03)\n\n\n(0.02)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(-0.07)\n\n\n(-0.03)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.04)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(-0.01)\n\n\n(-0.06)\n\n\n(-0.06)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.03)\n\n\n(0.02)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(-0.07)\n\n\n(-0.03)\n\n\n\n\nXGB\n\n\n0.68\n\n\n0.22\n\n\n0.01\n\n\n0.05\n\n\n0.87\n\n\n0.68\n\n\n0.22\n\n\n0.01\n\n\n0.04\n\n\n0.89\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n-0.01\n\n\n0.02\n\n\n0.67\n\n\n0.22\n\n\n0.01\n\n\n0.05\n\n\n0.89\n\n\n-0.01\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n0.02\n\n\n0.67\n\n\n0.22\n\n\n0.02\n\n\n0.00\n\n\n1.01\n\n\n-0.01\n\n\n0.00\n\n\n0.00\n\n\n-0.05\n\n\n0.14\n\n\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.03)\n\n\n(0.05)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.02)\n\n\n(0.04)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(-0.01)\n\n\n(-0.01)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.04)\n\n\n(0.05)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.02)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(-0.02)\n\n\n(-0.03)\n\n\n\n\nGLM\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0.69\n\n\n0.22\n\n\n0.01\n\n\n0.01\n\n\n1.03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.03)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGAM\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0.67\n\n\n0.22\n\n\n0.03\n\n\n0.03\n\n\n1.11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.01)\n\n\n(0.03)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGAMSEL\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0.69\n\n\n0.22\n\n\n0.01\n\n\n0.01\n\n\n0.93\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.03)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComparison of metrics for models chosen based on AUC, on Brier Score, on ICI, or on KL divergence (DGP 3, 50 noise variables)\n\n\n\n\n\n\n\n\n\n\n\n\n\nAUC*\n\n\n\n\nBrier*\n\n\n\n\nICI*\n\n\n\n\nKL*\n\n\n\n\n\nScenario\n\n\nModel\n\n\nAUC\n\n\nBrier\n\n\nICI\n\n\nKL\n\n\nQuant. Ratio\n\n\nAUC\n\n\nBrier\n\n\nICI\n\n\nKL\n\n\nQuant. Ratio\n\n\nΔAUC\n\n\nΔBrier\n\n\nΔICI\n\n\nΔKL\n\n\nΔQR\n\n\nAUC\n\n\nBrier\n\n\nICI\n\n\nKL\n\n\nQuant. Ratio\n\n\nΔAUC\n\n\nΔBrier\n\n\nΔICI\n\n\nΔKL\n\n\nΔQR\n\n\nAUC\n\n\nBrier\n\n\nICI\n\n\nKL\n\n\nQuant. Ratio\n\n\nΔAUC\n\n\nΔBrier\n\n\nΔICI\n\n\nΔKL\n\n\nΔQR\n\n\n\n\n\n\n12\n\n\nTrees\n\n\n0.55\n\n\n0.24\n\n\n0.02\n\n\n1.24\n\n\n0.36\n\n\n0.55\n\n\n0.24\n\n\n0.01\n\n\n1.29\n\n\n0.35\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n0.06\n\n\n0.00\n\n\n0.55\n\n\n0.24\n\n\n0.01\n\n\n1.34\n\n\n0.35\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n0.10\n\n\n-0.01\n\n\n0.52\n\n\n0.27\n\n\n0.13\n\n\n0.05\n\n\n1.00\n\n\n-0.03\n\n\n0.02\n\n\n0.11\n\n\n-1.19\n\n\n0.65\n\n\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.23)\n\n\n(0.05)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.22)\n\n\n(0.06)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(-0.01)\n\n\n(0.01)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.23)\n\n\n(0.05)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.02)\n\n\n(0.06)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(-0.22)\n\n\n(0.01)\n\n\n\n\nRandom Forests\n\n\n0.67\n\n\n0.23\n\n\n0.06\n\n\n0.68\n\n\n0.46\n\n\n0.67\n\n\n0.23\n\n\n0.05\n\n\n0.49\n\n\n0.53\n\n\n0.00\n\n\n0.00\n\n\n-0.01\n\n\n-0.19\n\n\n0.07\n\n\n0.50\n\n\n0.24\n\n\n0.01\n\n\n3.20\n\n\n0.00\n\n\n-0.17\n\n\n0.02\n\n\n-0.05\n\n\n2.52\n\n\n-0.46\n\n\n0.67\n\n\n0.23\n\n\n0.05\n\n\n0.46\n\n\n0.54\n\n\n-0.01\n\n\n0.00\n\n\n-0.02\n\n\n-0.22\n\n\n0.08\n\n\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.12)\n\n\n(0.04)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.04)\n\n\n(0.02)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(-0.08)\n\n\n(-0.02)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.04)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(-0.08)\n\n\n(-0.04)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.04)\n\n\n(0.02)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(-0.09)\n\n\n(-0.03)\n\n\n\n\nXGB\n\n\n0.68\n\n\n0.22\n\n\n0.01\n\n\n0.06\n\n\n0.85\n\n\n0.68\n\n\n0.22\n\n\n0.01\n\n\n0.05\n\n\n0.87\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n-0.02\n\n\n0.02\n\n\n0.67\n\n\n0.22\n\n\n0.01\n\n\n0.06\n\n\n0.88\n\n\n-0.01\n\n\n0.00\n\n\n-0.01\n\n\n-0.01\n\n\n0.03\n\n\n0.67\n\n\n0.22\n\n\n0.02\n\n\n0.00\n\n\n1.01\n\n\n-0.01\n\n\n0.00\n\n\n0.00\n\n\n-0.06\n\n\n0.16\n\n\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.03)\n\n\n(0.04)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.02)\n\n\n(0.03)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(-0.01)\n\n\n(-0.01)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.05)\n\n\n(0.05)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.02)\n\n\n(0.01)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(-0.03)\n\n\n(-0.03)\n\n\n\n\nGLM\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0.68\n\n\n0.22\n\n\n0.02\n\n\n0.01\n\n\n1.05\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.01)\n\n\n(0.03)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGAM\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0.66\n\n\n0.23\n\n\n0.05\n\n\n0.09\n\n\n1.20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.02)\n\n\n(0.03)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGAMSEL\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0.69\n\n\n0.22\n\n\n0.01\n\n\n0.01\n\n\n0.93\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.03)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComparison of metrics for models chosen based on AUC, on Brier Score, on ICI, or on KL divergence (DGP 3, 100 noise variables)\n\n\n\n\n\n\n\n\n01050100\n\n\n\n\n\n\n\n\n\n\n\nAUC*\n\n\n\n\nBrier*\n\n\n\n\nICI*\n\n\n\n\nKL*\n\n\n\n\n\nScenario\n\n\nModel\n\n\nAUC\n\n\nBrier\n\n\nICI\n\n\nKL\n\n\nQuant. Ratio\n\n\nAUC\n\n\nBrier\n\n\nICI\n\n\nKL\n\n\nQuant. Ratio\n\n\nΔAUC\n\n\nΔBrier\n\n\nΔICI\n\n\nΔKL\n\n\nΔQR\n\n\nAUC\n\n\nBrier\n\n\nICI\n\n\nKL\n\n\nQuant. Ratio\n\n\nΔAUC\n\n\nΔBrier\n\n\nΔICI\n\n\nΔKL\n\n\nΔQR\n\n\nAUC\n\n\nBrier\n\n\nICI\n\n\nKL\n\n\nQuant. Ratio\n\n\nΔAUC\n\n\nΔBrier\n\n\nΔICI\n\n\nΔKL\n\n\nΔQR\n\n\n\n\n\n\n13\n\n\nTrees\n\n\n0.65\n\n\n0.23\n\n\n0.02\n\n\n0.49\n\n\n0.69\n\n\n0.65\n\n\n0.23\n\n\n0.02\n\n\n0.53\n\n\n0.68\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n0.04\n\n\n0.00\n\n\n0.64\n\n\n0.23\n\n\n0.01\n\n\n1.23\n\n\n0.63\n\n\n-0.01\n\n\n0.00\n\n\n-0.01\n\n\n0.74\n\n\n-0.06\n\n\n0.62\n\n\n0.25\n\n\n0.11\n\n\n0.05\n\n\n1.01\n\n\n-0.04\n\n\n0.02\n\n\n0.09\n\n\n-0.44\n\n\n0.33\n\n\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.15)\n\n\n(0.05)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.16)\n\n\n(0.06)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.29)\n\n\n(0.05)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.14)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.01)\n\n\n(0.01)\n\n\n(0.01)\n\n\n(0.05)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(-0.14)\n\n\n(-0.01)\n\n\n\n\nRandom Forests\n\n\n0.74\n\n\n0.21\n\n\n0.01\n\n\n0.04\n\n\n0.95\n\n\n0.74\n\n\n0.21\n\n\n0.01\n\n\n0.04\n\n\n0.95\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n0.58\n\n\n0.24\n\n\n0.01\n\n\n2.52\n\n\n0.31\n\n\n-0.16\n\n\n0.03\n\n\n0.00\n\n\n2.48\n\n\n-0.64\n\n\n0.74\n\n\n0.21\n\n\n0.02\n\n\n0.01\n\n\n1.03\n\n\n-0.01\n\n\n0.00\n\n\n0.01\n\n\n-0.03\n\n\n0.08\n\n\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.02)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.02)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.11)\n\n\n(0.02)\n\n\n(0.00)\n\n\n(1.75)\n\n\n(0.45)\n\n\n(0.11)\n\n\n(0.02)\n\n\n(0.00)\n\n\n(1.73)\n\n\n(0.42)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.02)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(-0.01)\n\n\n(-0.01)\n\n\n\n\nXGB\n\n\n0.75\n\n\n0.20\n\n\n0.01\n\n\n0.04\n\n\n0.96\n\n\n0.75\n\n\n0.20\n\n\n0.01\n\n\n0.04\n\n\n0.96\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n0.74\n\n\n0.21\n\n\n0.01\n\n\n0.05\n\n\n0.94\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n0.01\n\n\n-0.01\n\n\n0.74\n\n\n0.21\n\n\n0.02\n\n\n0.01\n\n\n1.03\n\n\n0.00\n\n\n0.00\n\n\n0.01\n\n\n-0.03\n\n\n0.07\n\n\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.03)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.02)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(-0.01)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.03)\n\n\n(0.03)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.02)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(-0.01)\n\n\n(-0.01)\n\n\n\n\nGLM\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0.68\n\n\n0.23\n\n\n0.01\n\n\n0.27\n\n\n0.67\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.03)\n\n\n(0.02)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGAM\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0.73\n\n\n0.21\n\n\n0.01\n\n\n0.08\n\n\n0.88\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.02)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGAMSEL\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0.73\n\n\n0.21\n\n\n0.02\n\n\n0.14\n\n\n0.79\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.02)\n\n\n(0.02)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComparison of metrics for models chosen based on AUC, on Brier Score, on ICI, or on KL divergence (DGP 4, 0 noise variables)\n\n\n\n\n\n\n\n\n\n\n\n\n\nAUC*\n\n\n\n\nBrier*\n\n\n\n\nICI*\n\n\n\n\nKL*\n\n\n\n\n\nScenario\n\n\nModel\n\n\nAUC\n\n\nBrier\n\n\nICI\n\n\nKL\n\n\nQuant. Ratio\n\n\nAUC\n\n\nBrier\n\n\nICI\n\n\nKL\n\n\nQuant. Ratio\n\n\nΔAUC\n\n\nΔBrier\n\n\nΔICI\n\n\nΔKL\n\n\nΔQR\n\n\nAUC\n\n\nBrier\n\n\nICI\n\n\nKL\n\n\nQuant. Ratio\n\n\nΔAUC\n\n\nΔBrier\n\n\nΔICI\n\n\nΔKL\n\n\nΔQR\n\n\nAUC\n\n\nBrier\n\n\nICI\n\n\nKL\n\n\nQuant. Ratio\n\n\nΔAUC\n\n\nΔBrier\n\n\nΔICI\n\n\nΔKL\n\n\nΔQR\n\n\n\n\n\n\n14\n\n\nTrees\n\n\n0.65\n\n\n0.23\n\n\n0.02\n\n\n0.48\n\n\n0.69\n\n\n0.65\n\n\n0.23\n\n\n0.02\n\n\n0.53\n\n\n0.69\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n0.05\n\n\n0.00\n\n\n0.64\n\n\n0.23\n\n\n0.01\n\n\n1.23\n\n\n0.63\n\n\n-0.01\n\n\n0.00\n\n\n-0.01\n\n\n0.75\n\n\n-0.06\n\n\n0.62\n\n\n0.25\n\n\n0.11\n\n\n0.05\n\n\n1.01\n\n\n-0.04\n\n\n0.02\n\n\n0.09\n\n\n-0.44\n\n\n0.33\n\n\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.14)\n\n\n(0.06)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.15)\n\n\n(0.06)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.29)\n\n\n(0.05)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.15)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.01)\n\n\n(0.01)\n\n\n(0.01)\n\n\n(0.05)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(-0.13)\n\n\n(-0.01)\n\n\n\n\nRandom Forests\n\n\n0.74\n\n\n0.21\n\n\n0.01\n\n\n0.06\n\n\n0.93\n\n\n0.74\n\n\n0.21\n\n\n0.01\n\n\n0.05\n\n\n0.93\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n0.59\n\n\n0.24\n\n\n0.01\n\n\n2.45\n\n\n0.32\n\n\n-0.16\n\n\n0.03\n\n\n0.00\n\n\n2.39\n\n\n-0.60\n\n\n0.73\n\n\n0.21\n\n\n0.02\n\n\n0.01\n\n\n0.96\n\n\n-0.01\n\n\n0.00\n\n\n0.00\n\n\n-0.04\n\n\n0.03\n\n\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.02)\n\n\n(0.03)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.02)\n\n\n(0.02)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(-0.01)\n\n\n(0.11)\n\n\n(0.02)\n\n\n(0.00)\n\n\n(1.77)\n\n\n(0.44)\n\n\n(0.11)\n\n\n(0.02)\n\n\n(0.00)\n\n\n(1.75)\n\n\n(0.42)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.02)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(-0.01)\n\n\n(-0.01)\n\n\n\n\nXGB\n\n\n0.74\n\n\n0.21\n\n\n0.01\n\n\n0.08\n\n\n0.89\n\n\n0.74\n\n\n0.21\n\n\n0.01\n\n\n0.07\n\n\n0.90\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n-0.01\n\n\n0.02\n\n\n0.74\n\n\n0.21\n\n\n0.01\n\n\n0.06\n\n\n0.92\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n-0.02\n\n\n0.04\n\n\n0.73\n\n\n0.21\n\n\n0.03\n\n\n0.02\n\n\n1.04\n\n\n-0.01\n\n\n0.00\n\n\n0.02\n\n\n-0.07\n\n\n0.15\n\n\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.02)\n\n\n(0.03)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.02)\n\n\n(0.02)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(-0.01)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.02)\n\n\n(0.03)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.02)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(-0.02)\n\n\n(-0.01)\n\n\n\n\nGLM\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0.68\n\n\n0.23\n\n\n0.01\n\n\n0.26\n\n\n0.67\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.03)\n\n\n(0.02)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGAM\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0.73\n\n\n0.21\n\n\n0.01\n\n\n0.07\n\n\n0.90\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.02)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGAMSEL\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0.73\n\n\n0.21\n\n\n0.02\n\n\n0.14\n\n\n0.79\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.02)\n\n\n(0.02)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComparison of metrics for models chosen based on AUC, on Brier Score, on ICI, or on KL divergence (DGP 4, 10 noise variables)\n\n\n\n\n\n\n\n\n\n\n\n\n\nAUC*\n\n\n\n\nBrier*\n\n\n\n\nICI*\n\n\n\n\nKL*\n\n\n\n\n\nScenario\n\n\nModel\n\n\nAUC\n\n\nBrier\n\n\nICI\n\n\nKL\n\n\nQuant. Ratio\n\n\nAUC\n\n\nBrier\n\n\nICI\n\n\nKL\n\n\nQuant. Ratio\n\n\nΔAUC\n\n\nΔBrier\n\n\nΔICI\n\n\nΔKL\n\n\nΔQR\n\n\nAUC\n\n\nBrier\n\n\nICI\n\n\nKL\n\n\nQuant. Ratio\n\n\nΔAUC\n\n\nΔBrier\n\n\nΔICI\n\n\nΔKL\n\n\nΔQR\n\n\nAUC\n\n\nBrier\n\n\nICI\n\n\nKL\n\n\nQuant. Ratio\n\n\nΔAUC\n\n\nΔBrier\n\n\nΔICI\n\n\nΔKL\n\n\nΔQR\n\n\n\n\n\n\n15\n\n\nTrees\n\n\n0.65\n\n\n0.23\n\n\n0.02\n\n\n0.49\n\n\n0.69\n\n\n0.65\n\n\n0.23\n\n\n0.02\n\n\n0.53\n\n\n0.68\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n0.04\n\n\n-0.01\n\n\n0.64\n\n\n0.23\n\n\n0.01\n\n\n1.23\n\n\n0.63\n\n\n-0.01\n\n\n0.00\n\n\n-0.01\n\n\n0.74\n\n\n-0.06\n\n\n0.62\n\n\n0.25\n\n\n0.11\n\n\n0.05\n\n\n1.01\n\n\n-0.04\n\n\n0.02\n\n\n0.09\n\n\n-0.44\n\n\n0.32\n\n\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.15)\n\n\n(0.05)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.16)\n\n\n(0.06)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.31)\n\n\n(0.05)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.15)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.01)\n\n\n(0.01)\n\n\n(0.01)\n\n\n(0.05)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(-0.14)\n\n\n(-0.01)\n\n\n\n\nRandom Forests\n\n\n0.73\n\n\n0.21\n\n\n0.04\n\n\n0.34\n\n\n0.66\n\n\n0.73\n\n\n0.21\n\n\n0.04\n\n\n0.32\n\n\n0.67\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n-0.02\n\n\n0.01\n\n\n0.50\n\n\n0.25\n\n\n0.01\n\n\n3.74\n\n\n0.00\n\n\n-0.23\n\n\n0.04\n\n\n-0.04\n\n\n3.40\n\n\n-0.66\n\n\n0.73\n\n\n0.21\n\n\n0.04\n\n\n0.30\n\n\n0.67\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n-0.04\n\n\n0.01\n\n\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.04)\n\n\n(0.02)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.03)\n\n\n(0.02)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(-0.01)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.05)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(-0.02)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.03)\n\n\n(0.02)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(-0.02)\n\n\n(0.00)\n\n\n\n\nXGB\n\n\n0.74\n\n\n0.21\n\n\n0.02\n\n\n0.12\n\n\n0.84\n\n\n0.74\n\n\n0.21\n\n\n0.01\n\n\n0.10\n\n\n0.87\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n-0.02\n\n\n0.02\n\n\n0.73\n\n\n0.21\n\n\n0.01\n\n\n0.07\n\n\n0.91\n\n\n-0.01\n\n\n0.00\n\n\n-0.01\n\n\n-0.06\n\n\n0.07\n\n\n0.72\n\n\n0.21\n\n\n0.03\n\n\n0.02\n\n\n1.03\n\n\n-0.02\n\n\n0.01\n\n\n0.02\n\n\n-0.11\n\n\n0.19\n\n\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.03)\n\n\n(0.03)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.02)\n\n\n(0.03)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(-0.01)\n\n\n(-0.01)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.02)\n\n\n(0.03)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(-0.01)\n\n\n(-0.01)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.02)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(-0.03)\n\n\n(-0.02)\n\n\n\n\nGLM\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0.67\n\n\n0.23\n\n\n0.01\n\n\n0.24\n\n\n0.69\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.03)\n\n\n(0.02)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGAM\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0.72\n\n\n0.22\n\n\n0.02\n\n\n0.04\n\n\n0.94\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.02)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGAMSEL\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0.73\n\n\n0.21\n\n\n0.02\n\n\n0.14\n\n\n0.79\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.02)\n\n\n(0.02)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComparison of metrics for models chosen based on AUC, on Brier Score, on ICI, or on KL divergence (DGP 4, 50 noise variables)\n\n\n\n\n\n\n\n\n\n\n\n\n\nAUC*\n\n\n\n\nBrier*\n\n\n\n\nICI*\n\n\n\n\nKL*\n\n\n\n\n\nScenario\n\n\nModel\n\n\nAUC\n\n\nBrier\n\n\nICI\n\n\nKL\n\n\nQuant. Ratio\n\n\nAUC\n\n\nBrier\n\n\nICI\n\n\nKL\n\n\nQuant. Ratio\n\n\nΔAUC\n\n\nΔBrier\n\n\nΔICI\n\n\nΔKL\n\n\nΔQR\n\n\nAUC\n\n\nBrier\n\n\nICI\n\n\nKL\n\n\nQuant. Ratio\n\n\nΔAUC\n\n\nΔBrier\n\n\nΔICI\n\n\nΔKL\n\n\nΔQR\n\n\nAUC\n\n\nBrier\n\n\nICI\n\n\nKL\n\n\nQuant. Ratio\n\n\nΔAUC\n\n\nΔBrier\n\n\nΔICI\n\n\nΔKL\n\n\nΔQR\n\n\n\n\n\n\n16\n\n\nTrees\n\n\n0.65\n\n\n0.23\n\n\n0.02\n\n\n0.48\n\n\n0.69\n\n\n0.65\n\n\n0.23\n\n\n0.02\n\n\n0.53\n\n\n0.68\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n0.05\n\n\n0.00\n\n\n0.64\n\n\n0.23\n\n\n0.01\n\n\n1.23\n\n\n0.63\n\n\n-0.01\n\n\n0.00\n\n\n-0.01\n\n\n0.75\n\n\n-0.05\n\n\n0.62\n\n\n0.25\n\n\n0.11\n\n\n0.05\n\n\n1.01\n\n\n-0.04\n\n\n0.02\n\n\n0.09\n\n\n-0.44\n\n\n0.33\n\n\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.15)\n\n\n(0.06)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.16)\n\n\n(0.06)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.29)\n\n\n(0.05)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.14)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.01)\n\n\n(0.01)\n\n\n(0.01)\n\n\n(0.05)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(-0.14)\n\n\n(-0.01)\n\n\n\n\nRandom Forests\n\n\n0.72\n\n\n0.22\n\n\n0.07\n\n\n0.72\n\n\n0.48\n\n\n0.72\n\n\n0.22\n\n\n0.07\n\n\n0.67\n\n\n0.49\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n-0.05\n\n\n0.01\n\n\n0.50\n\n\n0.25\n\n\n0.01\n\n\n3.74\n\n\n0.00\n\n\n-0.21\n\n\n0.03\n\n\n-0.07\n\n\n3.02\n\n\n-0.48\n\n\n0.72\n\n\n0.22\n\n\n0.07\n\n\n0.65\n\n\n0.49\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n-0.06\n\n\n0.01\n\n\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.05)\n\n\n(0.02)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.04)\n\n\n(0.02)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(-0.01)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.05)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(-0.02)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.04)\n\n\n(0.02)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(-0.01)\n\n\n(0.00)\n\n\n\n\nXGB\n\n\n0.74\n\n\n0.21\n\n\n0.02\n\n\n0.14\n\n\n0.82\n\n\n0.74\n\n\n0.21\n\n\n0.02\n\n\n0.11\n\n\n0.85\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n-0.03\n\n\n0.02\n\n\n0.73\n\n\n0.21\n\n\n0.01\n\n\n0.07\n\n\n0.90\n\n\n-0.01\n\n\n0.00\n\n\n-0.01\n\n\n-0.07\n\n\n0.08\n\n\n0.72\n\n\n0.22\n\n\n0.04\n\n\n0.02\n\n\n1.03\n\n\n-0.02\n\n\n0.01\n\n\n0.02\n\n\n-0.12\n\n\n0.21\n\n\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.04)\n\n\n(0.04)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.03)\n\n\n(0.03)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(-0.01)\n\n\n(-0.01)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.03)\n\n\n(0.03)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(-0.01)\n\n\n(-0.01)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(-0.03)\n\n\n(-0.02)\n\n\n\n\nGLM\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0.67\n\n\n0.23\n\n\n0.01\n\n\n0.22\n\n\n0.71\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.00)\n\n\n(0.03)\n\n\n(0.02)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGAM\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0.71\n\n\n0.22\n\n\n0.04\n\n\n0.02\n\n\n1.00\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.02)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGAMSEL\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0.73\n\n\n0.21\n\n\n0.02\n\n\n0.14\n\n\n0.79\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(0.01)\n\n\n(0.00)\n\n\n(0.01)\n\n\n(0.02)\n\n\n(0.02)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComparison of metrics for models chosen based on AUC, on Brier Score, on ICI, or on KL divergence (DGP 4, 100 noise variables)",
    "crumbs": [
      "Simulated Data",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Comparison of Models</span>"
    ]
  },
  {
    "objectID": "book_real_example.html",
    "href": "book_real_example.html",
    "title": "13  Priors: Illustration",
    "section": "",
    "text": "13.1 Raw Data\nTo illustrate the process, we use the spambase dataset (available on UCI Machine Learning Repository). The dataset contains 4,601 rows. The target variable, is_spam will be explained using the 57 continuous predictors.\nThe dataset can be downloaded as follows:\nif (!dir.exists(\"data\")) dir.create(\"data\")\ndownload.file(\n  url = \"https://archive.ics.uci.edu/static/public/94/spambase.zip\", \n  destfile = \"data/spambase.zip\"\n)\nThe names of the columns are given in the spambase.names file in that archive.\n# This chunk is not run\ninfo_data &lt;- scan(\n  unz(\"data/spambase.zip\", \"spambase.names\"), what = \"character\", sep = \"\\n\"\n)\n# Print the names for this dataset (not very convenient...)\nstr_extract(info_data[31:length(info_data)], \"^(.*):\") |&gt; \n  str_remove(\":$\") |&gt; \n  (\\(.x) str_c('\"', .x, '\",'))() |&gt; \n  cat()\nThen, we can import the dataset:\ndataset &lt;- read_csv(\n  file = unz(\"data/spambase.zip\", \"spambase.data\"),\n  col_names = c(\n    \"word_freq_make\", \"word_freq_address\", \"word_freq_all\", \"word_freq_3d\",\n    \"word_freq_our\", \"word_freq_over\", \"word_freq_remove\", \"word_freq_internet\",\n    \"word_freq_order\", \"word_freq_mail\", \"word_freq_receive\", \"word_freq_will\",\n    \"word_freq_people\", \"word_freq_report\", \"word_freq_addresses\", \n    \"word_freq_free\", \"word_freq_business\", \"word_freq_email\", \"word_freq_you\",\n    \"word_freq_credit\", \"word_freq_your\", \"word_freq_font\", \"word_freq_000\",\n    \"word_freq_money\", \"word_freq_hp\", \"word_freq_hpl\", \"word_freq_george\",\n    \"word_freq_650\", \"word_freq_lab\", \"word_freq_labs\", \"word_freq_telnet\",\n    \"word_freq_857\", \"word_freq_data\", \"word_freq_415\", \"word_freq_85\",\n    \"word_freq_technology\", \"word_freq_1999\", \"word_freq_parts\", \"word_freq_pm\",\n    \"word_freq_direct\", \"word_freq_cs\", \"word_freq_meeting\", \n    \"word_freq_original\", \"word_freq_project\", \"word_freq_re\", \"word_freq_edu\",\n    \"word_freq_table\", \"word_freq_conference\", \"char_freq_;\", \"char_freq_(\",\n    \"char_freq_[\", \"char_freq_!\", \"char_freq_$\", \"char_freq_#\",\n    \"capital_run_length_average\", \"capital_run_length_longest\",\n    \"capital_run_length_total\", \"is_spam\"\n  )\n)\n\nRows: 4601 Columns: 58\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (58): word_freq_make, word_freq_address, word_freq_all, word_freq_3d, wo...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nThe target variable is is_spam.\ntarget_name &lt;- \"is_spam\"",
    "crumbs": [
      "Real-world Data",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Priors: Illustration</span>"
    ]
  },
  {
    "objectID": "book_real_example.html#data-pre-processing",
    "href": "book_real_example.html#data-pre-processing",
    "title": "13  Priors: Illustration",
    "section": "13.2 Data Pre-processing",
    "text": "13.2 Data Pre-processing\nWe define two functions to pre-process the data. The first one, split_train_test() simply split the dataset into two subsets: one for training the models (train) and another one for testing the models (test).\n\n#' Split dataset into train and test set\n#'\n#' @param data dataset\n#' @param prop_train proportion in the train test (default to .8)\n#' @param seed desired seed (default to `NULL`)\n#'\n#' @returns a list with two elements: the train set, the test set\nsplit_train_test &lt;- function(data,\n                             prop_train = .8,\n                             seed = NULL) {\n  if (!is.null(seed)) set.seed(seed)\n  size_train &lt;- round(prop_train * nrow(data))\n  ind_sample &lt;- sample(1:nrow(data), replace = FALSE, size = size_train)\n\n  list(\n    train = data |&gt; dplyr::slice(ind_sample),\n    test = data |&gt; dplyr::slice(-ind_sample)\n  )\n}\n\nWith the current dataset:\n\ndata &lt;- split_train_test(data = dataset, prop_train = .8, seed = 1234)\nnames(data)\n\n[1] \"train\" \"test\" \n\n\nSome of the models we use need the data to be numerical. We thus build a function, encode_dataset() that transforms the categorical columns into sets of dummy variables. For each categorical variable, we remove one of the levels to avoid colinearity in the predictor matrix. This step is made using the convenient functions from the {recipes} package. In addition, the spline function from the {gam} package does not support variables with names that do not respect the R naming conventions. We thus rename all the variables and keep track of the changes.\nThe encode_dataset() returns a list with five elements:\n\ntrain: the train set where categorical variables have been transformed into dummy variables\ntest: the test set where categorical variables have been transformed into dummy variables\ninitial_covariate_names: vector of names of all explanatory variables\ncateg_names: vector of new names of categorical variables (if any)\ncovariate_names: vector of new names of all explanatory variables (including categorigal ones).\n\n\n#' One-hot encoding, and renaming variables to avoid naming that do not respect\n#' r old naming conventions\n#'\n#' @param data_train train set\n#' @param data_test test set\n#' @param target_name name of the target (response) variable\n#' @param intercept should a column for an intercept be added? Default to\n#'  `FALSE`\n#'\n#' @returns list with five elements:\n#'  - `train`: train set\n#'  - `test`: test set\n#'  - `initial_covariate_names`: vector of names of all explanatory variables\n#'  - `categ_names`: vector of new names of categorical variables (if any)\n#'  - `covariate_names`: vector of new names of all explanatory variables (including\n#'     categorical ones).\nencode_dataset &lt;- function(data_train,\n                           data_test,\n                           target_name,\n                           intercept = FALSE) {\n\n  col_names &lt;- colnames(data_train)\n  col_names_covariates &lt;- col_names[-which(col_names == target_name)]\n  new_names_covariates &lt;- str_c(\"X_\", 1:length(col_names_covariates))\n  data_train &lt;- data_train |&gt;\n    rename_with(.cols = all_of(col_names_covariates), .fn = ~new_names_covariates)\n  data_test &lt;- data_test |&gt;\n    rename_with(.cols = all_of(col_names_covariates), .fn = ~new_names_covariates)\n\n  data_rec &lt;- recipes::recipe(\n    formula(str_c(target_name, \" ~ .\")),\n    data = data_train\n  )\n\n  ref_cell &lt;- data_rec |&gt; recipes::step_dummy(\n    recipes::all_nominal(), -recipes::all_outcomes(),\n    one_hot = TRUE\n  ) |&gt;\n    recipes::prep(training = data_train)\n\n  X_train_dmy &lt;- recipes::bake(ref_cell, new_data = data_train)\n  X_test_dmy  &lt;- recipes::bake(ref_cell, new_data = data_test)\n\n  # Identify categorical variables\n  # Bake the recipe to apply the transformation\n  df_transformed &lt;- recipes::bake(ref_cell, new_data = NULL)\n  # Get the names of the transformed data\n  new_names &lt;- names(X_train_dmy)\n  original_vars &lt;- names(data_train)\n  categ_names &lt;- setdiff(new_names, original_vars)\n  covariate_names &lt;- colnames(X_train_dmy)\n  covariate_names &lt;- covariate_names[!covariate_names == target_name]\n  categ_names &lt;- categ_names[!categ_names == target_name]\n  list(\n    train = X_train_dmy,\n    test = X_test_dmy,\n    initial_covariate_names = col_names_covariates,\n    categ_names = categ_names,\n    covariate_names = covariate_names\n  )\n}\n\nLet us use the encode_dataset() function to rename the columns here. As there is no categorical variable among the predictors, no dummy variable will be created.\n\ndata_dmy &lt;- encode_dataset(\n  data_train = data$train,\n  data_test = data$test,\n  target_name = target_name,\n  intercept = FALSE\n)",
    "crumbs": [
      "Real-world Data",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Priors: Illustration</span>"
    ]
  },
  {
    "objectID": "book_real_example.html#estimation-functions",
    "href": "book_real_example.html#estimation-functions",
    "title": "13  Priors: Illustration",
    "section": "13.3 Estimation Functions",
    "text": "13.3 Estimation Functions\n\n13.3.1 GLM\nLet us estimate the probability that the event occurs (the email is a spam) using a Generalized Linear Model with a logistic link function.\nWe first build the formula:\n\nform &lt;- str_c(target_name, \"~.\") |&gt; as.formula()\n\nThen, we fit the model:\n\nfit &lt;- glm(form, data = data_dmy$train, family = \"binomial\")\n\nLastly, we can get the predicted scores:\n\nscores_train &lt;- predict(fit, newdata = data_dmy$train, type = \"response\")\nscores_test &lt;- predict(fit, newdata = data_dmy$test, type = \"response\")\n\nWe encompass these steps in a helper function:\n\n#' Train a GLM-logistic model\n#'\n#' @param data_train train set\n#' @param data_test test set\n#' @param target_name name of the target (response) variable\n#' @param return_model if TRUE, the estimated model is returned\n#'\n#' @returns list with estimated scores on train set (`scores_train`) and on\n#'  test set (`scores_test`)\ntrain_glm &lt;- function(data_train,\n                      data_test,\n                      target_name,\n                      return_model = FALSE) {\n  # Encode dataset so that categorical variables become dummy variables\n  data_dmy &lt;- encode_dataset(\n    data_train = data_train,\n    data_test = data_test,\n    target_name = target_name,\n    intercept = FALSE\n  )\n  # Formula for the model\n  form &lt;- str_c(target_name, \"~.\") |&gt; as.formula()\n  # Estimation\n  fit &lt;- glm(form, data = data_dmy$train, family = \"binomial\")\n  # Scores on train and test set\n  scores_train &lt;- predict(fit, newdata = data_dmy$train, type = \"response\")\n  scores_test &lt;- predict(fit, newdata = data_dmy$test, type = \"response\")\n\n  if (return_model == TRUE) {\n    res &lt;- list(\n      scores_train = scores_train, \n      scores_test = scores_test, \n      fit = fit)\n  } else {\n    list(scores_train = scores_train, scores_test = scores_test, fit = NULL)\n  }\n}\n\nThis function can then be used in a very simple way:\n\nscores_glm &lt;- train_glm(\n    data_train = data$train, data_test = data$test, target_name = target_name\n  )\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\n\n\n\n13.3.2 GAM\nWe then estimate the probability that the event occurs (the email is a spam) using a Generalized Additive Model.\nWe first build the formula. Here, this is a tiny bit more complex than with the GLM. The model will contain smooth terms (for numerical variables) and linear terms (for categorical variables which, if present in the data, were encoded as dummy variables).\nWe identify the numerical variables\n\nnum_names &lt;- data_dmy$covariate_names[!data_dmy$covariate_names %in% data_dmy$categ_names]\nnum_names &lt;- num_names[!num_names %in% target_name]\n\nThen, we count the number of unique values for each variable. This step ensures that the smoothing parameter of the spline function applied to numerical variables is not larger than the number of unique values. We arbitrarily set the smoothing parameter to 6. But if the number of unique values for a variable is lower than this, then we use the number of unique values minus 1 as the smoothing parameter for that variable.\n\nspline_df &lt;- 6\nnum_nb_val &lt;- map_dbl(num_names, ~data_dmy$train |&gt; pull(.x) |&gt; unique() |&gt; length())\ndeg_num &lt;- ifelse(num_nb_val &lt; spline_df, num_nb_val, spline_df)\n\nThen, we can construct the formula object. We begin with the numerical variables:\n\nnum_term &lt;- str_c(\"s(\", num_names, \", df = \", deg_num, \")\", collapse = \" + \")\n\nNote: if there are no numerical variables, the num_term is simply set to NULL.\nIf there are categorical variables, we do not use a smoothing function for them.\n\nif (length(data_dmy$categ_names &gt; 0)) {\n  categ_terms &lt;- str_c(data_dmy$categ_names, collapse = \" + \")\n} else {\n  categ_terms &lt;- NULL\n}\n\nLastly, we can create the whole part of the formula which contains the predictors:\n\nform_terms &lt;- str_c(num_term, categ_terms, sep = \" + \")\n\nThe formula can eventually be created:\n\nform_gam &lt;- str_c(target_name, \" ~ \", form_terms) |&gt; as.formula()\nform_gam\n\nis_spam ~ s(X_1, df = 6) + s(X_2, df = 6) + s(X_3, df = 6) + \n    s(X_4, df = 6) + s(X_5, df = 6) + s(X_6, df = 6) + s(X_7, \n    df = 6) + s(X_8, df = 6) + s(X_9, df = 6) + s(X_10, df = 6) + \n    s(X_11, df = 6) + s(X_12, df = 6) + s(X_13, df = 6) + s(X_14, \n    df = 6) + s(X_15, df = 6) + s(X_16, df = 6) + s(X_17, df = 6) + \n    s(X_18, df = 6) + s(X_19, df = 6) + s(X_20, df = 6) + s(X_21, \n    df = 6) + s(X_22, df = 6) + s(X_23, df = 6) + s(X_24, df = 6) + \n    s(X_25, df = 6) + s(X_26, df = 6) + s(X_27, df = 6) + s(X_28, \n    df = 6) + s(X_29, df = 6) + s(X_30, df = 6) + s(X_31, df = 6) + \n    s(X_32, df = 6) + s(X_33, df = 6) + s(X_34, df = 6) + s(X_35, \n    df = 6) + s(X_36, df = 6) + s(X_37, df = 6) + s(X_38, df = 6) + \n    s(X_39, df = 6) + s(X_40, df = 6) + s(X_41, df = 6) + s(X_42, \n    df = 6) + s(X_43, df = 6) + s(X_44, df = 6) + s(X_45, df = 6) + \n    s(X_46, df = 6) + s(X_47, df = 6) + s(X_48, df = 6) + s(X_49, \n    df = 6) + s(X_50, df = 6) + s(X_51, df = 6) + s(X_52, df = 6) + \n    s(X_53, df = 6) + s(X_54, df = 6) + s(X_55, df = 6) + s(X_56, \n    df = 6) + s(X_57, df = 6)\n\n\nThen, we fit the model:\n\nfit &lt;- gam::gam(formula = form_gam, family = binomial, data = data_dmy$train)\n\nLastly, we can get the predicted scores:\n\nscores_train &lt;- predict(fit, newdata = data_dmy$train, type = \"response\")\nscores_test &lt;- predict(fit, newdata = data_dmy$test, type = \"response\")\n\nWe encompass these steps in a helper function:\n\n#' Train a GAM model\n#'\n#' @param data_train train set\n#' @param data_test test set\n#' @param target_name name of the target (response) variable\n#' @param spline_df degree of freedom for the splines\n#' @param return_model if TRUE, the estimated model is returned\n#'\n#' @returns list with estimated scores on train set (`scores_train`) and on\n#'  test set (`scores_test`)\ntrain_gam &lt;- function(data_train,\n                      data_test,\n                      target_name,\n                      spline_df = 5,\n                      return_model = FALSE) {\n  # Encode dataset so that categorical variables become dummy variables\n  data_dmy &lt;- encode_dataset(\n    data_train = data_train,\n    data_test = data_test,\n    target_name = target_name,\n    intercept = FALSE\n  )\n\n  # Formula for the model\n  ## Names of numerical variables\n  num_names &lt;- data_dmy$covariate_names[!data_dmy$covariate_names %in% data_dmy$categ_names]\n  num_names &lt;- num_names[!num_names %in% target_name]\n  if (length(num_names) &gt; 0) {\n    ## Number of unique values\n    num_nb_val &lt;- map_dbl(num_names, ~data_dmy$train |&gt; pull(.x) |&gt; unique() |&gt; length())\n    ## Degree for numerical variables\n    deg_num &lt;- ifelse(num_nb_val &lt; spline_df, num_nb_val, spline_df)\n    num_term &lt;- str_c(\"s(\", num_names, \", df = \", deg_num, \")\", collapse = \" + \")\n  } else {\n    num_term &lt;- NULL\n  }\n  if (length(data_dmy$categ_names &gt; 0)) {\n    categ_terms &lt;- str_c(data_dmy$categ_names, collapse = \" + \")\n  } else {\n    categ_terms &lt;- NULL\n  }\n  \n  form_terms &lt;- str_c(num_term, categ_terms, sep = \" + \")\n  \n  form_gam &lt;- str_c(target_name, \" ~ \", form_terms) |&gt; as.formula()\n  # Estimation\n  fit &lt;- gam::gam(formula = form_gam, family = binomial, data = data_dmy$train)\n  # Scores on train and test set\n  scores_train &lt;- predict(fit, newdata = data_dmy$train, type = \"response\")\n  scores_test &lt;- predict(fit, newdata = data_dmy$test, type = \"response\")\n\n  if (return_model == TRUE) {\n    res &lt;- list(\n      scores_train = scores_train,\n      scores_test = scores_test,\n      fit = fit)\n  } else {\n    list(scores_train = scores_train, scores_test = scores_test, fit = NULL)\n  }\n}\n\nThis function can then be used in a very simple way:\n\nscores_gam &lt;- train_gam(\n    data_train = data$train, data_test = data$test, target_name = target_name,\n    spline_df = 6\n)\n\n\n\n13.3.3 GAMSEL\nWe then estimate the probability that the event occurs (the email is a spam) using a Generalized Additive Model with model selection.\nFirst, we need to split the target variable and the predictors in distinct objects.\n\nX_dmy_train &lt;- data_dmy$train |&gt; dplyr::select(-!!target_name)\nX_dmy_test &lt;- data_dmy$test |&gt; dplyr::select(-!!target_name)\n\nThen, we need to make sure that all variables obtained after using the encode_dataset() function are coded as numeric: the estimation function from {gamsel} does not allow integer variables.\n\nX_dmy_train &lt;- X_dmy_train |&gt; mutate(across(everything(), as.numeric))\nX_dmy_test &lt;- X_dmy_test |&gt; mutate(across(everything(), as.numeric))\n\nThe target variable:\n\ny_train &lt;- data_dmy$train |&gt; dplyr::pull(!!target_name)\ny_test &lt;- data_dmy$test |&gt; dplyr::pull(!!target_name)\n\nThen we need to build the formula. As for the GAM, this is a bit more complex that with the GLM. We need to create a vector that gives the maximum spline basis function to use for each variable. For dummy variables, this needs to be set to 1. For other variables, let us use either 6 or the minimum number of distinct values minus 1.\n\ndegrees &lt;- 6\ndeg &lt;- rep(NA, ncol(X_dmy_train))\ncol_names_X &lt;- colnames(X_dmy_train)\nnb_val &lt;- map_dbl(\n  col_names_X, ~X_dmy_train |&gt; pull(.x) |&gt; unique() |&gt; length()\n)\nfor (i_var_name in 1:ncol(X_dmy_train)) {\n  var_name &lt;- col_names_X[i_var_name]\n  if (var_name %in% data_dmy$categ_names) {\n    deg[i_var_name] &lt;- 1\n  } else {\n    deg[i_var_name] &lt;- min(nb_val[i_var_name]-1, degrees)\n  }\n}\n\nThen, we fit the model. The penalty parameter \\(\\lambda\\) is selected by 10-fold cross validation in a first step:\n\ngamsel_cv &lt;- gamsel::cv.gamsel(\n  x = as.data.frame(X_dmy_train), y = y_train, family = \"binomial\",\n  degrees = deg\n)\n\nWe use the value of lambda which gives the minimum cross validation metric. Note that we could also use the largest value of lambda such that the error is within 1 standard error of the minimum (using lambda = gamsel_cv$lambda.1se):\n\ngamsel_out &lt;- gamsel(\n  x = as.data.frame(X_dmy_train), y = y_train, family = \"binomial\",\n  degrees = deg,\n  lambda = gamsel_cv$lambda.min\n)\n\nLastly, we can get the predicted scores:\n\nscores_train &lt;- predict(\n    gamsel_out, newdata = as.data.frame(X_dmy_train), type = \"response\")[, 1]\n  scores_test &lt;- predict(\n    gamsel_out, newdata = as.data.frame(X_dmy_test), type = \"response\")[, 1]\n\nWe encompass these steps in a helper function:\n\n#' Train a GAMSEL model\n#'\n#' @param data_train train set\n#' @param data_test test set\n#' @param target_name name of the target (response) variable\n#' @param degrees degree for the splines\n#' @param return_model if TRUE, the estimated model is returned\n#'\n#' @returns list with estimated scores on train set (`scores_train`) and on\n#'  test set (`scores_test`)\ntrain_gamsel &lt;- function(data_train,\n                         data_test,\n                         target_name,\n                         degrees = 6,\n                         return_model = FALSE) {\n  # Encode dataset so that categorical variables become dummy variables\n  data_dmy &lt;- encode_dataset(\n    data_train = data_train,\n    data_test = data_test,\n    target_name = target_name,\n    intercept = FALSE\n  )\n  # Estimation\n  X_dmy_train &lt;- data_dmy$train |&gt; dplyr::select(-!!target_name)\n  X_dmy_train &lt;- X_dmy_train |&gt; mutate(across(everything(), as.numeric))\n  X_dmy_test &lt;- data_dmy$test |&gt; dplyr::select(-!!target_name)\n  X_dmy_test &lt;- X_dmy_test |&gt; mutate(across(everything(), as.numeric))\n  y_train &lt;- data_dmy$train |&gt; dplyr::pull(!!target_name)\n  y_test &lt;- data_dmy$test |&gt; dplyr::pull(!!target_name)\n\n  deg &lt;- rep(NA, ncol(X_dmy_train))\n  col_names_X &lt;- colnames(X_dmy_train)\n  nb_val &lt;- map_dbl(\n    col_names_X, ~X_dmy_train |&gt; pull(.x) |&gt; unique() |&gt; length()\n  )\n  for (i_var_name in 1:ncol(X_dmy_train)) {\n    var_name &lt;- col_names_X[i_var_name]\n    if (var_name %in% data_dmy$categ_names) {\n      deg[i_var_name] &lt;- 1\n    } else {\n      deg[i_var_name] &lt;- min(nb_val[i_var_name]-1, degrees)\n    }\n  }\n  gamsel_cv &lt;- gamsel::cv.gamsel(\n    x = as.data.frame(X_dmy_train), y = y_train, family = \"binomial\",\n    degrees = deg\n  )\n  gamsel_out &lt;- gamsel::gamsel(\n    x = as.data.frame(X_dmy_train), y = y_train, family = \"binomial\",\n    degrees = deg,\n    lambda = gamsel_cv$lambda.min\n  )\n  # Scores on train and test set\n  scores_train &lt;- predict(\n    gamsel_out, newdata = as.data.frame(X_dmy_train), type = \"response\")[, 1]\n  scores_test &lt;- predict(\n    gamsel_out, newdata = as.data.frame(X_dmy_test), type = \"response\")[, 1]\n\n  if (return_model == TRUE) {\n    res &lt;- list(\n      scores_train = scores_train,\n      scores_test = scores_test,\n      fit = fit)\n  } else {\n    list(scores_train = scores_train, scores_test = scores_test, fit = NULL)\n  }\n}\n\nThis function can then be used in a very simple way:\n\nscores_gamsel &lt;- train_gamsel(\n    data_train = data$train, data_test = data$test, target_name = target_name,\n    degrees = 6\n  )",
    "crumbs": [
      "Real-world Data",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Priors: Illustration</span>"
    ]
  },
  {
    "objectID": "book_real_example.html#fitting-a-beta-distribution",
    "href": "book_real_example.html#fitting-a-beta-distribution",
    "title": "13  Priors: Illustration",
    "section": "13.4 Fitting a Beta Distribution",
    "text": "13.4 Fitting a Beta Distribution\nOnce the scores from the models have been estimated, we fit a Beta distribution to them. This will provide a prior distribution of the true probabilities in the exercise.\nTo avoid crashing the ML estimation of the two parameters of the Beta distribution, let us make sure that any score is in \\((0,1)\\) and not exactly equal to 0 or 1.\n\nx_glm &lt;- (scores_glm$scores_test * (1 - 1e-6)) + 1e-6 / 2\nx_gam &lt;- (scores_gam$scores_test * (1 - 1e-6)) + 1e-6 / 2\nx_gamsel &lt;- (scores_gamsel$scores_test * (1 - 1e-6)) + 1e-6 / 2\n\nTo estimate the two parameters of the Beta distribution, we define a small function, fit_beta_scores() that calls the fitdistr() function from {MASS}.\n\n#' Maximum-likelihood fitting of Beta distribution on scores\n#'\n#' @param scores vector of estimated scores\n#' @param shape1 non-negative first parameter of the Beta distribution\n#' @param shape1 non-negative second parameter of the Beta distribution\n#'\n#' @returns An object of class `fitdistr`, a list with four components\n#'  (see: MASS::fitdistr())\n#'  - `estimate`: the parameter estimates\n#'  - `sd`: the estimated standard errors\n#'  - `vcov`: the estimated variance-covariance matrix\n#'  - `loglik`: the log-likelihood\nfit_beta_scores &lt;- function(scores, shape1 = 1, shape2 = 1) {\n  # Fit a beta distribution\n  mle_fit &lt;- MASS::fitdistr(\n    scores, \"beta\", start = list(shape1 = 1, shape2 = 1)\n  )\n  mle_fit\n}\n\n\n(mle_gam &lt;- fit_beta_scores(scores = x_gam))\n\n     shape1        shape2   \n  0.062913176   0.081751894 \n (0.002447438) (0.003470926)\n\n(mle_glm &lt;- fit_beta_scores(scores = x_glm))\n\n     shape1        shape2   \n  0.141335615   0.210571524 \n (0.005452414) (0.009260466)\n\n(mle_gamsel &lt;- fit_beta_scores(scores = x_gamsel[!is.nan(x_gamsel)]))\n\n     shape1       shape2  \n  0.42945278   0.51642775 \n (0.01755855) (0.02225033)\n\n\nLet us plot the distribution of the scores obtained with the GAMSEL model. On top of the graph, we draw the density of the Beta distribution with the parameters estimated for each model.\n\n\nCode\nval_u &lt;- seq(0, 1, length = 651)\nlayout(mat = matrix(1:2), heights = c(3,1))\n# Histogram of scores obtained with the GAMSEL, on test set\npar(mar = c(4.1, 4.1, 1, 2.1))\nhist(\n  scores_gamsel$scores_test,\n  breaks = seq(0, 1, by = .05), probability = TRUE,\n  main = \"spambase\", xlab = latex2exp::TeX(\"$\\\\hat{s}(x)$\")\n)\n# Beta dist. estimated using the scores from the GLM\nlines(\n  val_u,\n  dbeta(val_u, mle_glm$estimate[1], mle_glm$estimate[2]),\n  col = \"#D55E00\",lwd = 1.5\n)\n# Beta dist. estimated using the scores from the GAM\nlines(\n  val_u,\n  dbeta(val_u, mle_gam$estimate[1], mle_gam$estimate[2]),\n  col = \"#0072B2\",lwd = 1.5\n)\n# Beta dist. estimated using the scores from the GAM\nlines(\n  val_u,\n  dbeta(val_u, mle_gamsel$estimate[1], mle_gamsel$estimate[2]),\n  col = \"#E69F00\",lwd = 1.5\n)\npar(mar = c(0, 4.1, 0, 2.1))\nplot.new()\nlegend(\n  xpd = TRUE, ncol = 3,\n  \"center\",\n  title = \"Model\",\n  lwd = 1.5,\n  col = c(\"#D55E00\", \"#0072B2\", \"#E69F00\"),\n  legend = c(\"GLM-logistic\", \"GAM\", \"GAMSEL\")\n)\n\n\n\n\n\nFigure 13.1: Distribution of estimated probabilities by the GAMSEL model and Beta distribution fitted to the scores of each of the three models.",
    "crumbs": [
      "Real-world Data",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Priors: Illustration</span>"
    ]
  },
  {
    "objectID": "book_real_example.html#wrapper-functions",
    "href": "book_real_example.html#wrapper-functions",
    "title": "13  Priors: Illustration",
    "section": "13.5 Wrapper Functions",
    "text": "13.5 Wrapper Functions\nFor convenience, we build a wrapper function, get_beta_fit() that takes a dataset as an input, the name of the target variable and possibly a seet. From these arguments, the function splits the dataset into a training and a test set. It then fits the models, and fit a Beta distribution on the scores estimated in the test set. This function returns a list with 6 elements: the first three are the estimated scores of the three models, the last three are the parameters of the Beta distribution estimated using the scores of each model.\n\n\nFunction get_beta_fit()\n#' Estimation of a GLM-logistic, a GAM and a GAMSEL model on a classification\n#' task. Then, on estimated scores from the test set, fits a Beta distribution.\n#'\n#' @param dataset dataset with response variable and predictors\n#' @param target_name name of the target (response) variable\n#' @param seed desired seed (default to `NULL`)\n#'\n#' @returns A list with the following elements:\n#'  - `scores_glm`: scores on train and test set (in a list) from the GLM\n#'  - `scores_gam`: scores on train and test set (in a list) from the GAM\n#'  - `scores_gamsel`: scores on train and test set (in a list) from the GAMSEL\n#'  - `mle_glm`: An object of class \"fitdistr\" for the GLM model\n#'    (see fit_beta_scores())\n#'  - `mle_gamsel`: An object of class \"fitdistr\" for the GAM model\n#'    (see fit_beta_scores())\n#'  - `mle_gamsel`: An object of class \"fitdistr\" for the GAMSEL model\n#'    (see fit_beta_scores())\nget_beta_fit &lt;- function(dataset,\n                         target_name,\n                         seed = NULL) {\n  # Split data into train/test\n  data &lt;- split_train_test(data = dataset, prop_train = .8, seed = seed)\n\n  # Train a GLM-logistic model\n  scores_glm &lt;- train_glm(\n    data_train = data$train, data_test = data$test, target_name = target_name\n  )\n  # Train a GAM model\n  scores_gam &lt;- train_gam(\n    data_train = data$train, data_test = data$test, target_name = target_name,\n    spline_df = 6\n  )\n  # Train a GAMSEL model\n  scores_gamsel &lt;- train_gamsel(\n    data_train = data$train, data_test = data$test, target_name = target_name,\n    degrees = 6\n  )\n  # Add a little noise to the estimated scores to avoid being in [0,1] and be\n  # in (0,1) instead.\n  x_glm &lt;- (scores_glm$scores_test * (1 - 1e-6)) + 1e-6 / 2\n  x_gam &lt;- (scores_gam$scores_test * (1 - 1e-6)) + 1e-6 / 2\n  x_gamsel &lt;- (scores_gamsel$scores_test * (1 - 1e-6)) + 1e-6 / 2\n  # Fit a Beta distribution on these scores\n  mle_gam &lt;- fit_beta_scores(scores = x_gam)\n  mle_glm &lt;- fit_beta_scores(scores = x_glm)\n  mle_gamsel &lt;- fit_beta_scores(scores = x_gamsel)\n\n  list(\n    scores_glm = scores_glm,\n    scores_gam = scores_gam,\n    scores_gamsel = scores_gamsel,\n    mle_glm = mle_glm,\n    mle_gam = mle_gam,\n    mle_gamsel = mle_gamsel\n  )\n}\n\n\nWe also define a function, plot_hist_scores_beta() to plot the distribution of scores obtained with the GAMSEL model and the density functions of the three Beta distributions whose parameters were estimated based on the scores of the three models.\n\n\nFunction plot_hist_scores_beta()\n#' Plots the histogram of scores estimated with GAMSEL\n#' add densities of Beta distribution for whith the parameters have been\n#' estimated using scores from the GLM, the GAM, or the GAMSEL model\n#'\n#' @param fit_resul results obtained from get_beta_fit()\n#' @param title title of the graph (e.g.: dataset name)\nplot_hist_scores_beta &lt;- function(fit_resul, title = NULL) {\n  val_u &lt;- seq(0, 1, length = 651)\n  layout(mat = matrix(1:2), heights = c(3,1))\n  \n  dbeta_val &lt;- vector(mode = \"list\", length = 3)\n  for (i_type in 1:3) {\n    type &lt;- c(\"mle_glm\", \"mle_gam\", \"mle_gamsel\")[i_type]\n    dbeta_val[[i_type]] &lt;- dbeta(\n        val_u, \n        fit_resul[[type]]$estimate[1], \n        fit_resul[[type]]$estimate[2]\n      )\n  }\n  y_lim &lt;- c(\n    0,\n    map(dbeta_val,\n        ~range(.x[!is.infinite(.x)], na.rm = TRUE)) |&gt;\n      unlist() |&gt; max(na.rm = TRUE)\n  )\n  \n  \n  # Histogram of scores obtained with the GAMSEL, on test set\n  par(mar = c(4.1, 4.1, 1, 2.1))\n  hist(\n    fit_resul$scores_gamsel$scores_test,\n    breaks = seq(0, 1, by = .05), probability = TRUE,\n    main = title, xlab = latex2exp::TeX(\"$\\\\hat{s}(x)$\"),\n    ylim = y_lim\n  )\n\n  legend_name &lt;- c(\"GLM-logistic\", \"GAM\", \"GAMSEL\")\n  colours &lt;- c(\n    \"mle_glm\" = \"#D55E00\",\n    \"mle_gam\" = \"#0072B2\",\n    \"mle_gamsel\" = \"#E69F00\"\n  )\n  for (i_type in 1:3) {\n    type &lt;- c(\"mle_glm\", \"mle_gam\", \"mle_gamsel\")[i_type]\n    lines(\n      val_u,\n      dbeta_val[[i_type]],\n      col = colours[i_type],lwd = 1.5\n    )\n  }\n  par(mar = c(0, 4.1, 0, 2.1))\n  plot.new()\n  legend(\n    xpd = TRUE, ncol = 3,\n    \"center\",\n    title = \"Model\",\n    lwd = 1.5,\n    col = colours,\n    legend = legend_name\n  )\n}\n\n\nThese two functions can be called as follows:\n\n# Chunk not evaluated\nresul &lt;- get_beta_fit(dataset = dataset, target_name = \"is_spam\", seed = 1234)\nplot_hist_scores_beta(resul, \"spambase\")",
    "crumbs": [
      "Real-world Data",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Priors: Illustration</span>"
    ]
  },
  {
    "objectID": "book_real_beta.html",
    "href": "book_real_beta.html",
    "title": "14  Datasets and Priors",
    "section": "",
    "text": "14.1 Datasets\nAll the datasets used here are from the UCI Machine Learning Repository.",
    "crumbs": [
      "Real-world Data",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Datasets and Priors</span>"
    ]
  },
  {
    "objectID": "book_real_beta.html#datasets",
    "href": "book_real_beta.html#datasets",
    "title": "14  Datasets and Priors",
    "section": "",
    "text": "14.1.1 Abalone\n\nURL to the data: https://archive.ics.uci.edu/dataset/1/abalone\nDescription: Predict the age of abalone from physical measurements.\nNumber of instances: 4,177\nFeatures: 8\nReference: Nash et al. (1995)\n\n\nname &lt;- \"abalone\"\n\nThe dataset needs to be download.\n\n\nCode to download the data\nif (!dir.exists(\"data\")) dir.create(\"data\")\ndownload.file(\n  url = str_c(\"https://archive.ics.uci.edu/static/public/1/\", name, \".zip\"), \n  destfile = str_c(\"data/\", name, \".zip\")\n)\n\n\nThen, we can import the dataset:\n\n\nCode to import the data\ntb_abalone &lt;- read_csv(\n  file = unz(str_c(\"data/\", name, \".zip\"), str_c(name, \".data\")), \n  col_names = c(\n    \"Sex\", \"Length\", \"Diameter\", \"Height\", \"Whole_weight\", \n  \"Shucked_weight\", \"Viscera_weight\", \"Shell_weight\", \"Rings\"),\n  show_col_types = FALSE\n)\n\n\nThe target variable is sex. Let us turn it in a \\(\\{0,1\\}\\) variable.\n\ntb_abalone &lt;- tb_abalone |&gt; \n  mutate(Sex = ifelse(Sex == \"M\", 1, 0)) \ntarget_name &lt;- \"Sex\"\n\nLet us call the get_beta_fit() from Chapter 13 to get our priors.\n\npriors_abalone &lt;- get_beta_fit(\n  dataset = tb_abalone, target_name = target_name, seed = 1234\n)\n\nLet us save the results and the dataset:\n\nsave(priors_abalone, file = \"output/real-data/priors_abalone.rda\")\nsave(tb_abalone, file = \"output/real-data/tb_abalone.rda\")\n\n\nplot_hist_scores_beta(priors_abalone, \"abalone\")\n\n\n\n\nFigure 14.1: Distribution of estimated probabilities by the GAMSEL model and Beta distribution fitted to the scores of each of the three models, for the Abalone dataset.\n\n\n\n\n\n\n\n\n\n\n14.1.2 Adult\n\nURL to the data: https://archive.ics.uci.edu/dataset/2/adult\nDescription: Predict whether income exceeds $50K/yr based on census data. Also known as “Census Income” dataset.\nNumber of instances: 48,842\nFeatures: 14\nReference: Becker and Kohavi (1996)\n\n\nname &lt;- \"adult\"\n\nThe dataset needs to be download.\n\n\nCode to download the data\nif (!dir.exists(\"data\")) dir.create(\"data\")\ndownload.file(\n  url = str_c(\"https://archive.ics.uci.edu/static/public/2/\", name, \".zip\"), \n  destfile = str_c(\"data/\", name, \".zip\")\n)\n\ninfo_data &lt;- scan(\n  unz(str_c(\"data/\", name, \".zip\"), str_c(name, \".names\")), \n  what = \"character\", sep = \"\\n\"\n)\n# Print the names for this dataset (not very convenient...)\nstr_extract(info_data[94:length(info_data)], \"^(.*):\") |&gt; \n  str_remove(\":$\") |&gt; \n  (\\(.x) str_c('\"', .x, '\",'))() |&gt; \n  cat()\n\n\nThen, we can import the dataset:\n\n\nCode to import the data\ntb_adult &lt;- read_csv(\n  file = unz(str_c(\"data/\", name, \".zip\"), str_c(name, \".data\")), \n  col_names = c(\n    \"age\", \"workclass\", \"fnlwgt\", \"education\", \"education-num\",\n    \"marital-status\", \"occupation\", \"relationship\", \"race\", \"sex\",\n    \"capital-gain\", \"capital-loss\", \"hours-per-week\", \"native-country\",\n    \"income\"\n  ),\n  show_col_types = FALSE\n)\n\n\nThe target variable is income. Let us turn it in a \\(\\{0,1\\}\\) variable and call it high_income.\n\ntb_adult &lt;- tb_adult |&gt; \n  mutate(high_income = ifelse(income == \"&gt;50K\", 1, 0)) |&gt; \n  dplyr::select(-income)\ntarget_name &lt;- \"high_income\"\n\nLet us call the get_beta_fit() from Chapter 13 to get our priors.\n\npriors_adult &lt;- get_beta_fit(\n  dataset = tb_adult, target_name = target_name, seed = 1234\n)\n\nLet us save the results and the dataset:\n\nsave(priors_adult, file = \"output/real-data/priors_adult.rda\")\nsave(tb_adult, file = \"output/real-data/tb_adult.rda\")\n\n\nplot_hist_scores_beta(priors_adult, \"adult\")\n\n\n\n\nFigure 14.2: Distribution of estimated probabilities by the GAMSEL model and Beta distribution fitted to the scores of each of the three models, for the Adult dataset.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n14.1.3 Bank Marketing\n\nURL to the data: https://archive.ics.uci.edu/dataset/222/bank+marketing\nDescription: The data is related with direct marketing campaigns (phone calls) of a Portuguese banking institution. The classification goal is to predict if the client will subscribe a term deposit (variable y).\nNumber of instances: 45,211\nFeatures: 16\nReference: Moro, Rita, and Cortez (2012)\n\n\nname &lt;- \"bank\"\n\nThe dataset needs to be download.\n\n\nCode to download the data\nif (!dir.exists(\"data\")) dir.create(\"data\")\ndownload.file(\n  url = \"https://archive.ics.uci.edu/static/public/222/bank+marketing.zip\", \n  destfile = str_c(\"data/\", name, \".zip\")\n)\n\n\nThen, we can import the dataset:\n\n\nCode to import the data\ndir.create(\"data/bank/\")\nsystem(\"unzip data/bank.zip -d data/bank/\")\nsystem(\"unzip data/bank/bank.zip -d data/bank/\")\ntb_bank &lt;- read_csv2(\n  file = unz(str_c(\"data/bank/\", name, \".zip\"), str_c(\"bank-full.csv\")), \n  skip = 1,\n  col_names = c(\n    \"age\", \"job\", \"marital\", \"education\", \"default\", \"balance\", \"housing\", \n    \"loan\", \"contact\", \"day\", \"month\", \"duration\", \"campaign\", \"pdays\", \n    \"previous\", \"poutcome\", \"y\"\n  ),\n  show_col_types = FALSE\n)\n\n\nℹ Using \"','\" as decimal and \"'.'\" as grouping mark. Use `read_delim()` for more control.\n\n\nCode to import the data\nsystem(\"rm -rf data/bank/\")\n\n\nThe target variable is y (whether the client will subscribe a term deposit). Let us turn it in a \\(\\{0,1\\}\\) variable.\n\ntb_bank &lt;- tb_bank |&gt; \n  mutate(y = ifelse(y == \"yes\", 1, 0)) \ntarget_name &lt;- \"y\"\n\nLet us call the get_beta_fit() from Chapter 13 to get our priors.\n\npriors_bank &lt;- get_beta_fit(\n  dataset = tb_bank, target_name = target_name, seed = 1234\n)\n\nLet us save the results and the dataset:\n\nsave(priors_bank, file = \"output/real-data/priors_bank.rda\")\nsave(tb_bank, file = \"output/real-data/tb_bank.rda\")\n\n\nplot_hist_scores_beta(priors_bank, \"bank\")\n\n\n\n\nFigure 14.3: Distribution of estimated probabilities by the GAMSEL model and Beta distribution fitted to the scores of each of the three models, for the Bank Marketing dataset.\n\n\n\n\n\n\n\n\n\n\n14.1.4 Default of Credit Card Clients\n\nURL to the data: https://archive.ics.uci.edu/dataset/350/default+of+credit+card+clients\nDescription: This research aimed at the case of customers’ default payments in Taiwan and compares the predictive accuracy of probability of default among six data mining methods.\nNumber of instances: 30,000\nFeatures: 23\nReference: Yeh (2016)\n\n\nname &lt;- \"default\"\n\nThe dataset needs to be download.\n\n\nCode to download the data\nif (!dir.exists(\"data\")) dir.create(\"data\")\ndownload.file(\n  url = str_c(\"https://archive.ics.uci.edu/static/public/350/\",\n              \"default+of+credit+card+clients.zip\"\n  ), \n  destfile = str_c(\"data/\", name, \".zip\")\n)\n\n\nThen, we can import the dataset:\n\n\nCode to import the data\ndir.create(\"data/default/\")\nsystem(\"unzip data/default.zip -d data/default/\")\ntb_default &lt;- readxl::read_excel(\n  path = \"data/default/default of credit card clients.xls\",\n  skip = 1\n) |&gt; \n  select(-ID)\nsystem(\"rm -rf data/default\")\n\n\nThe target variable is defalut (1 if default, 0 otherwise).\n\ntb_default &lt;- \n  tb_default |&gt; \n  mutate(\n    across(all_of(c(\n      \"SEX\", \"EDUCATION\", \"MARRIAGE\", \n      \"PAY_0\", \"PAY_2\", \"PAY_3\", \"PAY_4\", \"PAY_5\", \"PAY_6\")), as.factor)\n  ) |&gt; \n  mutate(\n    across(all_of(c(\n      \"EDUCATION\", \"PAY_0\", \"PAY_2\", \"PAY_3\", \"PAY_4\", \"PAY_5\", \"PAY_6\"\n    )), ~fct_lump(.x, prop = .05)\n    )\n  ) |&gt; \n  rename(default = `default payment next month`)\ntarget_name &lt;- \"default\"\n\nLet us call the get_beta_fit() from Chapter 13 to get our priors.\n\npriors_default &lt;- get_beta_fit(\n  dataset = tb_default, target_name = target_name, seed = 1234\n)\n\nLet us save the results and the dataset:\n\nsave(priors_default, file = \"output/real-data/priors_default.rda\")\nsave(tb_default, file = \"output/real-data/tb_default.rda\")\n\n\nplot_hist_scores_beta(priors_default, \"default\")\n\n\n\n\nFigure 14.4: Distribution of estimated probabilities by the GAMSEL model and Beta distribution fitted to the scores of each of the three models, for the Default of Credit Card Clients dataset.\n\n\n\n\n\n\n\n\n\n\n14.1.5 Dry Bean\n\nURL to the data: https://archive.ics.uci.edu/dataset/602/dry+bean+dataset\nDescription: Images of 13,611 grains of 7 different registered dry beans were taken with a high-resolution camera. A total of 16 features; 12 dimensions and 4 shape forms, were obtained from the grains.\nNumber of instances: 13,611\nFeatures: 16\nReferences: “Dry Bean” (2020)\n\n\nname &lt;- \"drybean\"\n\nThe dataset needs to be download.\n\n\nCode to download the data\nif (!dir.exists(\"data\")) dir.create(\"data\")\ndownload.file(\n  url = \"https://archive.ics.uci.edu/static/public/602/dry+bean+dataset.zip\", \n  destfile = str_c(\"data/\", name, \".zip\")\n)\n\n\nThen, we can import the dataset:\n\n\nCode to import the data\ndir.create(\"data/drybean/\")\nsystem(\"unzip data/drybean.zip -d data/drybean/\")\ntb_drybean &lt;- readxl::read_excel(\n  path = \"data/drybean/DryBeanDataset/Dry_Bean_Dataset.xlsx\"\n)\nsystem(\"rm -rf data/drybean\")\n\n\nThe target variable is sex. Let us turn it in a \\(\\{0,1\\}\\) variable.\n\ntb_drybean &lt;- tb_drybean |&gt; \n  mutate(is_dermason = ifelse(Class == \"DERMASON\", 1, 0)) |&gt; \n  select(-Class)\ntarget_name &lt;- \"is_dermason\"\n\nLet us call the get_beta_fit() from Chapter 13 to get our priors.\n\npriors_drybean &lt;- get_beta_fit(\n  dataset = tb_drybean, target_name = target_name, seed = 1234\n)\n\nLet us save the results and the dataset:\n\nsave(priors_drybean, file = \"output/real-data/priors_drybean.rda\")\nsave(tb_drybean, file = \"output/real-data/tb_drybean.rda\")\n\n\nplot_hist_scores_beta(priors_drybean, \"drybean\")\n\n\n\n\nFigure 14.5: Distribution of estimated probabilities by the GAMSEL model and Beta distribution fitted to the scores of each of the three models, for the Dry Bean dataset.\n\n\n\n\n\n\n\n\n\n\n14.1.6 In-Vehicle Coupon Recommendation\n\nURL to the data: https://archive.ics.uci.edu/dataset/603/in+vehicle+coupon+recommendation\nDescription: This data studies whether a person will accept the coupon recommended to him in different driving scenarios.\nNumber of instances: 12,684\nFeatures: 25\nReferences: “In-Vehicle Coupon Recommendation” (2020)\n\n\nname &lt;- \"coupon\"\n\nThe dataset needs to be download.\n\n\nCode to download the data\nif (!dir.exists(\"data\")) dir.create(\"data\")\ndownload.file(\n  url = str_c(\"https://archive.ics.uci.edu/static/public/603/\", \n              \"in+vehicle+coupon+recommendation.zip\"), \n  destfile = str_c(\"data/\", name, \".zip\")\n)\n\n\nThen, we can import the dataset:\n\n\nCode to import the data\ntb_coupon &lt;- read_csv(\n  file = unz(str_c(\"data/\", name, \".zip\"), \"in-vehicle-coupon-recommendation.csv\"),\n  show_col_types = FALSE\n)\n\n\nThe target variable is y (1 if the person accepted the coupon, 0 otherwise).\n\ntb_coupon &lt;- \n  tb_coupon |&gt; \n  mutate(\n    temperature = as.factor(temperature),\n    has_children = as.factor(has_children),\n    toCoupon_GEQ15min = as.factor(toCoupon_GEQ15min),\n    toCoupon_GEQ25min = as.factor(toCoupon_GEQ25min),\n    direction_same = as.factor(direction_same)\n  ) |&gt; \n  select(-toCoupon_GEQ5min, -direction_opp, -car) |&gt; \n  rename(y = Y)\n\ntb_coupon &lt;- na.omit(tb_coupon)\n\ntarget_name &lt;- \"y\"\n\nLet us call the get_beta_fit() from Chapter 13 to get our priors.\n\npriors_coupon &lt;- get_beta_fit(\n  dataset = tb_coupon, target_name = target_name, seed = 1234\n)\n\nLet us save the results and the dataset:\n\nsave(priors_coupon, file = \"output/real-data/priors_coupon.rda\")\nsave(tb_coupon, file = \"output/real-data/tb_coupon.rda\")\n\n\nplot_hist_scores_beta(priors_coupon, \"coupon\")\n\n\n\n\nFigure 14.6: Distribution of estimated probabilities by the GAMSEL model and Beta distribution fitted to the scores of each of the three models, for the In-Vehicle Coupon Recommendation dataset.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n14.1.7 Mushroom\n\nURL to the data: https://archive.ics.uci.edu/dataset/73/mushroom\nDescription: From Audobon Society Field Guide; mushrooms described in terms of physical characteristics; classification: poisonous or edible.\nNumber of instances: 8,124\nFeatures: 22\nReferences: “Mushroom” (1987)\n\n\nname &lt;- \"mushroom\"\n\nThe dataset needs to be download.\n\n\nCode to download the data\nif (!dir.exists(\"data\")) dir.create(\"data\")\ndownload.file(\n  url = str_c(\"https://archive.ics.uci.edu/static/public/73/mushroom.zip\"), \n  destfile = str_c(\"data/\", name, \".zip\")\n)\n\n\nThen, we can import the dataset:\n\n\nCode to import the data\ntb_mushroom &lt;- read_csv(\n  file = unz(str_c(\"data/\", name, \".zip\"), \"agaricus-lepiota.data\"), \n  col_names = c(\n    \"edible\",\n    \"cap_shape\", \"cap_surface\", \"cap_color\", \"bruises\", \"odor\", \n    \"gill_attachment\", \"gill_spacing\", \"gill_size\", \"gill_color\", \n    \"stalk_shape\", \"stalk_root\", \"stalk_surface_above_ring\",\n    \"stalk_surface_below_ring\", \"stalk_color_above_ring\", \n    \"stalk_color_below_ring\", \"veil_type\", \"veil_color\", \"ring_number\", \n    \"ring_type\", \"spore_print_color\", \"population\", \"habitat\"\n  ),\n  show_col_types = FALSE\n)\n\n\nThe target variable is edible. Let us turn it in a \\(\\{0,1\\}\\) variable.\n\ntb_mushroom &lt;- tb_mushroom |&gt; \n  mutate(bruises = ifelse(bruises == TRUE, \"yes\", \"no\")) |&gt; \n  mutate(edible = ifelse(edible == \"e\", 1, 0)) |&gt; \n  select(-veil_type)\ntarget_name &lt;- \"edible\"\n\nLet us call the get_beta_fit() from Chapter 13 to get our priors.\n\npriors_mushroom &lt;- get_beta_fit(\n  dataset = tb_mushroom, target_name = target_name, seed = 1234\n)\n\nLet us save the results and the dataset:\n\nsave(priors_mushroom, file = \"output/real-data/priors_mushroom.rda\")\nsave(tb_mushroom, file = \"output/real-data/tb_mushroom.rda\")\n\n\nplot_hist_scores_beta(priors_mushroom, \"mushroom\")\n\n\n\n\nFigure 14.7: Distribution of estimated probabilities by the GAMSEL model and Beta distribution fitted to the scores of each of the three models, for the Mushroom dataset.\n\n\n\n\n\n\n\n\n\n\n14.1.8 Occupancy Detection\n\nURL to the data: https://archive.ics.uci.edu/dataset/357/occupancy+detection\nDescription: Predict the age of occupancy from physical measurements.\nNumber of instances: 20,560\nFeatures: 6\nReferences: Candanedo (2016)\n\n\nname &lt;- \"occupancy\"\n\nThe dataset needs to be download.\n\n\nCode to download the data\nif (!dir.exists(\"data\")) dir.create(\"data\")\ndownload.file(\n  url = str_c(\"https://archive.ics.uci.edu/static/public/357/\",\n              \"occupancy+detection.zip\"), \n  destfile = str_c(\"data/\", name, \".zip\")\n)\n\n\nThen, we can import the dataset:\n\n\nCode to import the data\ntb_occupancy &lt;- read_csv(\n  file = unz(str_c(\"data/\", name, \".zip\"), \"datatraining.txt\"), \n  col_names = c(\n    \"id\", \"date\",\"Temperature\",\"Humidity\",\"Light\",\"CO2\",\n    \"HumidityRatio\",\"Occupancy\"\n  ),\n  show_col_types = FALSE, skip = 1\n) |&gt; \n  bind_rows(\n    read_csv(\n      file = unz(str_c(\"data/\", name, \".zip\"), \"datatest.txt\"), \n      col_names = c(\n        \"id\", \"date\",\"Temperature\",\"Humidity\",\"Light\",\"CO2\",\n        \"HumidityRatio\",\"Occupancy\"\n      ),\n      show_col_types = FALSE, skip = 1,\n    )\n  ) |&gt; \n  bind_rows(\n    read_csv(\n      file = unz(str_c(\"data/\", name, \".zip\"), \"datatest2.txt\"), \n      show_col_types = FALSE, skip = 1,\n      col_names = c(\n        \"id\", \"date\",\"Temperature\",\"Humidity\",\"Light\",\"CO2\",\n        \"HumidityRatio\",\"Occupancy\"\n      ),\n    )\n  ) |&gt; \n  select(-id)\n\n\nThe target variable is Occupancy.\n\ntb_occupancy &lt;- tb_occupancy |&gt; \n  select(-date)\ntarget_name &lt;- \"Occupancy\"\n\nLet us call the get_beta_fit() from Chapter 13 to get our priors.\n\npriors_occupancy &lt;- get_beta_fit(\n  dataset = tb_occupancy, target_name = target_name, seed = 1234\n)\n\nLet us save the results and the dataset:\n\nsave(priors_occupancy, file = \"output/real-data/priors_occupancy.rda\")\nsave(tb_occupancy, file = \"output/real-data/tb_occupancy.rda\")\n\n\nplot_hist_scores_beta(priors_occupancy, \"occupancy\")\n\n\n\n\nFigure 14.8: Distribution of estimated probabilities by the GAMSEL model and Beta distribution fitted to the scores of each of the three models, for the Occupancy Detection dataset.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n14.1.9 Wine Quality\n\nURL to the data: https://archive.ics.uci.edu/dataset/186/wine+quality\nDescription: Two datasets are included, related to red and white vinho verde wine samples, from the north of Portugal. The goal is to model wine quality based on physicochemical tests (see [Cortez et al., 2009], http://www3.dsi.uminho.pt/pcortez/wine/).\nNumber of instances: 4,898\nFeatures: 11\nReferences: Cortez et al. (2009)\n\n\nname &lt;- \"winequality\"\n\nThe dataset needs to be download.\n\n\nCode to download the data\nif (!dir.exists(\"data\")) dir.create(\"data\")\ndownload.file(\n  url = str_c(\"https://archive.ics.uci.edu/static/public/186/\",\n              \"wine+quality.zip\"), \n  destfile = str_c(\"data/\", name, \".zip\")\n)\n\n\nThen, we can import the dataset:\n\n\nCode to import the data\nred_wine &lt;- read_csv2(\n  file = unz(str_c(\"data/\", name, \".zip\"), \"winequality-red.csv\"),\n  show_col_types = FALSE) |&gt;\n  mutate(wine_type = \"red\")\n\n\nℹ Using \"','\" as decimal and \"'.'\" as grouping mark. Use `read_delim()` for more control.\n\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\nCode to import the data\nwhite_wine &lt;- read_csv2(\n  file = unz(str_c(\"data/\", name, \".zip\"), \"winequality-white.csv\"),\n  show_col_types = FALSE) |&gt; \n  mutate(wine_type = \"white\") |&gt; \n  mutate(`residual sugar` = as.numeric(`residual sugar`))\n\n\nℹ Using \"','\" as decimal and \"'.'\" as grouping mark. Use `read_delim()` for more control.\n\n\nThe target variable is quality. Let us use it to define a \\(\\{0,1\\}\\) variable. We define the variable high_quality which equals 1 if the quality is larger or equal than 6.\n\ntb_winequality &lt;- red_wine |&gt; bind_rows(white_wine) |&gt; \n  mutate(high_quality = ifelse(quality &gt;= 6, 1, 0)) |&gt; \n  mutate(across(all_of(c(\n    \"density\", \"chlorides\", \"volatile acidity\", \"sulphates\", \"citric acid\"\n    )), ~as.numeric(.x))) |&gt; \n  select(-quality)\ntb_winequality &lt;- na.omit(tb_winequality)\ntarget_name &lt;- \"high_quality\"\n\nLet us call the get_beta_fit() from Chapter 13 to get our priors.\n\npriors_winequality &lt;- get_beta_fit(\n  dataset = tb_winequality, target_name = target_name, seed = 1234\n)\n\nLet us save the results and the dataset:\n\nsave(priors_winequality, file = \"output/real-data/priors_winequality.rda\")\nsave(tb_winequality, file = \"output/real-data/tb_winequality.rda\")\n\n\nplot_hist_scores_beta(priors_winequality, \"winequality\")\n\n\n\n\nFigure 14.9: Distribution of estimated probabilities by the GAMSEL model and Beta distribution fitted to the scores of each of the three models, for the Wine Quality dataset.\n\n\n\n\n\n\n\n\n\n\n14.1.10 Spambase\n\nURL to the data: https://archive.ics.uci.edu/dataset/94/spambase\nDescription: Classifying Email as Spam or Non-Spam\nNumber of instances: 4,601\nFeatures: 57\nReferences: Hopkins et al. (1999)\n\n\nname &lt;- \"spambase\"\n\nThe dataset needs to be download.\n\n\nCode to download the data\nif (!dir.exists(\"data\")) dir.create(\"data\")\ndownload.file(\n  url = str_c(\"https://archive.ics.uci.edu/static/public/2/\", name, \".zip\"), \n  destfile = str_c(\"data/\", name, \".zip\")\n)\n\ninfo_data &lt;- scan(\n  unz(str_c(\"data/\", name, \".zip\"), str_c(name, \".names\")), \n  what = \"character\", sep = \"\\n\"\n)\n# Print the names for this dataset (not very convenient...)\nstr_extract(info_data[94:length(info_data)], \"^(.*):\") |&gt; \n  str_remove(\":$\") |&gt; \n  (\\(.x) str_c('\"', .x, '\",'))() |&gt; \n  cat()\n\n\nThen, we can import the dataset:\n\n\nCode to import the data\ntb_spambase &lt;- read_csv(\n  file = unz(str_c(\"data/\", name, \".zip\"), str_c(name, \".data\")),\n  col_names = c(\n    \"word_freq_make\", \"word_freq_address\", \"word_freq_all\", \"word_freq_3d\",\n    \"word_freq_our\", \"word_freq_over\", \"word_freq_remove\", \"word_freq_internet\",\n    \"word_freq_order\", \"word_freq_mail\", \"word_freq_receive\", \"word_freq_will\",\n    \"word_freq_people\", \"word_freq_report\", \"word_freq_addresses\",\n    \"word_freq_free\", \"word_freq_business\", \"word_freq_email\", \"word_freq_you\",\n    \"word_freq_credit\", \"word_freq_your\", \"word_freq_font\", \"word_freq_000\",\n    \"word_freq_money\", \"word_freq_hp\", \"word_freq_hpl\", \"word_freq_george\",\n    \"word_freq_650\", \"word_freq_lab\", \"word_freq_labs\", \"word_freq_telnet\",\n    \"word_freq_857\", \"word_freq_data\", \"word_freq_415\", \"word_freq_85\",\n    \"word_freq_technology\", \"word_freq_1999\", \"word_freq_parts\", \"word_freq_pm\",\n    \"word_freq_direct\", \"word_freq_cs\", \"word_freq_meeting\",\n    \"word_freq_original\", \"word_freq_project\", \"word_freq_re\", \"word_freq_edu\",\n    \"word_freq_table\", \"word_freq_conference\", \"char_freq_;\", \"char_freq_(\",\n    \"char_freq_[\", \"char_freq_!\", \"char_freq_$\", \"char_freq_#\",\n    \"capital_run_length_average\", \"capital_run_length_longest\",\n    \"capital_run_length_total\", \"is_spam\"\n  ),\n  show_col_types = FALSE\n)\n\n\nThe target variable:\n\ntarget_name &lt;- \"is_spam\"\n\nLet us call the get_beta_fit() from Chapter 13 to get our priors.\n\npriors_spambase &lt;- get_beta_fit(\n  dataset = tb_spambase, target_name = target_name, seed = 1234\n)\n\nLet us save the results and the dataset:\n\nsave(priors_spambase, file = str_c(\"output/real-data/priors_spambase.rda\"))\nsave(tb_spambase, file = \"output/real-data/tb_spambase.rda\")\n\n\nplot_hist_scores_beta(priors_spambase, \"spambase\")\n\n\n\n\nFigure 14.10: Distribution of estimated probabilities by the GAMSEL model and Beta distribution fitted to the scores of each of the three models, for the spambase dataset.",
    "crumbs": [
      "Real-world Data",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Datasets and Priors</span>"
    ]
  },
  {
    "objectID": "book_real_beta.html#summary",
    "href": "book_real_beta.html#summary",
    "title": "14  Datasets and Priors",
    "section": "14.2 Summary",
    "text": "14.2 Summary\n\n\nCodes to get the key characteristics of the datasets\ndatasets &lt;- tribble(\n  ~name, ~target_name, ~reference,\n  \"abalone\", \"Sex\", \"@misc_abalone_1\",\n  \"adult\", \"high_income\", \"@misc_adult_2\",\n  \"bank\", \"y\", \"@misc_bank_marketing_222\",\n  \"default\", \"default\", \"@misc_default_of_credit_card_clients_350\",\n  \"drybean\", \"is_dermason\", \"@misc_dry_bean_602\",\n  \"coupon\", \"y\", \"@misc_vehicle_coupon_recommendation_603\",\n  \"mushroom\", \"edible\", \"@misc_mushroom_73\",\n  \"occupancy\", \"Occupancy\", \"@misc_occupancy_detection__357\",\n  \"winequality\", \"high_quality\", \"@misc_wine_quality_186\",\n  \"spambase\", \"is_spam\", \"@misc_spambase_94\"\n)\n\ndataset_info &lt;- vector(mode = \"list\", length = nrow(datasets))\nfor (i in 1:nrow(datasets)) {\n  name &lt;- datasets$name[i]\n  target_name &lt;- datasets$target_name[i]\n  current_data &lt;- get(str_c('tb_', name))\n  current_target &lt;- current_data |&gt; pull(!!target_name)\n  current_ref &lt;- datasets$reference[i]\n  n &lt;- nrow(current_data)\n  n_col &lt;- ncol(current_data)\n  n_numeric &lt;- current_data |&gt; select(-!!target_name) |&gt; \n    select(where(is.numeric)) |&gt; \n    ncol()\n  dataset_info[[i]] &lt;- tibble(\n    Dataset = name, \n    n = n, \n    `# features` = n_col-1,\n    `# numeric features` = n_numeric,\n    `Prop. target = 1` = round(sum(current_target == 1) / n, 2),\n    Reference = current_ref\n  )\n}\n\ndataset_info &lt;- list_rbind(dataset_info)\nknitr::kable(dataset_info, booktabs = TRUE, format.args = list(big.mark = \",\"))\n\n\n\n\nTable 14.1: Key characteristics of the datasets.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDataset\nn\n# features\n# numeric features\nProp. target = 1\nReference\n\n\n\n\nabalone\n4,177\n8\n8\n0.37\nNash et al. (1995)\n\n\nadult\n32,561\n14\n6\n0.24\nBecker and Kohavi (1996)\n\n\nbank\n45,211\n16\n7\n0.12\nMoro, Rita, and Cortez (2012)\n\n\ndefault\n30,000\n23\n14\n0.22\nYeh (2016)\n\n\ndrybean\n13,611\n16\n16\n0.26\n“Dry Bean” (2020)\n\n\ncoupon\n12,079\n22\n0\n0.57\n“In-Vehicle Coupon Recommendation” (2020)\n\n\nmushroom\n8,124\n21\n0\n0.52\n“Mushroom” (1987)\n\n\noccupancy\n20,560\n5\n5\n0.23\nCandanedo (2016)\n\n\nwinequality\n6,495\n12\n11\n0.63\nCortez et al. (2009)\n\n\nspambase\n4,601\n57\n57\n0.39\nHopkins et al. (1999)\n\n\n\n\n\n\n\n\n\n\n\n\nBecker, Barry, and Ronny Kohavi. 1996. “Adult.” UCI Machine Learning Repository.\n\n\nCandanedo, Luis. 2016. “Occupancy Detection .” UCI Machine Learning Repository.\n\n\nCortez, Paulo, A. Cerdeira, F. Almeida, T. Matos, and J. Reis. 2009. “Wine Quality.” UCI Machine Learning Repository.\n\n\n“Dry Bean.” 2020. UCI Machine Learning Repository.\n\n\nHopkins, Mark, Erik Reeber, George Forman, and Jaap Suermondt. 1999. “Spambase.” UCI Machine Learning Repository.\n\n\n“In-Vehicle Coupon Recommendation.” 2020. UCI Machine Learning Repository.\n\n\nMoro, S., P. Rita, and P. Cortez. 2012. “Bank Marketing.” UCI Machine Learning Repository.\n\n\n“Mushroom.” 1987. UCI Machine Learning Repository.\n\n\nNash, Warwick, Tracy Sellers, Simon Talbot, Andrew Cawthorn, and Wes Ford. 1995. “Abalone.” UCI Machine Learning Repository.\n\n\nYeh, I-Cheng. 2016. “Default of Credit Card Clients.” UCI Machine Learning Repository.",
    "crumbs": [
      "Real-world Data",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Datasets and Priors</span>"
    ]
  },
  {
    "objectID": "book_real_estimations.html",
    "href": "book_real_estimations.html",
    "title": "15  Estimations",
    "section": "",
    "text": "15.1 Functions",
    "crumbs": [
      "Real-world Data",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Estimations</span>"
    ]
  },
  {
    "objectID": "book_real_estimations.html#functions",
    "href": "book_real_estimations.html#functions",
    "title": "15  Estimations",
    "section": "",
    "text": "15.1.1 Metrics\nLet us define the metrics function presented in Chapter 3.\n\nsource(\"functions/metrics.R\")\n\nThe functions for data pre-processing and Beta distribution fitting are stored in functions/real-data.R (see Chapter 13).\n\nsource(\"functions/real-data.R\")\n\n\n15.1.1.1 Performance and Calibration Metrics\nWe introduce a helper function, get_perf_metrics() (see Chapter 3), which serves as a wrapper for compute_metrics(). This function applies compute_metrics() to the estimated scores from both the training and testing datasets.\n\n\nFunction get_perf_metrics()\n#' Get the performance and calibration metrics for estimated scores\n#'\n#' @param scores_train vector of scores on the train test\n#' @param scores_valid vector of scores on the validation test\n#' @param scores_test vector of scores on the test test\n#' @param tb_train train set\n#' @param tb_valid valdation set\n#' @param tb_test test set\n#' @param target_name name of target variable\nget_perf_metrics &lt;- function(scores_train,\n                             scores_valid,\n                             scores_test,\n                             tb_train,\n                             tb_valid,\n                             tb_test,\n                             target_name) {\n  # We add very small noise to predicted scores\n  # otherwise the local regression may crash\n  scores_train_noise &lt;- scores_train +\n    runif(n = length(scores_train), min = 0, max = 0.01)\n  scores_train_noise[scores_train_noise &gt; 1] &lt;- 1\n  metrics_train &lt;- compute_metrics(\n    obs = tb_train |&gt; pull(!!target_name),\n    scores = scores_train_noise, true_probas = NULL\n  ) |&gt; mutate(sample = \"train\")\n\n  scores_valid_noise &lt;- scores_valid +\n    runif(n = length(scores_valid), min = 0, max = 0.01)\n  scores_valid_noise[scores_valid_noise &gt; 1] &lt;- 1\n  metrics_valid &lt;- compute_metrics(\n    obs = tb_valid |&gt; pull(!!target_name),\n    scores = scores_valid_noise, true_probas = NULL\n  ) |&gt; mutate(sample = \"validation\")\n  \n  scores_test_noise &lt;- scores_test +\n    runif(n = length(scores_test), min = 0, max = 0.01)\n  scores_test_noise[scores_test_noise &gt; 1] &lt;- 1\n  metrics_test &lt;- compute_metrics(\n    obs = tb_test |&gt; pull(!!target_name),\n    scores = scores_test_noise, true_probas = NULL\n  ) |&gt; mutate(sample = \"test\")\n\n  tb_metrics &lt;- metrics_train |&gt;\n    bind_rows(metrics_valid) |&gt; \n    bind_rows(metrics_test)\n  tb_metrics\n}\n\n\n\n\n15.1.1.2 Dispersion Metrics\nWe modify our dispersion_metrics() function (refer to Chapter 3) to replace the vector of simulated true probabilities with the parameters of a Beta distribution. This adjustment allows us to compute the divergence between the model-estimated scores and the specified Beta distribution.\n\n\nFunction dispersion_metrics_beta()\n#' Computes the dispersion and divergence metrics for a vector of scores and\n#' a Beta distribution\n#'\n#' @param shape_1 first parameter of the beta distribution\n#' @param shape_2 second parameter of the beta distribution\n#' @param scores predicted scores\n#'\n#' @returns\n#' \\itemize{\n#'   \\item \\code{inter_quantile_25_75}: Difference of inter-quantile between 25% and 75%\n#'   \\item \\code{inter_quantile_10_90}: Difference of inter-quantile between 10% and 90%\n#'   \\item \\code{KL_10_true_probas}: KL of of predicted probabilities w.r. to true probabilities with 10 bins\n#'   \\item \\code{KL_10_scores}: KL of of true probabilities w.r. to predicted probabilities with 10 bins\n#'   \\item \\code{KL_20_true_probas}: KL of of predicted probabilities w.r. to true probabilities with 20 bins\n#'   \\item \\code{KL_20_scores}: KL of of true probabilities w.r. to predicted probabilities with 20 bins\n#'   \\item \\code{ind_cov}: Difference between the variance of true probabilities and the covariance between true probabilities and predicted scores\n#' }\ndispersion_metrics_beta &lt;- function(shape_1 = 1, shape_2 = 1, scores){\n\n  # Inter-quantiles\n  inter_q_80 &lt;- diff(quantile(scores, c(.9, .1))) /\n    diff(qbeta(c(.9, .1), shape_1, shape_2))\n  inter_q_50 &lt;- diff(quantile(scores, c(.75,.25))) /\n    diff(qbeta(c(.75,.25), shape_1, shape_1))\n\n  # KL divergences\n  m &lt;- 10 # Number of bins\n  h_phat &lt;- hist(scores, breaks = seq(0, 1, length = m + 1), plot = FALSE)\n  h_p &lt;- list(breaks = h_phat$breaks, mids = h_phat$mids)\n  h_p$density = diff(pbeta(h_p$breaks, shape_1, shape_2))\n  h_p$counts =  h_p$density*length(scores)\n\n  # Densities\n  h1 &lt;- rbind(h_phat$density / m, h_p$density / m) # Reference : true probabilities\n  h2 &lt;- rbind(h_p$density / m, h_phat$density / m) # Reference : predicted scores\n  KL_10_true_probas &lt;- distance(\n    h1, method = \"kullback-leibler\", unit = \"log2\", mute.message = TRUE)\n  KL_10_scores &lt;- distance(\n    h2, method = \"kullback-leibler\", unit = \"log2\", mute.message = TRUE)\n\n\n  m &lt;- 20 # Number of bins\n  h_phat &lt;- hist(scores, breaks = seq(0, 1, length = m + 1), plot = FALSE)\n  h_p &lt;- list(breaks = h_phat$breaks, mids = h_phat$mids)\n  h_p$density = diff(pbeta(h_p$breaks, shape_1, shape_2))\n  h_p$counts =  h_p$density * length(scores)\n  # Densities\n  h1 &lt;- rbind(h_phat$density / m, h_p$density) # Reference : true probabilities\n  h2 &lt;- rbind(h_p$density, h_phat$density / m) # Reference : predicted scores\n  KL_20_true_probas &lt;- distance(\n    h1, method = \"kullback-leibler\", unit = \"log2\", mute.message = TRUE)\n  KL_20_scores &lt;- distance(\n    h2, method = \"kullback-leibler\", unit = \"log2\", mute.message = TRUE)\n\n  # Indicator of the difference between variance and covariance\n  var_p &lt;- shape_1 * shape_2 / ((shape_1 + shape_2)^2 * (shape_1 + shape_2 + 1))\n  cov_p_phat &lt;- cov(\n    qbeta(\n      rank(scores, ties.method = \"average\") / (1 + length(scores)),\n      shape_1,\n      shape_2),\n    scores\n  )\n  ind_cov &lt;- abs(cov_p_phat - var_p)\n\n  # Collection\n  dispersion_metrics &lt;- tibble(\n    \"inter_quantile_25_75\" = as.numeric(inter_q_50),\n    \"inter_quantile_10_90\" = as.numeric(inter_q_80),\n    \"KL_10_true_probas\" = as.numeric(KL_10_true_probas),\n    \"KL_10_scores\" = as.numeric(KL_10_scores),\n    \"KL_20_true_probas\" = as.numeric(KL_20_true_probas),\n    \"KL_20_scores\" = as.numeric(KL_20_scores),\n    \"ind_cov\" = ind_cov\n  )\n\n  dispersion_metrics\n}\n\n\nWe also introduce a helper function that acts as a wrapper for dispersion_metrics_beta(), which calculates divergence metrics for three prior distributions. These distributions are Beta distributions, with shape parameters derived from scores estimated by GLM, GAM, and GAMSEL models, respectively (see Chapter 14).\n\n\nFunction disp_metrics_dataset()\n#' Computes the dispersion and divergence metrics between estimated scores and\n#' the Beta distributions whose parameters were estimated using scores estimated\n#' with a GLM-loistic, a GAM and a GAM with model selection.\n#' (helper function)\n#'\n#' @param priors priors obtained with `get_beta_fit()`\n#' @param scores estimated scores from a model\ndisp_metrics_dataset &lt;- function(priors, scores) {\n  # GLM priors\n  shape_1_glm &lt;- priors$mle_glm$estimate[\"shape1\"]\n  shape_2_glm &lt;- priors$mle_glm$estimate[\"shape2\"]\n  # GAM priors\n  shape_1_gam &lt;- priors$mle_gam$estimate[\"shape1\"]\n  shape_2_gam &lt;- priors$mle_gam$estimate[\"shape2\"]\n  # GAMSEL priors\n  shape_1_gamsel &lt;- priors$mle_gamsel$estimate[\"shape1\"]\n  shape_2_gamsel &lt;- priors$mle_gamsel$estimate[\"shape2\"]\n\n  # Divergence metrics\n  dist_prior_glm &lt;- dispersion_metrics_beta(\n    shape_1 = shape_1_glm, shape_2 = shape_2_glm, scores = scores\n  )\n  dist_prior_gam &lt;- dispersion_metrics_beta(\n    shape_1 = shape_1_gam, shape_2 = shape_2_gam, scores = scores\n  )\n  dist_prior_gamsel &lt;- dispersion_metrics_beta(\n    shape_1 = shape_1_gamsel, shape_2 = shape_2_gamsel, scores = scores\n  )\n\n  dist_prior_glm |&gt;\n    mutate(prior = \"glm\", shape_1 = shape_1_glm, shape_2 = shape_2_glm) |&gt;\n    bind_rows(\n      dist_prior_gam |&gt;\n        mutate(prior = \"gam\", shape_1 = shape_1_gam, shape_2 = shape_2_gam)\n    ) |&gt;\n    bind_rows(\n      dist_prior_gamsel |&gt;\n        mutate(\n          prior = \"gamsel\", shape_1 = shape_1_gamsel, shape_2 = shape_2_gamsel\n        )\n    )\n}\n\n\nWe also define helper functions, as detailed in Chapter 9, to generate histograms of estimated scores and to calculate \\(\\mathbb{P}(q_1 &lt; \\mathbf{s}(\\mathbb{x}) &lt; q_2)\\).\n\n\nHelper Functions\n#' Counts the number of scores in each of the 20 equal-sized bins over [0,1]\n#'\n#' @param scores_train vector of scores on the train test\n#' @param scores_valid vector of scores on the validation test\n#' @param scores_test vector of scores on the test test\nget_histogram &lt;- function(scores_train,\n                          scores_valid,\n                          scores_test) {\n  breaks &lt;- seq(0, 1, by = .05)\n  scores_train_hist &lt;- hist(scores_train, breaks = breaks, plot = FALSE)\n  scores_valid_hist &lt;- hist(scores_valid, breaks = breaks, plot = FALSE)\n  scores_test_hist &lt;- hist(scores_test, breaks = breaks, plot = FALSE)\n  scores_hist &lt;- list(\n    train = scores_train_hist,\n    valid = scores_valid_hist,\n    test = scores_test_hist\n  )\n  scores_hist\n}\n\n#' Estimation of P(q1 &lt; score &lt; q2)\n#'\n#' @param scores_train vector of scores on the train test\n#' @param scores_valid vector of scores on the validation test\n#' @param scores_test vector of scores on the test test\n#' @param q1 vector of desired values for q1 (q2 = 1-q1)\nestim_prop &lt;- function(scores_train,\n                       scores_valid,\n                       scores_test,\n                       q1 = c(.1, .2, .3, .4)) {\n  proq_scores_train &lt;- map(\n    q1,\n    ~prop_btw_quantiles(s = scores_train, q1 = .x)\n  ) |&gt;\n    list_rbind() |&gt;\n    mutate(sample = \"train\")\n  proq_scores_valid &lt;- map(\n    q1,\n    ~prop_btw_quantiles(s = scores_valid, q1 = .x)\n  ) |&gt;\n    list_rbind() |&gt;\n    mutate(sample = \"validation\")\n  proq_scores_test &lt;- map(\n    q1,\n    ~prop_btw_quantiles(s = scores_test, q1 = .x)\n  ) |&gt;\n    list_rbind() |&gt;\n    mutate(sample = \"test\")\n\n  proq_scores_train |&gt;\n    bind_rows(proq_scores_valid) |&gt; \n    bind_rows(proq_scores_test)\n}\n\n\n\n\n15.1.1.3 Wrapper Metrics Function\nFor convenience, we define a function, get_metrics_simul(), which calculates performance metrics, calibration metrics, and divergence metrics for estimated scores on both the train and test sets.\n\n\nFunction get_metrics_simul()\n#' Get the performance/calibration/dispersion/divergence metrics\n#' given estimated scores on the train, validation, and test sets\n#'\n#' @param scores_train scores estimated on train set\n#' @param scores_valid scores estimated on validation set\n#' @param scores_test scores estimated on test set\n#' @param tb_train train set\n#' @param tb_valid validation set\n#' @param tb_test test set\n#' @param target_name name of the target variable\n#' @param priors priors\n#'\n#' @returns A list with 4 elements:\n#'  - `tb_metrics`: performance / calibration metrics\n#'  - `tb_disp_metrics`: disp and div metrics\n#'  - `tb_prop_scores`: table with P(q1 &lt; score &lt; q2)\n#'  - `scores_hist`: histogram of scores\nget_metrics_simul &lt;- function(scores_train,\n                              scores_valid,\n                              scores_test,\n                              tb_train,\n                              tb_valid,\n                              tb_test,\n                              priors,\n                              target_name) {\n  ## Histogram of scores----\n  scores_hist &lt;- get_histogram(scores_train, scores_valid, scores_test)\n\n  # Performance and Calibration Metrics----\n  tb_metrics &lt;- get_perf_metrics(\n    scores_train = scores_train,\n    scores_valid = scores_valid,\n    scores_test = scores_test,\n    tb_train = tb_train,\n    tb_valid = tb_valid,\n    tb_test = tb_test,\n    target_name = target_name)\n\n  # Dispersion Metrics----\n  tb_disp_metrics &lt;-\n    disp_metrics_dataset(priors = priors, scores = scores_train) |&gt;\n    mutate(sample = \"train\") |&gt;\n    bind_rows(\n      disp_metrics_dataset(priors = priors, scores = scores_valid) |&gt;\n        mutate(sample = \"validation\")\n    ) |&gt; \n    bind_rows(\n      disp_metrics_dataset(priors = priors, scores = scores_test) |&gt;\n        mutate(sample = \"test\")\n    )\n\n  # Estimation of P(q1 &lt; score &lt; q2)----\n  tb_prop_scores &lt;- estim_prop(\n    scores_train = scores_train,\n    scores_valid = scores_valid,\n    scores_test = scores_test\n  )\n\n  list(\n    tb_metrics = tb_metrics,           # performance / calibration metrics\n    tb_disp_metrics = tb_disp_metrics, # disp and div metrics\n    tb_prop_scores = tb_prop_scores,   # table with P(q1 &lt; score &lt; q2)\n    scores_hist = scores_hist          # histogram of scores\n  )\n}\n\n\n\n\n\n15.1.2 Estimation Functions\nIn Section 15.3, we will train Random Forests and XGB models on various datasets. To accommodate multiple sets of hyperparameters for each model, we must define two functions: one that estimates a model for a single set of hyperparameters, and another that iterates this estimation across all hyperparameter sets. Additionally, for the XGB model, a third function is required to calculate metrics at each boosting iteration and a fourth one to predict the scores (this fourth function is defined because of issues arising with parallel computations and the serialization of the objects from the {xgboost} package).\n\n15.1.2.1 Random Forests\nFor a specific dataset, we train a random forest using various hyperparameter sets. We adjust the minimum number of observations per terminal leaf node (unique(round(2^seq(1, 14, by = .4)))) and the number of candidate variables considered for each split (2, 4, or 10). We fix the number of trees in the forest at 500.\nThe simul_forest_real() function estimates multiple forests for a given dataset. It relies on the simul_forest_helper() function, which trains a forest using a specific set of hyperparameters. Each forest is trained using the same training sample. Performance metrics are computed for each forest on both the training and testing samples.\n\n\nFunction simul_forest_real()\n#' Train a random forest on a dataset for a binary task for various\n#' hyperparameters and computes metrics based on scores and on a set of prior\n#' distributions of the underlying probability\n#'\n#' @param data dataset\n#' @param target_name name of the target variable\n#' @param priors priors obtained with `get_beta_fit()`\n#' @param seed desired seed (default to `NULL`)\n#'\n#' @returns A list with two elements:\n#'  - `res`: results for each estimated model of the grid. Each element is a\n#'   list with the following 4 arguments:\n#'      - `tb_metrics`: performance / calibration metrics\n#'      - `tb_disp_metrics`: disp and div metrics\n#'      - `tb_prop_scores`: table with P(q1 &lt; score &lt; q2)\n#'      - `scores_hist`: histogram of scores.\n#'  - `grid`: the grid search.\nsimul_forest_real &lt;- function(data,\n                              target_name,\n                              priors,\n                              seed = NULL) {\n\n  if (!is.null(seed)) set.seed(seed)\n\n  min_bucket_values &lt;- unique(round(2^seq(1, 14, by = .4)))\n  min_bucket_values &lt;- min_bucket_values[min_bucket_values &lt;=  nrow(data)]\n  \n  mtry &lt;- c(2, 4, 10)\n  mtry &lt;- mtry[mtry &lt;= ncol(data)]\n  \n  # Grid for hyperparameters\n  grid &lt;- expand_grid(\n    mtry = mtry,\n    num_trees = 500,\n    min_node_size = min_bucket_values\n  ) |&gt;\n    mutate(ind = row_number())\n\n  # Split data into train and test set\n  data_splitted &lt;- split_train_test(data = data, prop_train = .8, seed = seed)\n  \n  data_encoded &lt;- encode_dataset(\n    data_train = data_splitted$train,\n    data_test = data_splitted$test,\n    target_name = target_name,\n    intercept = FALSE\n  )\n  \n  # Further split train intro two samples (train/valid)\n  data_splitted_train &lt;- \n    split_train_test(data = data_encoded$train, prop_train = .8, seed = seed)\n\n  progressr::with_progress({\n    p &lt;- progressr::progressor(steps = nrow(grid))\n    res_grid &lt;- furrr::future_map(\n      .x = seq_len(nrow(grid)),\n      .f = ~{\n        p()\n        simul_forest_helper(\n          data_train = data_splitted_train$train,\n          data_valid = data_splitted_train$test,\n          data_test = data_encoded$test,\n          target_name = target_name,\n          params = grid |&gt; dplyr::slice(.x),\n          priors = priors\n        )\n      },\n      .options = furrr::furrr_options(seed = NULL)\n    )\n  })\n\n  list(\n    res = res_grid,\n    grid = grid\n  )\n}\n\n\n\n\nFunction simul_forest_helper()\n#' Fit a random forest and returns metrics based on scores. The divergence\n#' metrics are obtained using the prior distributions.\n#'\n#' @param data_train train set\n#' @param data_valid validation set\n#' @param data_test test set\n#' @param target_name name of the target variable\n#' @param parms tibble with hyperparameters for the current estimation\n#' @param priors priors obtained with `get_beta_fit()`\n#'\n#' @returns A list with 4 elements:\n#'  - `tb_metrics`: performance / calibration metrics\n#'  - `tb_disp_metrics`: disp and div metrics\n#'  - `tb_prop_scores`: table with P(q1 &lt; score &lt; q2)\n#'  - `scores_hist`: histogram of scores\nsimul_forest_helper &lt;- function(data_train,\n                                data_valid,\n                                data_test,\n                                target_name,\n                                params,\n                                priors) {\n  ## Estimation----\n  fit_rf &lt;- ranger(\n    str_c(target_name, \" ~ .\"),\n    data = data_train,\n    min.bucket = params$min_node_size,\n    mtry = params$mtry,\n    num.trees = params$num_trees\n  )\n  ## Predicted scores----\n  scores_train &lt;- predict(fit_rf, data = data_train, type = \"response\")$predictions\n  scores_valid &lt;- predict(fit_rf, data = data_valid, type = \"response\")$predictions\n  scores_test &lt;- predict(fit_rf, data = data_test, type = \"response\")$predictions\n  ## Metrics----\n  metrics &lt;- get_metrics_simul(\n    scores_train = scores_train,\n    scores_valid = scores_valid,\n    scores_test = scores_test,\n    tb_train = data_train,\n    tb_valid = data_valid,\n    tb_test = data_test,\n    priors = priors,\n    target_name = target_name\n  )\n  # Add index of the grid search\n  metrics$tb_metrics &lt;- metrics$tb_metrics |&gt; mutate(ind = params$ind)\n  metrics$tb_disp_metrics &lt;- metrics$tb_disp_metrics |&gt; mutate(ind = params$ind)\n  metrics$tb_prop_scores &lt;- metrics$tb_prop_scores |&gt; mutate(ind = params$ind)\n  metrics$scores_hist$ind &lt;- params$ind\n\n  metrics\n}\n\n\n\n\n15.1.2.2 Extreme Gradient Boosting\nWe train XGB models on various datasets, varying the maximum tree depth (with values of 2, 4, or 6) and the number of boosting iterations (ranging from 2 to 500). At each iteration, we compute metrics based on scores from both the train and test samples.\nThe simul_xgb_real() function estimates all XGB models for a specific dataset, iterating over a grid of hyperparameters. For each hyperparameter set, it calls the simul_xgb_helper() function to estimate the model. During each boosting iteration, metrics are calculated using scores from the get_metrics_xgb_iter() and predict_score_iter() functions.\n\n\nFunction simul_xgb_real()\n#' Train an XGB on a dataset for a binary task for various\n#' hyperparameters and computes metrics based on scores and on a set of prior\n#' distributions of the underlying probability\n#'\n#' @param data dataset\n#' @param target_name name of the target variable\n#' @param priors priors obtained with `get_beta_fit()`\n#' @param seed desired seed (default to `NULL`)\n#'\n#' @returns A list with two elements:\n#'  - `res`: results for each estimated model of the grid. Each element is a\n#'  list with the following elements:\n#'      - `tb_metrics`: performance / calibration metrics\n#'      - `tb_disp_metrics`: disp and div metrics\n#'      - `tb_prop_scores`: table with P(q1 &lt; score &lt; q2)\n#'      - `scores_hist`: histogram of scores.\n#'  - `grid`: the grid search.\nsimul_xgb_real &lt;- function(data,\n                           target_name,\n                           priors,\n                           seed = NULL) {\n\n  if (!is.null(seed)) set.seed(seed)\n\n  # Grid for hyperparameters\n  grid &lt;- expand_grid(\n    max_depth = c(2, 4, 6),\n    nb_iter_total = 500,\n    eta = 0.3\n  ) |&gt;\n    mutate(ind = row_number())\n\n  # Split data into train and test set\n  data_splitted &lt;- split_train_test(data = data, prop_train = .8, seed = seed)\n  data_encoded &lt;- encode_dataset(\n    data_train = data_splitted$train,\n    data_test = data_splitted$test,\n    target_name = target_name,\n    intercept = FALSE\n  )\n\n  # Further split train intro two samples (train/valid)\n  data_splitted_train &lt;- \n    split_train_test(data = data_encoded$train, prop_train = .8, seed = seed)\n  \n  res_grid &lt;- vector(mode = \"list\", length = nrow(grid))\n  for (i_grid in 1:nrow(grid)) {\n    res_grid[[i_grid]] &lt;- simul_xgb_helper(\n      data_train = data_splitted_train$train,\n      data_valid = data_splitted_train$test,\n      data_test = data_encoded$test,\n      target_name = target_name,\n      params = grid |&gt; dplyr::slice(i_grid),\n      priors = priors\n    )\n  }\n\n  list(\n    res = res_grid,\n    grid = grid\n  )\n}\n\n\n\n\nFunction get_metrics_xgb_iter()\n#' Get the metrics based on scores estimated at a given boosting iteration\n#'\n#' @param scores scores estimated a boosting iteration `nb_iter` (list with\n#'   train and test scores, returned by `predict_score_iter()`)\n#' @param data_train train set\n#' @param data_valid validation set\n#' @param data_test test set\n#' @param target_name name of the target variable\n#' @param ind index of the grid search\n#' @param nb_iter boosting iteration to consider\n#'\n#' @returns A list with 4 elements:\n#'  - `tb_metrics`: performance / calibration metrics\n#'  - `tb_disp_metrics`: disp and div metrics\n#'  - `tb_prop_scores`: table with P(q1 &lt; score &lt; q2)\n#'  - `scores_hist`: histogram of scores\nget_metrics_xgb_iter &lt;- function(scores,\n                                 data_train,\n                                 data_valid,\n                                 data_test,\n                                 target_name,\n                                 ind,\n                                 nb_iter) {\n\n  scores_train &lt;- scores$scores_train\n  scores_valid &lt;- scores$scores_valid\n  scores_test &lt;- scores$scores_test\n\n  ## Metrics----\n  metrics &lt;- get_metrics_simul(\n    scores_train = scores_train,\n    scores_valid = scores_valid,\n    scores_test = scores_test,\n    tb_train = data_train,\n    tb_valid = data_valid,\n    tb_test = data_test,\n    priors = priors,\n    target_name = target_name\n  )\n  # Add index of the grid search\n  metrics$tb_metrics &lt;- metrics$tb_metrics |&gt;\n    mutate(ind = ind, nb_iter = nb_iter)\n  metrics$tb_disp_metrics &lt;- metrics$tb_disp_metrics |&gt;\n    mutate(ind = ind, nb_iter = nb_iter)\n  metrics$tb_prop_scores &lt;- metrics$tb_prop_scores |&gt;\n    mutate(ind = ind, nb_iter = nb_iter)\n  metrics$scores_hist$ind &lt;- ind\n  metrics$scores_hist$nb_iter &lt;- nb_iter\n\n  metrics\n}\n\n\n\n\nFunction predict_score_iter()\n#' Predicts the scores at a given iteration of the XGB model\n#'\n#' @param fit_xgb estimated XGB model\n#' @param tb_train_xgb train set\n#' @param tb_valid_xgb validation set\n#' @param tb_test_xgb test set\n#' @param ind index of the grid search\n#' @param nb_iter boosting iteration to consider\n#'\n#' @returns A list with three elements: `scores_train`, `scores_valid`, and\n#'  `scores_train` which contain the estimated scores on the train and on the \n#'  test score, resp.\npredict_score_iter &lt;- function(fit_xgb,\n                               tb_train_xgb,\n                               tb_valid_xgb,\n                               tb_test_xgb,\n                               nb_iter) {\n\n  ## Predicted scores----\n  scores_train &lt;- predict(fit_xgb, tb_train_xgb, iterationrange = c(1, nb_iter))\n  scores_valid &lt;- predict(fit_xgb, tb_valid_xgb, iterationrange = c(1, nb_iter))\n  scores_test &lt;- predict(fit_xgb, tb_test_xgb, iterationrange = c(1, nb_iter))\n\n  list(\n    scores_train = scores_train,\n    scores_valid = scores_valid,\n    scores_test = scores_test\n  )\n}\n\n\n\n\nFunction simul_xgb_helper()\n#' Fit an XGB and returns metrics based on scores. The divergence metrics are\n#' obtained using the prior distributions.\n#'\n#' @param data_train train set\n#' @param data_valid validation set\n#' @param data_test test set\n#' @param target_name name of the target variable\n#' @param parms tibble with hyperparameters for the current estimation\n#' @param priors priors obtained with `get_beta_fit()`\n#'\n#' @returns A list with 4 elements:\n#'  - `tb_metrics`: performance / calibration metrics\n#'  - `tb_disp_metrics`: disp and div metrics\n#'  - `tb_prop_scores`: table with P(q1 &lt; score &lt; q2)\n#'  - `scores_hist`: histogram of scores\nsimul_xgb_helper &lt;- function(data_train,\n                             data_valid,\n                             data_test,\n                             target_name,\n                             params,\n                             priors) {\n\n  ## Format data for xgboost----\n  tb_train_xgb &lt;- xgb.DMatrix(\n    data = data_train |&gt; dplyr::select(-!!target_name) |&gt; as.matrix(),\n    label = data_train |&gt; dplyr::pull(!!target_name) |&gt; as.matrix()\n  )\n  tb_valid_xgb &lt;- xgb.DMatrix(\n    data = data_valid |&gt; dplyr::select(-!!target_name) |&gt; as.matrix(),\n    label = data_valid |&gt; dplyr::pull(!!target_name) |&gt; as.matrix()\n  )\n  tb_test_xgb &lt;- xgb.DMatrix(\n    data = data_test |&gt; dplyr::select(-!!target_name) |&gt; as.matrix(),\n    label = data_test |&gt; dplyr::pull(!!target_name) |&gt; as.matrix()\n  )\n  # Parameters for the algorithm\n  param &lt;- list(\n    max_depth = params$max_depth, #Note: root node is indexed 0\n    eta = params$eta,\n    nthread = 1,\n    objective = \"binary:logistic\",\n    eval_metric = \"auc\"\n  )\n  watchlist &lt;- list(train = tb_train_xgb, eval = tb_valid_xgb)\n  ## Estimation----\n  fit_xgb &lt;- xgb.train(\n    param, tb_train_xgb,\n    nrounds = params$nb_iter_total,\n    watchlist,\n    verbose = 0\n  )\n\n  # First, we estimate the scores at each boosting iteration\n  # As the xgb.Dmatrix objects cannot be easily serialised, we first estimate\n  # these scores in a classical way, without parallelism...\n  scores_iter &lt;- vector(mode = \"list\", length = params$nb_iter_total)\n  for (i_iter in 1:params$nb_iter_total) {\n    scores_iter[[i_iter]] &lt;- predict_score_iter(\n      fit_xgb = fit_xgb,\n      tb_train_xgb = tb_train_xgb,\n      tb_valid_xgb = tb_valid_xgb,\n      tb_test_xgb = tb_test_xgb,\n      nb_iter = i_iter)\n  }\n\n  # Then, to compute the metrics, as it is a bit slower, we can use parallelism\n\n  ncl &lt;- detectCores() - 1\n  (cl &lt;- makeCluster(ncl))\n  clusterEvalQ(cl, {\n    library(tidyverse)\n    library(locfit)\n    library(philentropy)\n  }) |&gt;\n    invisible()\n\n  clusterExport(cl, c(\n    \"scores_iter\", \"data_train\", \"data_valid\", \"data_test\", \"priors\", \"params\", \n    \"target_name\"\n  ), envir = environment())\n  clusterExport(cl, c(\n    \"get_metrics_xgb_iter\", \"get_metrics_simul\", \"get_histogram\",\n    \"brier_score\",\n    \"get_perf_metrics\", \"compute_metrics\",\n    \"disp_metrics_dataset\", \"dispersion_metrics_beta\", \"prop_btw_quantiles\",\n    \"estim_prop\"\n  ))\n\n  metrics_iter &lt;-\n    pbapply::pblapply(\n      X = seq_len(params$nb_iter_total),\n      FUN = function(i_iter) {\n        get_metrics_xgb_iter(\n          scores = scores_iter[[i_iter]],\n          data_train = data_train,\n          data_valid = data_valid,\n          data_test = data_test,\n          target_name = target_name,\n          ind = params$ind,\n          nb_iter = i_iter\n        )\n      },\n      cl = cl\n    )\n  stopCluster(cl)\n\n  # Merge tibbles from each iteration into a single one\n  tb_metrics &lt;-\n    map(metrics_iter, \"tb_metrics\") |&gt;\n    list_rbind()\n  tb_disp_metrics &lt;-\n    map(metrics_iter, \"tb_disp_metrics\") |&gt;\n    list_rbind()\n  tb_prop_scores &lt;-\n    map(metrics_iter, \"tb_prop_scores\") |&gt;\n    list_rbind()\n  scores_hist &lt;- map(metrics_iter, \"scores_hist\")\n\n  list(\n    tb_metrics = tb_metrics,\n    tb_disp_metrics = tb_disp_metrics,\n    tb_prop_scores = tb_prop_scores,\n    scores_hist = scores_hist\n  )\n}\n\n\n\n\n15.1.2.3 GLM\nWe define a function to train a GLM model on a dataset and compute metrics on the estimated scores.\n\n\nFunction simul_glm()\n#' Train GLM (with logistic link) on a dataset for a binary task\n#' and computes metrics based on scores and on a set of prior\n#' distributions of the underlying probability (assumed to be \"true\" probs)\n#'\n#' @param data dataset\n#' @param target_name name of the target variable\n#' @param priors priors obtained with `get_beta_fit()`\n#' @param seed desired seed (default to `NULL`)\n#'\n#' @returns A list with one elements:\n#'  - `res`: results for each estimated model of the grid. Each element is a\n#'   list with the following 4 arguments:\n#'      - `tb_metrics`: performance / calibration metrics\n#'      - `tb_disp_metrics`: disp and div metrics\n#'      - `tb_prop_scores`: table with P(q1 &lt; score &lt; q2)\n#'      - `scores_hist`: histogram of scores.\nsimul_glm &lt;- function(data,\n                      target_name,\n                      priors,\n                      seed = NULL) {\n  \n  # Split data into train and test set\n  data_splitted &lt;- split_train_test(data = data, prop_train = .8, seed = seed)\n  \n  # Further split train intro two samples (train/valid)\n  # Should not be done, but we need to do it here to have comparable results\n  # with ml models\n  data_splitted_train &lt;- split_train_test(\n    data = data_splitted$train, prop_train = .8, seed = seed\n  )\n  \n  ## Estimation----\n  fit &lt;- train_glm(\n    data_train = data_splitted_train$train, \n    data_test = data_splitted$test, \n    target_name = target_name,\n    return_model = FALSE\n  )\n  \n  ## Predicted scores----\n  scores_train &lt;- fit$scores_train\n  scores_test &lt;- fit$scores_test\n  ## Metrics----\n  metrics &lt;- get_metrics_simul(\n    scores_train = scores_train,\n    scores_valid = scores_test,# will not be used, sorry it is dirty\n    scores_test = scores_test,\n    tb_train = data_splitted_train$train,\n    tb_valid = data_splitted$test,# same here, will not be used\n    tb_test = data_splitted$test,\n    priors = priors,\n    target_name = target_name\n  )\n  \n  # Remove wrong info on validation sample, since there is no validation sample\n  metrics$tb_metrics &lt;- \n    metrics$tb_metrics |&gt; filter(sample != \"validation\")\n  metrics$tb_disp_metrics &lt;- \n    metrics$tb_disp_metrics |&gt; filter(sample != \"validation\")\n  metrics$tb_prop_scores &lt;- \n    metrics$tb_prop_scores |&gt; filter(sample != \"validation\")\n  metrics$scores_hist$valid &lt;- NULL\n  metrics\n}\n\n\n\n\n15.1.2.4 GAM\nWe define a function to train a GAM model on a dataset and compute metrics on the estimated scores.\n\n\nFunction simul_gam()\n#' Train GAM on a dataset for a binary task\n#' and computes metrics based on scores and on a set of prior\n#' distributions of the underlying probability (assumed to be \"true\" probs)\n#'\n#' @param data dataset\n#' @param target_name name of the target variable\n#' @param spline_df degree of freedom for the splines\n#' @param priors priors obtained with `get_beta_fit()`\n#' @param seed desired seed (default to `NULL`)\n#'\n#' @returns A list with one elements:\n#'  - `res`: results for each estimated model of the grid. Each element is a\n#'   list with the following 4 arguments:\n#'      - `tb_metrics`: performance / calibration metrics\n#'      - `tb_disp_metrics`: disp and div metrics\n#'      - `tb_prop_scores`: table with P(q1 &lt; score &lt; q2)\n#'      - `scores_hist`: histogram of scores.\nsimul_gam &lt;- function(data,\n                      target_name,\n                      spline_df = 6,\n                      priors,\n                      seed = NULL) {\n  \n  # Split data into train and test set\n  data_splitted &lt;- split_train_test(data = data, prop_train = .8, seed = seed)\n  \n  # Further split train intro two samples (train/valid)\n  # Should not be done, but we need to do it here to have comparable results\n  # with ml models\n  data_splitted_train &lt;- split_train_test(\n    data = data_splitted$train, prop_train = .8, seed = seed\n  )\n  \n  ## Estimation----\n  fit &lt;- train_gam(\n    data_train = data_splitted_train$train,\n    data_test = data_splitted$test, \n    target_name = target_name,\n    spline_df = spline_df,\n    return_model = FALSE\n  )\n  \n  ## Predicted scores----\n  scores_train &lt;- fit$scores_train\n  scores_test &lt;- fit$scores_test\n  ## Metrics----\n  metrics &lt;- get_metrics_simul(\n    scores_train = scores_train,\n    scores_valid = scores_test,# will not be used, sorry it is dirty\n    scores_test = scores_test,\n    tb_train = data_splitted_train$train,\n    tb_valid = data_splitted$test,# same here, will not be used\n    tb_test = data_splitted$test,\n    priors = priors,\n    target_name = target_name\n  )\n  \n  # Remove wrong info on validation sample, since there is no validation sample\n  metrics$tb_metrics &lt;- \n    metrics$tb_metrics |&gt; filter(sample != \"validation\")\n  metrics$tb_disp_metrics &lt;- \n    metrics$tb_disp_metrics |&gt; filter(sample != \"validation\")\n  metrics$tb_prop_scores &lt;- \n    metrics$tb_prop_scores |&gt; filter(sample != \"validation\")\n  metrics$scores_hist$valid &lt;- NULL\n  metrics\n}\n\n\n\n\n15.1.2.5 GAMSEL\nWe define a function to train a GAMSEL model on a dataset and compute metrics on the estimated scores.\n\n\nFunction simul_gamsel()\n#' Train GAMSEL on a dataset for a binary task\n#' and computes metrics based on scores and on a set of prior\n#' distributions of the underlying probability (assumed to be \"true\" probs)\n#'\n#' @param data dataset\n#' @param target_name name of the target variable\n#' @param degrees degree for the splines\n#' @param priors priors obtained with `get_beta_fit()`\n#' @param seed desired seed (default to `NULL`)\n#'\n#' @returns A list with one elements:\n#'  - `res`: results for each estimated model of the grid. Each element is a\n#'   list with the following 4 arguments:\n#'      - `tb_metrics`: performance / calibration metrics\n#'      - `tb_disp_metrics`: disp and div metrics\n#'      - `tb_prop_scores`: table with P(q1 &lt; score &lt; q2)\n#'      - `scores_hist`: histogram of scores.\nsimul_gamsel &lt;- function(data,\n                         target_name,\n                         degrees = 6,\n                         priors,\n                         seed = NULL) {\n  \n  # Split data into train and test set\n  data_splitted &lt;- split_train_test(data = data, prop_train = .8, seed = seed)\n  \n  # Further split train intro two samples (train/valid)\n  # Should not be done, but we need to do it here to have comparable results\n  # with ml models\n  data_splitted_train &lt;- split_train_test(\n    data = data_splitted$train, prop_train = .8, seed = seed\n  )\n  \n  ## Estimation----\n  fit &lt;- train_gamsel(\n    data_train = data_splitted_train$train, \n    data_test = data_splitted$test, \n    target_name = target_name,\n    degrees = degrees,\n    return_model = FALSE\n  )\n  \n  ## Predicted scores----\n  scores_train &lt;- fit$scores_train\n  scores_test &lt;- fit$scores_test\n  \n  ind_na_test &lt;- which(is.na(scores_test))\n  if (length(ind_na_test) &gt; 0) {\n    scores_test &lt;- scores_test[-ind_na_test]\n    data_splitted$test &lt;- data_splitted$test[-ind_na_test,]\n  }\n  \n  ## Metrics----\n  metrics &lt;- get_metrics_simul(\n    scores_train = scores_train,\n    scores_valid = scores_test,# will not be used, sorry it is dirty\n    scores_test = scores_test,\n    tb_train = data_splitted_train$train,\n    tb_valid = data_splitted$test,# same here, will not be used\n    tb_test = data_splitted$test,\n    priors = priors,\n    target_name = target_name\n  )\n  \n  # Remove wrong info on validation sample, since there is no validation sample\n  metrics$tb_metrics &lt;- \n    metrics$tb_metrics |&gt; filter(sample != \"validation\")\n  metrics$tb_disp_metrics &lt;- \n    metrics$tb_disp_metrics |&gt; filter(sample != \"validation\")\n  metrics$tb_prop_scores &lt;- \n    metrics$tb_prop_scores |&gt; filter(sample != \"validation\")\n  metrics$scores_hist$valid &lt;- NULL\n  metrics\n}",
    "crumbs": [
      "Real-world Data",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Estimations</span>"
    ]
  },
  {
    "objectID": "book_real_estimations.html#data",
    "href": "book_real_estimations.html#data",
    "title": "15  Estimations",
    "section": "15.2 Data",
    "text": "15.2 Data\nIn Chapter 14, we estimated the shapes of Beta distributions using fitted scores from three models (GLM, GAM, and GAMSEL) applied to various datasets from the UCI Machine Learning Repository. We saved these estimated priors and the datasets in R data files, which can now be easily loaded.\nThe list of datasets and the name of the target variable:\n\ndatasets &lt;- tribble(\n  ~name, ~target_name,\n  \"abalone\", \"Sex\",\n  \"adult\", \"high_income\",\n  \"bank\", \"y\",\n  \"default\", \"default\",\n  \"drybean\", \"is_dermason\",\n  \"coupon\", \"y\",\n  \"mushroom\", \"edible\",\n  \"occupancy\", \"Occupancy\",\n  \"winequality\", \"high_quality\",\n  \"spambase\", \"is_spam\"\n)\n\n\nfor (name in datasets$name) {\n  # The data\n  load(str_c(\"output/real-data/tb_\", name, \".rda\"))\n  # The Prior on the distribution of the scores\n  load(str_c(\"output/real-data/priors_\", name, \".rda\"))\n}",
    "crumbs": [
      "Real-world Data",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Estimations</span>"
    ]
  },
  {
    "objectID": "book_real_estimations.html#sec-real-estimations-estimations",
    "href": "book_real_estimations.html#sec-real-estimations-estimations",
    "title": "15  Estimations",
    "section": "15.3 Estimations",
    "text": "15.3 Estimations\nThe models are estimated in parallel. The number of available cores can be determined using the following command:\n\nlibrary(future)\nnb_cores &lt;- future::availableCores() - 1\n\nWe use the following loop to estimate all the models across each dataset:\n\nseed &lt;- 1234\nfor (name in datasets$name) {\n  current_data &lt;- get(str_c(\"tb_\", name))\n  current_priors &lt;- get(str_c(\"priors_\", name))\n  current_target_name &lt;- datasets |&gt;\n    filter(name == !!name) |&gt; pull(target_name)\n  ## Random Forests----\n  plan(multisession, workers = nb_cores)\n  rf_resul &lt;- simul_forest_real(\n    data = current_data,\n    target_name = current_target_name,\n    priors = current_priors,\n    seed = seed\n  )\n  save(rf_resul, file = str_c(\"output/real-data/rf_resul_\", name, \".rda\"))\n\n  ## Extreme Gradient Boosting----\n  xgb_resul &lt;- simul_xgb_real(\n    data = current_data,\n    target_name = current_target_name,\n    priors = current_priors,\n    seed = seed\n  )\n  save(xgb_resul, file = str_c(\"output/real-data/xgb_resul_\", name, \".rda\"))\n\n  ## GLM----\n  glm_resul &lt;- simul_glm(\n    data = current_data,\n    target_name = current_target_name,\n    priors = current_priors,\n    seed = seed\n  )\n  save(glm_resul, file = str_c(\"output/real-data/glm_resul_\", name, \".rda\"))\n  \n  ## GAM----\n  gam_resul &lt;- simul_gam(\n    data = current_data,\n    target_name = current_target_name,\n    spline_df = 6,\n    priors = current_priors,\n    seed = seed\n  )\n  save(gam_resul, file = str_c(\"output/real-data/gam_resul_\", name, \".rda\"))\n\n  ## GAMSEL----\n  gamsel_resul &lt;- simul_gamsel(\n    data = current_data,\n    target_name = current_target_name,\n    degrees = 6,\n    priors = current_priors,\n    seed = seed\n  )\n  save(gamsel_resul, file = str_c(\"output/real-data/gamsel_resul_\", name, \".rda\"))\n}",
    "crumbs": [
      "Real-world Data",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Estimations</span>"
    ]
  },
  {
    "objectID": "book_real_results.html",
    "href": "book_real_results.html",
    "title": "16  Results",
    "section": "",
    "text": "16.1 Estimated Metrics\nWe loop over the results obtained from Chapter 15 to extract the metrics.\ndatasets &lt;- tribble(\n  ~name, ~target_name,\n  \"abalone\", \"Sex\",\n  \"adult\", \"high_income\",\n  \"bank\", \"y\",\n  \"default\", \"default\",\n  \"drybean\", \"is_dermason\",\n  \"coupon\", \"y\",\n  \"mushroom\", \"edible\",\n  \"occupancy\", \"Occupancy\",\n  \"winequality\", \"high_quality\",\n  \"spambase\", \"is_spam\"\n)\nresult_table &lt;- vector(mode = \"list\", length = nrow(datasets))\npriors &lt;- vector(mode = \"list\", length = nrow(datasets))\nnames(priors) &lt;- datasets$name\nscores_hist &lt;- list()\nfor (i_model in 1:nrow(datasets)) {\n  name &lt;- datasets$name[i_model]\n  # Load priors\n  load(str_c(\"output/real-data/priors_\", name, \".rda\"))\n  priors[[i_model]] &lt;- get(str_c(\"priors_\", name))\n  # Load results\n  load(str_c(\"output/real-data/rf_resul_\", name, \".rda\"))\n  load(str_c(\"output/real-data/xgb_resul_\", name, \".rda\"))\n  model_interest &lt;-\n    get_model_interest(resul = rf_resul, model_type = \"rf\") |&gt;\n    bind_rows(get_model_interest(resul = xgb_resul, model_type = \"xgb\"))\n  result_table_ml &lt;- get_row_table(model_interest = model_interest, name = name)\n  \n  load(str_c(\"output/real-data/glm_resul_\", name, \".rda\"))\n  load(str_c(\"output/real-data/gam_resul_\", name, \".rda\"))\n  load(str_c(\"output/real-data/gamsel_resul_\", name, \".rda\"))\n  model_glms &lt;- get_model_interest(resul = glm_resul, model_type = \"glm\") |&gt; \n    bind_rows(get_model_interest(resul = gam_resul, model_type = \"gam\")) |&gt; \n    bind_rows(get_model_interest(resul = gamsel_resul, model_type = \"gamsel\"))\n  result_table_gl &lt;- get_row_table_glm(model_glms = model_glms, name = name)\n  \n  result_table[[i_model]] &lt;- result_table_ml |&gt; bind_rows(result_table_gl)\n\n  tb_ind_model_interest &lt;- \n    model_interest |&gt; filter(sample == \"test\") |&gt; \n    select(model_interest, model_type, ind)\n  \n  # Extract histograms for model of interest\n  scores_hist_current &lt;- list()\n  for (model in c(\"rf\", \"xgb\")) {\n    scores_hist_current_model &lt;- \n      tb_ind_model_interest |&gt; \n      filter(model_type == model) |&gt; \n      pull(\"ind\") |&gt; \n      map(~rf_resul$res[[.x]]$scores_hist)\n    for (j in 1:length(scores_hist_current_model)) {\n      scores_hist_current_model[[j]]$model_interest &lt;- \n        tb_ind_model_interest |&gt; \n        filter(model_type == !!model) |&gt; \n        pull(model_interest) |&gt; pluck(j)\n      scores_hist_current_model[[j]]$model_type &lt;- model\n      scores_hist_current_model[[j]]$name &lt;- name\n    }\n    scores_hist_current &lt;- c(scores_hist_current, scores_hist_current_model)\n  }\n  scores_hist &lt;- c(scores_hist, scores_hist_current)\n}\nCodes to create result tables\nred_colours &lt;- c(\n  \"#FFD6D6\", \"#FFCCCC\", \"#FFC2C2\", \"#FFB8B8\", \"#FFADAD\", \n  \"#FFA3A3\", \"#FF9999\", \"#FF8F8F\", \"#FF8585\", \"#FF7A7A\"\n)\nred_colours_txt &lt;- c(\n  \"#333333\", \"#333333\", \"#2B2B2B\", \"#2B2B2B\", \"#232323\", \n  \"#1F1F1F\", \"#1A1A1A\", \"#141414\", \"#101010\", \"#0A0A0A\"\n)\ngreen_colours &lt;- c(\n  \"#E9F6E9\", \"#D4F2D4\", \"#BFEFBF\", \"#AADCA9\", \"#96C996\",\n  \"#81B781\", \"#6CA56C\", \"#578252\", \"#426F42\", \"#2F5D2F\"\n)\ngreen_colours_txt &lt;- c(\n  \"#1A1A1A\", \"#1A1A1A\", \"#1A1A1A\", \"#1A1A1A\", \"#1A1A1A\",\n  \"#E6E6E6\", \"#E6E6E6\", \"#E6E6E6\", \"#E6E6E6\", \"#E6E6E6\"\n)\n\naccuracy_digits &lt;- 0.01\n\nget_range_for_colours &lt;- function(variable_name, table_kb) {\n  value &lt;- table_kb |&gt; \n    # filter(value_type == \"mean\") |&gt; \n    pull(!!variable_name) |&gt; \n    range(na.rm = TRUE) |&gt; abs() |&gt; max()\n  value * c(-1, 1)\n}\n\nget_colour &lt;- function(variable, value_type, min_or_max, colour_type, table_kb) {\n  variable_string &lt;- deparse(substitute(variable))\n  if (colour_type == \"bg\") {\n    # background colour\n    if (min_or_max == \"min\") {\n      colours &lt;- rev(c(rev(red_colours), green_colours))\n    } else {\n      colours &lt;- c(rev(red_colours), rev(green_colours))\n    }\n  } else {\n    # text colour\n    if (min_or_max == \"min\") {\n      colours &lt;- rev(c(rev(red_colours_txt), green_colours_txt))\n    } else {\n      colours &lt;- c(rev(red_colours_txt), rev(green_colours_txt))\n    }\n  }\n  kableExtra::spec_color(\n    variable,\n    palette = colours,\n    scale_from = get_range_for_colours(variable_string, table_kb = table_kb),\n    na_color = \"white\"\n  )\n}\n\nprint_table &lt;- function(format, \n                        table_kb, \n                        prior_model = c(\"glm\", \"gam\", \"gamsel\")) {\n  tb_with_colours &lt;- table_kb |&gt; \n    rowwise() |&gt; \n    mutate(\n      # When min KL\n      ## Delta AUC\n      diff_auc_bgcol = get_colour(\n        !!sym(str_c(\"diff_auc_\", prior_model)), value_type, \"max\", \"bg\", table_kb\n      ),\n      diff_auc_txtcol = get_colour(\n        !!sym(str_c(\"diff_auc_\", prior_model)), value_type, \"max\", \"txt\", table_kb\n      ),\n      ## Delta Brier\n      diff_brier_bgcol = get_colour(\n        !!sym(str_c(\"diff_brier_\", prior_model)), value_type, \"min\", \"bg\", table_kb\n      ),\n      diff_brier_txtcol = get_colour(\n        !!sym(str_c(\"diff_brier_\", prior_model)), value_type, \"min\", \"txt\", table_kb\n      ),\n      ## Delta ICI\n      diff_ici_bgcol = get_colour(\n        !!sym(str_c(\"diff_ici_\", prior_model)), value_type, \"min\", \"bg\", table_kb\n      ),\n      diff_ici_txtcol = get_colour(\n        !!sym(str_c(\"diff_ici_\", prior_model)), value_type, \"min\", \"txt\", table_kb\n      ),\n      ## Delta KL\n      diff_kl_bgcol = get_colour(\n        !!sym(str_c(\"diff_kl_\", prior_model)), value_type, \"min\", \"bg\", table_kb\n      ),\n      diff_kl_txtcol = get_colour(\n        !!sym(str_c(\"diff_kl_\", prior_model)), value_type, \"min\", \"txt\", table_kb\n      ),\n    )\n  \n  table_kb |&gt; \n     mutate(\n      across(\n        where(is.numeric), \n        ~scales::number(.x, accuracy = accuracy_digits)\n      )\n    ) |&gt; \n    knitr::kable(\n      col.names = c(\n        \"Dataset\", \"Model\",\n        \"AUC\", \"brier\", \"ICI\", \"KL\", \"Quant. Ratio\", # model with max AUC\n        \"AUC\", \"brier\", \"ICI\", \"KL\", \"Quant. Ratio\", # model with min Brier\n        \"AUC\", \"brier\", \"ICI\", \"KL\", \"Quant. Ratio\", # model with min ICI\n        \"AUC\", \"brier\", \"ICI\", \"KL\", \"Quant. Ratio\", \"ΔAUC\", \"ΔBrier\", \"ΔICI\", \"ΔKL\"\n      ),\n      escape = FALSE, booktabs = T, digits = 3, format = format) |&gt;\n    ## Delta AUC\n    kableExtra::column_spec(\n      which(colnames(table_kb) == str_c(\"diff_auc_\", prior_model)),\n      background = tb_with_colours$diff_auc_bgcol,\n      color = tb_with_colours$diff_auc_txtcol\n    ) |&gt;\n    ## Delta Brier\n    kableExtra::column_spec(\n      which(colnames(table_kb) == str_c(\"diff_brier_\", prior_model)),\n      background = tb_with_colours$diff_brier_bgcol,\n      color = tb_with_colours$diff_brier_txtcol\n    ) |&gt;\n    ## Delta ICI\n    kableExtra::column_spec(\n      which(colnames(table_kb) == str_c(\"diff_ici_\", prior_model)),\n      background = tb_with_colours$diff_ici_bgcol,\n      color = tb_with_colours$diff_ici_txtcol\n    ) |&gt;\n    ## Delta KL\n    kableExtra::column_spec(\n      which(colnames(table_kb) == str_c(\"diff_kl_\", prior_model)),\n      background = tb_with_colours$diff_kl_bgcol,\n      color = tb_with_colours$diff_kl_txtcol\n    ) |&gt;\n    kableExtra::collapse_rows(columns = 1:2, valign = \"top\") |&gt;\n    kableExtra::add_header_above(\n      c(\" \" = 2,\n        \"AUC*\" = 5,\n        \"Brier*\" = 5,\n        \"ICI*\" = 5,\n        \"KL*\" = 9\n      )\n    )\n}\n\nopts &lt;- options(knitr.kable.NA = \"\")\nThe estimated metrics for the GLM, GAM, and GAMSEL:\nCodes to create the table\ntbl_kb &lt;- result_table |&gt;\n  list_rbind() |&gt;\n  filter(model_type %in% c(\"glm\", \"gam\", \"gamsel\")) |&gt; \n  select(\n    dataset, model_type,\n    AUC_glm, brier_glm, ici_glm,\n    quant_ratio_glm_glm, quant_ratio_gam_gam, quant_ratio_gamsel_gamsel,\n    KL_glm_glm, KL_gam_gam, KL_gamsel_gamsel\n  ) |&gt;\n  mutate(\n    model_type = factor(\n      model_type, \n      levels = c(\"glm\", \"gam\", \"gamsel\"), \n      labels = c(\"GLM\", \"GAM\", \"GAMSEL\")\n    )\n  ) |&gt; \n  mutate(\n    across(\n      where(is.numeric), \n      ~scales::number(.x, accuracy = accuracy_digits)\n    )\n  )\n\ncbind(\n  tbl_kb |&gt; filter(dataset %in% datasets$name[1:5]),\n  tbl_kb |&gt; filter(dataset %in% datasets$name[6:10])\n) |&gt; \n  knitr::kable(\n    col.names = c(\n      rep(c(\"Dataset\",\"Model\",\n      \"AUC\", \"brier\", \"ICI\",\n      \"QR-GLM\", \"QR-GAM\", \"QR-GAMSEL\",\n      \"KL-GLM\", \"KL-GAM\", \"KL-GAMSEL\"\n      ), 2)\n    ),\n    escape = FALSE, booktabs = T, digits = 3, format = \"markdown\") |&gt;\n  kableExtra::collapse_rows(columns = c(1, 12), valign = \"top\")\n\n\n\n\n\n\nDataset\nModel\nAUC\nbrier\nICI\nQR-GLM\nQR-GAM\nQR-GAMSEL\nKL-GLM\nKL-GAM\nKL-GAMSEL\nDataset\nModel\nAUC\nbrier\nICI\nQR-GLM\nQR-GAM\nQR-GAMSEL\nKL-GLM\nKL-GAM\nKL-GAMSEL\n\n\n\n\nabalone\nGLM\n0.70\n0.20\n0.06\n1.06\n0.81\n1.06\n0.05\n0.11\n0.05\ncoupon\nGLM\n0.75\n0.20\n0.01\n1.05\n1.05\n1.17\n0.02\n0.02\n0.06\n\n\nGAM\n0.71\n0.20\n0.02\n1.27\n0.96\n1.26\n0.32\n0.13\n0.30\nGAM\n0.75\n0.20\n0.01\n1.05\n1.05\n1.17\n0.02\n0.02\n0.06\n\n\nGAMSEL\n0.71\n0.20\n0.04\n1.09\n0.83\n1.09\n0.11\n0.17\n0.10\nGAMSEL\n0.75\n0.20\n0.02\n0.94\n0.94\n1.05\n0.05\n0.05\n0.03\n\n\nadult\nGLM\n0.90\n0.10\n0.00\n0.92\n0.86\n1.13\n0.02\n0.02\n0.16\nmushroom\nGLM\n1.00\n0.00\n0.00\n1.00\n1.00\n1.02\n0.29\n0.29\n1.40\n\n\nGAM\n0.91\n0.10\n0.01\n0.96\n0.90\n1.18\n0.05\n0.03\n0.23\nGAM\n1.00\n0.00\n0.00\n1.00\n1.00\n1.02\n0.29\n0.29\n1.40\n\n\nGAMSEL\n0.90\n0.11\n0.03\n0.81\n0.76\n1.00\n0.06\n0.10\n0.05\nGAMSEL\n1.00\n0.01\n0.04\n0.94\n0.94\n0.96\n0.61\n0.61\n0.90\n\n\nbank\nGLM\n0.91\n0.07\n0.03\n0.83\n0.84\n1.00\n0.20\n0.15\n0.32\noccupancy\nGLM\n1.00\n0.01\n0.01\n1.02\n1.01\n1.13\n0.46\n0.35\n0.85\n\n\nGAM\n0.92\n0.07\n0.02\n0.92\n0.93\n1.10\n0.20\n0.14\n0.34\nGAM\n1.00\n0.01\n0.01\n1.02\n1.02\n1.14\n0.50\n0.38\n0.92\n\n\nGAMSEL\n0.91\n0.07\n0.03\n0.78\n0.79\n0.94\n0.13\n0.10\n0.20\nGAMSEL\n0.99\n0.02\n0.04\n0.93\n0.93\n1.04\n0.41\n0.34\n0.63\n\n\ndefault\nGLM\n0.77\n0.14\n0.02\n1.09\n1.06\n1.23\n0.45\n0.45\n0.49\nwinequality\nGLM\n0.74\n0.20\n0.04\n0.97\n0.75\n1.03\n0.06\n0.22\n0.06\n\n\nGAM\n0.78\n0.13\n0.01\n1.13\n1.10\n1.27\n0.29\n0.28\n0.35\nGAM\n0.79\n0.18\n0.04\n1.22\n0.95\n1.30\n0.14\n0.02\n0.21\n\n\nGAMSEL\n0.76\n0.14\n0.03\n0.94\n0.92\n1.07\n0.73\n0.75\n0.70\nGAMSEL\n0.75\n0.20\n0.04\n0.92\n0.72\n0.99\n0.04\n0.24\n0.03\n\n\ndrybean\nGLM\n0.99\n0.03\n0.01\n1.00\n1.00\n1.16\n0.06\n0.05\n0.57\nspambase\nGLM\n0.97\n0.06\n0.02\n1.00\n1.00\n1.05\n0.02\n0.15\n0.37\n\n\nGAM\n0.99\n0.03\n0.01\n1.00\n1.00\n1.17\n0.08\n0.07\n0.64\nGAM\n0.91\n0.08\n0.07\n1.00\n1.00\n1.05\n0.67\n0.29\n1.69\n\n\nGAMSEL\n0.99\n0.04\n0.06\n0.91\n0.91\n1.06\n0.19\n0.21\n0.06\nGAMSEL\n0.96\n0.07\n0.06\n0.96\n0.96\n1.01\n0.44\n0.98\n0.18",
    "crumbs": [
      "Real-world Data",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Results</span>"
    ]
  },
  {
    "objectID": "book_real_results.html#estimated-metrics",
    "href": "book_real_results.html#estimated-metrics",
    "title": "16  Results",
    "section": "",
    "text": "Priors from GLMPriors from GAMPriors from GAMSEL\n\n\n\n\nCodes to create the table\nresult_table_glm &lt;- \n  result_table |&gt;\n  list_rbind() |&gt;\n  filter(model_type %in% c(\"rf\", \"xgb\")) |&gt; \n  select(\n    dataset, model_type,\n    # model with max AUC\n    AUC_max_auc, brier_max_auc, ici_max_auc, KL_glm_max_auc, quant_ratio_glm_max_auc,\n    # model with min Brier\n    AUC_min_brier, brier_min_brier, ici_min_brier, KL_glm_min_brier, quant_ratio_glm_min_brier,\n    # model with min ICI\n    AUC_min_ici, brier_min_ici, ici_min_ici, KL_glm_min_ici, quant_ratio_glm_min_ici,\n    # model with min KL distance with prior from GLM\n    AUC_glm, brier_glm, ici_glm, KL_glm_glm, quant_ratio_glm_glm,\n    diff_auc_glm, diff_brier_glm, diff_ici_glm, diff_kl_glm \n  ) |&gt;\n  mutate(\n    model_type = factor(\n      model_type, \n      levels = c(\"rf\", \"xgb\", \"glm\", \"gam\", \"gamsel\"), \n      labels = c(\"RF\", \"XGB\", \"GLM\", \"GAM\", \"GAMSEL\")\n    )\n  )\n\nprint_table(\n  format = \"markdown\", table_kb = result_table_glm, prior_model = \"glm\"\n)\n\n\n\n\nTable 16.1: Comparison of metrics for models chosen based on AUC, on AIC, or on KL divergence with a prior on the distribution of the probabilities estimated with a GLM.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAUC*\n\n\nBrier*\n\n\nICI*\n\n\nKL*\n\n\n\nDataset\nModel\nAUC\nbrier\nICI\nKL\nQuant. Ratio\nAUC\nbrier\nICI\nKL\nQuant. Ratio\nAUC\nbrier\nICI\nKL\nQuant. Ratio\nAUC\nbrier\nICI\nKL\nQuant. Ratio\nΔAUC\nΔBrier\nΔICI\nΔKL\n\n\n\n\nabalone\nRF\n0.71\n0.20\n0.03\n0.34\n1.21\n0.71\n0.20\n0.03\n0.34\n1.24\n0.51\n0.23\n0.02\n2.73\n0.00\n0.71\n0.20\n0.03\n0.33\n1.28\n0.00\n0.00\n0.00\n-0.01\n\n\nXGB\n0.69\n0.20\n0.03\n0.42\n1.45\n0.69\n0.20\n0.04\n0.56\n1.06\n0.70\n0.20\n0.04\n0.80\n1.03\n0.69\n0.21\n0.05\n0.24\n1.23\n0.00\n0.00\n0.02\n-0.18\n\n\nadult\nRF\n0.92\n0.10\n0.03\n0.03\n0.88\n0.92\n0.10\n0.03\n0.02\n0.89\n0.51\n0.18\n0.00\n4.46\n0.00\n0.92\n0.10\n0.03\n0.02\n0.89\n0.00\n0.00\n0.00\n-0.01\n\n\nXGB\n0.93\n0.09\n0.01\n0.09\n1.00\n0.93\n0.09\n0.01\n0.09\n1.00\n0.93\n0.09\n0.01\n0.09\n0.97\n0.91\n0.10\n0.02\n0.04\n0.90\n-0.01\n0.01\n0.01\n-0.05\n\n\nbank\nRF\n0.94\n0.06\n0.02\n0.19\n1.08\n0.94\n0.06\n0.02\n0.21\n1.10\n0.94\n0.06\n0.02\n0.21\n1.12\n0.92\n0.07\n0.04\n0.07\n0.82\n-0.02\n0.01\n0.02\n-0.12\n\n\nXGB\n0.93\n0.06\n0.02\n0.36\n1.17\n0.93\n0.06\n0.02\n0.28\n1.12\n0.93\n0.06\n0.02\n0.34\n1.15\n0.91\n0.07\n0.03\n0.07\n0.93\n-0.02\n0.00\n0.01\n-0.29\n\n\ndefault\nRF\n0.78\n0.13\n0.02\n0.20\n1.10\n0.78\n0.13\n0.01\n0.18\n1.12\n0.78\n0.13\n0.01\n0.16\n1.15\n0.77\n0.14\n0.02\n0.13\n1.17\n-0.02\n0.00\n0.00\n-0.07\n\n\nXGB\n0.78\n0.13\n0.01\n0.23\n1.17\n0.78\n0.13\n0.01\n0.29\n1.15\n0.78\n0.13\n0.01\n0.22\n1.19\n0.77\n0.13\n0.01\n0.19\n1.17\n-0.01\n0.00\n0.00\n-0.04\n\n\ndrybean\nRF\n0.99\n0.03\n0.01\n0.06\n1.00\n0.99\n0.03\n0.01\n0.07\n1.00\n0.99\n0.03\n0.01\n0.06\n1.00\n0.99\n0.03\n0.02\n0.02\n0.98\n0.00\n0.00\n0.01\n-0.04\n\n\nXGB\n0.99\n0.03\n0.01\n0.08\n1.00\n0.99\n0.03\n0.01\n0.09\n1.00\n0.99\n0.03\n0.01\n0.09\n1.00\n0.99\n0.03\n0.04\n0.07\n0.92\n0.00\n0.00\n0.03\n-0.02\n\n\ncoupon\nRF\n0.83\n0.17\n0.07\n0.04\n0.98\n0.83\n0.17\n0.07\n0.04\n0.98\n0.51\n0.24\n0.00\n3.60\n0.00\n0.83\n0.17\n0.07\n0.04\n0.98\n0.00\n0.00\n0.00\n0.00\n\n\nXGB\n0.84\n0.17\n0.10\n2.27\n1.74\n0.84\n0.16\n0.03\n0.81\n1.53\n0.83\n0.16\n0.02\n0.37\n1.39\n0.78\n0.19\n0.03\n0.04\n1.03\n-0.06\n0.02\n-0.07\n-2.23\n\n\nmushroom\nRF\n1.00\n0.01\n0.05\n0.23\n0.96\n1.00\n0.00\n0.01\n0.22\n1.00\n1.00\n0.00\n0.01\n0.22\n1.00\n1.00\n0.01\n0.04\n0.11\n0.99\n0.00\n0.00\n-0.02\n-0.12\n\n\nXGB\n1.00\n0.00\n0.00\n0.28\n1.00\n1.00\n0.00\n0.00\n0.29\n1.00\n1.00\n0.00\n0.00\n0.28\n1.00\n1.00\n0.01\n0.04\n0.13\n0.97\n0.00\n0.01\n0.03\n-0.15\n\n\noccupancy\nRF\n1.00\n0.01\n0.00\n0.56\n1.04\n1.00\n0.01\n0.00\n0.57\n1.04\n1.00\n0.01\n0.00\n0.57\n1.04\n1.00\n0.01\n0.04\n0.31\n0.97\n0.00\n0.01\n0.03\n-0.25\n\n\nXGB\n1.00\n0.01\n0.01\n0.60\n1.04\n1.00\n0.01\n0.01\n0.66\n1.04\n1.00\n0.01\n0.00\n0.54\n1.03\n1.00\n0.01\n0.04\n0.47\n0.95\n0.00\n0.00\n0.04\n-0.13\n\n\nwinequality\nRF\n0.89\n0.14\n0.07\n0.32\n1.42\n0.89\n0.13\n0.03\n0.69\n1.58\n0.51\n0.24\n0.03\n3.43\n0.00\n0.84\n0.17\n0.08\n0.05\n1.07\n-0.05\n0.03\n0.01\n-0.27\n\n\nXGB\n0.87\n0.15\n0.12\n4.06\n1.97\n0.86\n0.14\n0.04\n1.63\n1.75\n0.83\n0.17\n0.03\n0.35\n1.39\n0.80\n0.18\n0.04\n0.11\n1.12\n-0.07\n0.03\n-0.08\n-3.96\n\n\nspambase\nRF\n0.99\n0.05\n0.06\n0.21\n0.96\n0.99\n0.04\n0.04\n0.10\n0.98\n0.51\n0.24\n0.01\n6.08\n0.00\n0.99\n0.04\n0.04\n0.10\n0.98\n0.00\n0.00\n-0.02\n-0.11\n\n\nXGB\n0.98\n0.04\n0.01\n0.23\n1.00\n0.99\n0.04\n0.01\n0.17\n1.00\n0.99\n0.04\n0.01\n0.17\n1.00\n0.98\n0.04\n0.01\n0.10\n0.99\n0.00\n0.01\n0.00\n-0.13\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodes to create the table\nresult_table_gam &lt;- \n  result_table |&gt;\n  list_rbind() |&gt;\n  filter(model_type %in% c(\"rf\", \"xgb\")) |&gt; \n  select(\n    dataset, model_type,\n    # model with max AUC\n    AUC_max_auc, brier_max_auc, ici_max_auc, KL_gam_max_auc, quant_ratio_gam_max_auc,\n    # model with min Brier\n    AUC_min_brier, brier_min_brier, ici_min_brier, quant_ratio_gam_min_brier, KL_gam_min_brier,\n    # model with min ICI\n    AUC_min_ici, brier_min_ici, ici_min_ici, KL_gam_min_ici, quant_ratio_gam_min_ici,\n    # model with min KL distance with prior from GAM\n    AUC_gam, brier_gam, ici_gam, KL_gam_gam, quant_ratio_gam_gam,\n    diff_auc_gam, diff_brier_gam, diff_ici_gam, diff_kl_gam \n  ) |&gt;\n  mutate(\n    model_type = factor(\n      model_type, \n      levels = c(\"rf\", \"xgb\", \"glm\", \"gam\", \"gamsel\"), \n      labels = c(\"RF\", \"XGB\", \"GLM\", \"GAM\", \"GAMSEL\")\n    )\n  )\n\n\nprint_table(\n  format = \"markdown\", table_kb = result_table_gam, prior_model = \"gam\"\n)\n\n\n\n\nTable 16.2: Comparison of metrics for models chosen based on AUC, on AIC, or on KL divergence with a prior on the distribution of the probabilities estimated with a GAM.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAUC*\n\n\nBrier*\n\n\nICI*\n\n\nKL*\n\n\n\nDataset\nModel\nAUC\nbrier\nICI\nKL\nQuant. Ratio\nAUC\nbrier\nICI\nKL\nQuant. Ratio\nAUC\nbrier\nICI\nKL\nQuant. Ratio\nAUC\nbrier\nICI\nKL\nQuant. Ratio\nΔAUC\nΔBrier\nΔICI\nΔKL\n\n\n\n\nabalone\nRF\n0.71\n0.20\n0.03\n0.24\n0.92\n0.71\n0.20\n0.03\n0.94\n0.21\n0.51\n0.23\n0.02\n3.18\n0.00\n0.70\n0.20\n0.04\n0.13\n1.07\n-0.01\n0.00\n0.01\n-0.11\n\n\nXGB\n0.69\n0.20\n0.03\n0.12\n1.11\n0.69\n0.20\n0.04\n0.80\n0.58\n0.70\n0.20\n0.04\n0.92\n0.78\n0.69\n0.21\n0.04\n0.14\n1.26\n0.00\n0.00\n0.01\n0.02\n\n\nadult\nRF\n0.92\n0.10\n0.03\n0.06\n0.83\n0.92\n0.10\n0.03\n0.83\n0.04\n0.51\n0.18\n0.00\n4.63\n0.00\n0.92\n0.10\n0.03\n0.04\n0.83\n0.00\n0.00\n0.00\n-0.01\n\n\nXGB\n0.93\n0.09\n0.01\n0.06\n0.93\n0.93\n0.09\n0.01\n0.93\n0.06\n0.93\n0.09\n0.01\n0.06\n0.91\n0.92\n0.09\n0.01\n0.05\n0.88\n-0.01\n0.00\n0.01\n-0.02\n\n\nbank\nRF\n0.94\n0.06\n0.02\n0.14\n1.09\n0.94\n0.06\n0.02\n1.11\n0.15\n0.94\n0.06\n0.02\n0.15\n1.13\n0.92\n0.07\n0.04\n0.05\n0.87\n-0.01\n0.01\n0.02\n-0.08\n\n\nXGB\n0.93\n0.06\n0.02\n0.28\n1.18\n0.93\n0.06\n0.02\n1.13\n0.21\n0.93\n0.06\n0.02\n0.26\n1.16\n0.91\n0.07\n0.03\n0.05\n0.96\n-0.02\n0.00\n0.01\n-0.23\n\n\ndefault\nRF\n0.78\n0.13\n0.02\n0.20\n1.07\n0.78\n0.13\n0.01\n1.09\n0.17\n0.78\n0.13\n0.01\n0.15\n1.12\n0.77\n0.14\n0.02\n0.12\n1.14\n-0.02\n0.00\n0.00\n-0.08\n\n\nXGB\n0.78\n0.13\n0.01\n0.22\n1.14\n0.78\n0.13\n0.01\n1.12\n0.29\n0.78\n0.13\n0.01\n0.20\n1.16\n0.77\n0.13\n0.01\n0.17\n1.14\n-0.01\n0.00\n0.00\n-0.05\n\n\ndrybean\nRF\n0.99\n0.03\n0.01\n0.05\n1.00\n0.99\n0.03\n0.01\n1.00\n0.06\n0.99\n0.03\n0.01\n0.05\n1.00\n0.99\n0.03\n0.02\n0.02\n0.98\n0.00\n0.00\n0.01\n-0.03\n\n\nXGB\n0.99\n0.03\n0.01\n0.07\n1.00\n0.99\n0.03\n0.01\n1.00\n0.08\n0.99\n0.03\n0.01\n0.08\n1.00\n0.99\n0.03\n0.02\n0.05\n0.97\n0.00\n0.00\n0.01\n-0.02\n\n\ncoupon\nRF\n0.83\n0.17\n0.07\n0.04\n0.98\n0.83\n0.17\n0.07\n0.98\n0.04\n0.51\n0.24\n0.00\n3.60\n0.00\n0.83\n0.17\n0.07\n0.04\n0.98\n0.00\n0.00\n0.00\n0.00\n\n\nXGB\n0.84\n0.17\n0.10\n2.27\n1.74\n0.84\n0.16\n0.03\n1.53\n0.81\n0.83\n0.16\n0.02\n0.37\n1.39\n0.78\n0.19\n0.03\n0.04\n1.03\n-0.06\n0.02\n-0.07\n-2.23\n\n\nmushroom\nRF\n1.00\n0.01\n0.05\n0.23\n0.96\n1.00\n0.00\n0.01\n1.00\n0.22\n1.00\n0.00\n0.01\n0.22\n1.00\n1.00\n0.01\n0.04\n0.11\n0.99\n0.00\n0.00\n-0.02\n-0.12\n\n\nXGB\n1.00\n0.00\n0.00\n0.28\n1.00\n1.00\n0.00\n0.00\n1.00\n0.29\n1.00\n0.00\n0.00\n0.28\n1.00\n1.00\n0.01\n0.04\n0.13\n0.97\n0.00\n0.01\n0.03\n-0.15\n\n\noccupancy\nRF\n1.00\n0.01\n0.00\n0.43\n1.03\n1.00\n0.01\n0.00\n1.03\n0.44\n1.00\n0.01\n0.00\n0.44\n1.03\n1.00\n0.01\n0.02\n0.26\n0.99\n0.00\n0.00\n0.02\n-0.17\n\n\nXGB\n1.00\n0.01\n0.01\n0.47\n1.03\n1.00\n0.01\n0.01\n1.03\n0.52\n1.00\n0.01\n0.00\n0.41\n1.02\n1.00\n0.01\n0.04\n0.37\n0.94\n0.00\n0.00\n0.04\n-0.10\n\n\nwinequality\nRF\n0.89\n0.14\n0.07\n0.04\n1.10\n0.89\n0.13\n0.03\n1.23\n0.12\n0.51\n0.24\n0.03\n3.95\n0.00\n0.86\n0.15\n0.06\n0.03\n1.02\n-0.03\n0.02\n-0.01\n-0.01\n\n\nXGB\n0.87\n0.15\n0.12\n1.91\n1.53\n0.86\n0.14\n0.04\n1.36\n0.53\n0.83\n0.17\n0.03\n0.04\n1.08\n0.82\n0.17\n0.03\n0.04\n1.00\n-0.06\n0.02\n-0.08\n-1.87\n\n\nspambase\nRF\n0.99\n0.05\n0.06\n0.63\n0.96\n0.99\n0.04\n0.04\n0.98\n0.37\n0.51\n0.24\n0.01\n7.17\n0.00\n0.99\n0.04\n0.04\n0.37\n0.98\n0.00\n0.00\n-0.02\n-0.26\n\n\nXGB\n0.98\n0.04\n0.01\n0.03\n1.00\n0.99\n0.04\n0.01\n1.00\n0.03\n0.99\n0.04\n0.01\n0.03\n1.00\n0.98\n0.04\n0.02\n0.04\n1.00\n0.00\n0.00\n0.00\n0.00\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCodes to create the table\nresult_table_gamsel &lt;- \n  result_table |&gt;\n  list_rbind() |&gt;\n  filter(model_type %in% c(\"rf\", \"xgb\")) |&gt; \n  select(\n    dataset, model_type,\n    # model with max AUC\n    AUC_max_auc, brier_max_auc, ici_max_auc, KL_gamsel_max_auc, quant_ratio_gamsel_max_auc,\n    # model with min Brier\n    AUC_min_brier, brier_min_brier, ici_min_brier, KL_gamsel_min_brier, quant_ratio_gamsel_min_brier,\n    # model with min ICI\n    AUC_min_ici, brier_min_ici, ici_min_ici, KL_gamsel_min_ici, quant_ratio_gamsel_min_ici,\n    # model with min KL distance with prior from GAMSEL\n    AUC_gamsel, brier_gamsel, ici_gamsel, KL_gamsel_gamsel, quant_ratio_gamsel_gamsel,\n    diff_auc_gamsel, diff_brier_gamsel, diff_ici_gamsel, diff_kl_gamsel \n  ) |&gt;\n  mutate(\n    model_type = factor(\n      model_type, \n      levels = c(\"rf\", \"xgb\", \"glm\", \"gam\", \"gamsel\"), \n      labels = c(\"RF\", \"XGB\", \"GLM\", \"GAM\", \"GAMSEL\")\n    )\n  )\n\n\nprint_table(\n  format = \"markdown\", table_kb = result_table_gamsel, prior_model = \"gamsel\"\n)\n\n\n\n\nTable 16.3: Comparison of metrics for models chosen based on AUC, on AIC, or on KL divergence with a prior on the distribution of the probabilities estimated with a GAMSEL.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAUC*\n\n\nBrier*\n\n\nICI*\n\n\nKL*\n\n\n\nDataset\nModel\nAUC\nbrier\nICI\nKL\nQuant. Ratio\nAUC\nbrier\nICI\nKL\nQuant. Ratio\nAUC\nbrier\nICI\nKL\nQuant. Ratio\nAUC\nbrier\nICI\nKL\nQuant. Ratio\nΔAUC\nΔBrier\nΔICI\nΔKL\n\n\n\n\nabalone\nRF\n0.71\n0.20\n0.03\n0.33\n1.21\n0.71\n0.20\n0.03\n0.33\n1.23\n0.51\n0.23\n0.02\n2.74\n0.00\n0.71\n0.20\n0.03\n0.32\n1.28\n0.00\n0.00\n0.00\n-0.01\n\n\nXGB\n0.69\n0.20\n0.03\n0.40\n1.45\n0.69\n0.20\n0.04\n0.56\n1.06\n0.70\n0.20\n0.04\n0.80\n1.02\n0.69\n0.21\n0.05\n0.24\n1.23\n0.00\n0.00\n0.02\n-0.16\n\n\nadult\nRF\n0.92\n0.10\n0.03\n0.08\n1.09\n0.92\n0.10\n0.03\n0.09\n1.10\n0.51\n0.18\n0.00\n3.96\n0.00\n0.91\n0.10\n0.05\n0.04\n0.98\n-0.01\n0.01\n0.02\n-0.04\n\n\nXGB\n0.93\n0.09\n0.01\n0.35\n1.23\n0.93\n0.09\n0.01\n0.35\n1.23\n0.93\n0.09\n0.01\n0.34\n1.20\n0.91\n0.10\n0.03\n0.09\n1.04\n-0.02\n0.01\n0.02\n-0.26\n\n\nbank\nRF\n0.94\n0.06\n0.02\n0.31\n1.30\n0.94\n0.06\n0.02\n0.34\n1.32\n0.94\n0.06\n0.02\n0.35\n1.34\n0.91\n0.07\n0.05\n0.06\n0.87\n-0.03\n0.01\n0.03\n-0.25\n\n\nXGB\n0.93\n0.06\n0.02\n0.57\n1.40\n0.93\n0.06\n0.02\n0.46\n1.34\n0.93\n0.06\n0.02\n0.54\n1.38\n0.89\n0.07\n0.04\n0.07\n0.80\n-0.04\n0.01\n0.02\n-0.50\n\n\ndefault\nRF\n0.78\n0.13\n0.02\n0.22\n1.24\n0.78\n0.13\n0.01\n0.23\n1.27\n0.78\n0.13\n0.01\n0.24\n1.30\n0.78\n0.14\n0.03\n0.20\n1.07\n0.00\n0.00\n0.01\n-0.02\n\n\nXGB\n0.78\n0.13\n0.01\n0.34\n1.32\n0.78\n0.13\n0.01\n0.36\n1.30\n0.78\n0.13\n0.01\n0.35\n1.35\n0.79\n0.13\n0.01\n0.34\n1.28\n0.00\n0.00\n0.00\n0.00\n\n\ndrybean\nRF\n0.99\n0.03\n0.01\n0.56\n1.17\n0.99\n0.03\n0.01\n0.59\n1.17\n0.99\n0.03\n0.01\n0.57\n1.17\n0.99\n0.04\n0.05\n0.12\n1.08\n0.00\n0.01\n0.04\n-0.44\n\n\nXGB\n0.99\n0.03\n0.01\n0.62\n1.16\n0.99\n0.03\n0.01\n0.63\n1.17\n0.99\n0.03\n0.01\n0.65\n1.17\n0.99\n0.03\n0.03\n0.37\n1.09\n0.00\n0.00\n0.02\n-0.25\n\n\ncoupon\nRF\n0.83\n0.17\n0.07\n0.04\n1.10\n0.83\n0.17\n0.07\n0.04\n1.10\n0.51\n0.24\n0.00\n3.40\n0.00\n0.82\n0.18\n0.07\n0.03\n1.02\n-0.01\n0.01\n0.00\n-0.01\n\n\nXGB\n0.84\n0.17\n0.10\n3.15\n1.94\n0.84\n0.16\n0.03\n1.27\n1.72\n0.83\n0.16\n0.02\n0.66\n1.55\n0.77\n0.19\n0.03\n0.05\n1.07\n-0.07\n0.02\n-0.07\n-3.10\n\n\nmushroom\nRF\n1.00\n0.01\n0.05\n0.65\n0.98\n1.00\n0.00\n0.01\n1.28\n1.02\n1.00\n0.00\n0.01\n1.28\n1.02\n1.00\n0.03\n0.07\n0.41\n0.97\n0.00\n0.02\n0.02\n-0.24\n\n\nXGB\n1.00\n0.00\n0.00\n1.40\n1.02\n1.00\n0.00\n0.00\n1.40\n1.02\n1.00\n0.00\n0.00\n1.40\n1.02\n1.00\n0.02\n0.05\n0.57\n0.95\n0.00\n0.02\n0.05\n-0.83\n\n\noccupancy\nRF\n1.00\n0.01\n0.00\n1.02\n1.15\n1.00\n0.01\n0.00\n1.02\n1.15\n1.00\n0.01\n0.00\n1.02\n1.15\n1.00\n0.02\n0.07\n0.37\n1.02\n0.00\n0.02\n0.07\n-0.65\n\n\nXGB\n1.00\n0.01\n0.01\n1.07\n1.15\n1.00\n0.01\n0.01\n1.14\n1.15\n1.00\n0.01\n0.00\n0.97\n1.14\n1.00\n0.01\n0.04\n0.82\n1.05\n0.00\n0.00\n0.04\n-0.24\n\n\nwinequality\nRF\n0.89\n0.14\n0.07\n0.44\n1.51\n0.89\n0.13\n0.03\n0.88\n1.68\n0.51\n0.24\n0.03\n3.35\n0.00\n0.83\n0.17\n0.08\n0.05\n1.05\n-0.06\n0.04\n0.01\n-0.38\n\n\nXGB\n0.87\n0.15\n0.12\n4.66\n2.10\n0.86\n0.14\n0.04\n1.95\n1.86\n0.83\n0.17\n0.03\n0.47\n1.48\n0.80\n0.18\n0.04\n0.13\n1.12\n-0.08\n0.03\n-0.07\n-4.53\n\n\nspambase\nRF\n0.99\n0.05\n0.06\n0.17\n1.00\n0.99\n0.04\n0.04\n0.26\n1.03\n0.51\n0.24\n0.01\n4.96\n0.00\n0.97\n0.07\n0.08\n0.07\n0.95\n-0.01\n0.02\n0.02\n-0.10\n\n\nXGB\n0.98\n0.04\n0.01\n1.01\n1.05\n0.99\n0.04\n0.01\n0.88\n1.05\n0.99\n0.04\n0.01\n0.87\n1.05\n0.98\n0.05\n0.04\n0.27\n1.00\n-0.01\n0.02\n0.02\n-0.75",
    "crumbs": [
      "Real-world Data",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Results</span>"
    ]
  },
  {
    "objectID": "book_real_results.html#distribution-of-scores",
    "href": "book_real_results.html#distribution-of-scores",
    "title": "16  Results",
    "section": "16.2 Distribution of Scores",
    "text": "16.2 Distribution of Scores\nLet us construct a tibble with the information to extract the histograms for models of interest:\n\nscores_ref_tibble &lt;- \n  map(scores_hist, ~tibble(\n    ind = .x$ind,\n    model_interest = .x$model_interest,\n    model_type = .x$model_type,\n    name = .x$name\n  )) |&gt; \n  list_rbind() |&gt; \n  mutate(ind_list = row_number())\n\nSome colours for the priors:\n\nprior_model_names &lt;- tribble(\n  ~name, ~label, ~colour,\n  \"glm\", \"GLM\", \"#D55E00\",\n  \"gam\", \"GAM\", \"#0072B2\",\n  \"gamsel\", \"GAMSEL\", \"#E69F00\"\n)\n\nWe define a (too long) function to create the plots. The left panel displays the score distribution estimated using a Generalized Linear Model. The middle panel presents scores estimated by the Random Forest and XGB models when maximizing the AUC. The right panel illustrates scores estimated by the Random Forest and XGB models when minimizing the KL divergence relative to the assumed probability distribution (a Beta distribution with shape parameters estimated from the scores in the left panel, see Chapter 14).\n\n\nDisplay the too long plot function\nprint_plot &lt;- function(prior_model, names) {\n  prior_name &lt;- prior_model_names |&gt; filter(name == !!prior_model) |&gt; \n    pull(\"label\")\n  col_titles &lt;- c(prior_name, \"AUC*\", \"KL*\")\n  layout(\n    matrix(\n      data = c(\n        1:3,\n        4:(length(names)*3+3),\n        rep(length(names)*3+4, 3)\n      ),\n      ncol = 3, byrow = TRUE\n    ), \n    heights = c(.5, rep(3, length(names)), .75)\n  )\n  \n  # layout(matrix(c(1:6), ncol=3, byrow=T),heights = c(.5,3))\n  par(mar = c(0, 4.1, 0, 2.1))\n  for (i in 1:3) {\n    plot(c(0, 1), c(0, 1), ann = F, bty = 'n', type = 'n', xaxt = 'n', yaxt = 'n')\n    text(x = 0.5, y = 0.5, col_titles[i], cex = 1.6, col = \"black\")\n  }\n  \n  colour_rf &lt;- \"#009E73\"\n  colour_xgb &lt;- \"#CC79A7\"\n  \n  for (name in names) {\n    # Get the histogram of scores estimated with the generalized linear model\n    scores_prior &lt;- priors[[name]][[str_c(\"scores_\", prior_model)]]$scores_test\n    priors_shapes &lt;- priors[[name]][[str_c(\"mle_\", prior_model)]]\n    colour_prior &lt;- prior_model_names |&gt; filter(name == !!prior_model) |&gt; \n      pull(\"colour\")\n    \n    \n    par(mar = c(4.1, 4.1, 1.1, 1.1))\n    breaks &lt;- seq(0, 1, by = .05)\n    p_scores_prior &lt;- hist(\n      scores_prior, \n      breaks = breaks,\n      plot = FALSE\n    )\n    val_u &lt;- seq(0, 1, length = 651)\n    dens_prior &lt;- \n      dbeta(val_u, priors_shapes$estimate[1], priors_shapes$estimate[2])\n    # Scores estimates with RF and XBG, maximizing AUC\n    ind_score_hist_rf_auc &lt;- \n      scores_ref_tibble |&gt; \n      filter(model_interest == \"max_auc\", model_type == \"rf\", name == !!name) |&gt; \n      pull(\"ind_list\")\n    ind_score_hist_xgb_auc &lt;- \n      scores_ref_tibble |&gt; \n      filter(model_interest == \"max_auc\", model_type == \"xgb\", name == !!name) |&gt; \n      pull(\"ind_list\")\n    # Scores estimates with RF and XBG, minimizing KL\n    ind_score_hist_rf_kl &lt;- \n      scores_ref_tibble |&gt; \n      filter(model_interest == !!prior_model, model_type == \"rf\", name == !!name) |&gt; \n      pull(\"ind_list\")\n    ind_score_hist_xgb_kl &lt;- \n      scores_ref_tibble |&gt; \n      filter(model_interest == !!prior_model, model_type == \"xgb\", name == !!name) |&gt; \n      pull(\"ind_list\")\n    \n    p_max_auc_rf &lt;- scores_hist[[ind_score_hist_rf_auc]]$test\n    p_max_auc_xgb &lt;- scores_hist[[ind_score_hist_xgb_auc]]$test\n    p_min_kl_rf &lt;- scores_hist[[ind_score_hist_rf_kl]]$test\n    p_min_kl_xgb &lt;- scores_hist[[ind_score_hist_xgb_kl]]$test\n    \n    y_lim &lt;- c(\n      range(dens_prior[!is.infinite(dens_prior)]),\n      range(p_scores_prior$density),\n      range(p_max_auc_rf$density),\n      range(p_max_auc_xgb$density),\n      range(p_min_kl_rf$density),\n      range(p_min_kl_xgb$density)\n    ) |&gt; range()\n    \n    x_lab &lt;- latex2exp::TeX(\"$\\\\hat{s}(x)$\")\n    plot(\n      p_scores_prior,\n      main = \"\",\n      xlab = x_lab,\n      ylab = \"\",\n      freq = FALSE,\n      ylim = y_lim,\n      col = adjustcolor(colour_prior, alpha.f = .5)\n    )\n    lines(val_u, dens_prior, col = colour_prior, lwd = 1.5)\n    # mtext(text = substitute(paste(bold(name))), side = 2, \n    #       line = 3, cex = 1, las = 0)\n    mtext(text = name, side = 2, line = 3, cex = 1.1, las = 0)\n    \n    # Plot for max AUC\n    plot(\n      p_max_auc_rf,\n      # main = \"AUC*\",\n      main = \"\",\n      xlab = x_lab,\n      ylab = \"\",\n      freq = FALSE,\n      col = adjustcolor(colour_rf, alpha.f = .5),\n      ylim = y_lim\n    )\n    plot(\n      p_max_auc_xgb,\n      add = TRUE,\n      freq = FALSE,\n      col = adjustcolor(colour_xgb, alpha.f = .5),\n      y_lim = y_lim\n    )\n    lines(val_u, dens_prior, col = colour_prior, lwd = 1.5)\n    \n    # Plot for min KL\n    plot(\n      p_min_kl_rf,\n      # main = \"KL*\",\n      main = \"\",\n      xlab = x_lab,\n      ylab = \"\",\n      freq = FALSE,\n      col = adjustcolor(colour_rf, alpha.f = .5),\n      ylim = y_lim\n    )\n    plot(\n      p_min_kl_xgb,\n      add = TRUE,\n      freq = FALSE,\n      col = adjustcolor(colour_xgb, alpha.f = .5),\n      ylim = y_lim\n    )\n    lines(val_u, dens_prior, col = colour_prior, lwd = 1.5)\n  }\n  \n  \n  par(mar = c(0, 4.1, 0, 1.1))\n  plot.new()\n  legend(\n    xpd = TRUE, ncol = 4,\n    \"center\",\n    lwd = c(1.5, rep(NA, 3)),\n    col = c(colour_prior, rep(NA, 3)),\n    fill = c(0, colour_prior, colour_rf, colour_xgb),\n    legend = c(str_c(\"Prior distribution (\", prior_name,\")\"), prior_name, \"Random forest\", \"Extreme Gradient Boosting\"),\n    border=c(NA, \"black\",\"black\",\"black\")\n  )\n}\n\n\n\nPrior from GLMPrior from GAMPrior from GAMSEL\n\n\n\nDatasets 1–5Datasets 6–10\n\n\n\nprint_plot(prior_model = \"glm\", names = datasets$name[1:5])\n\n\n\nEstimated scores on the first five datasets: GLM (left), models selected by AUC maximization (middle), and KL divergence minimization relative to prior assumptions (right).\n\n\n\n\n\n\n\n\nprint_plot(prior_model = \"glm\", names = datasets$name[6:10])\n\n\n\nEstimated scores on the first last datasets: GLM (left), models selected by AUC maximization (middle), and KL divergence minimization relative to prior assumptions (right).\n\n\n\n\n\n\n\n\n\n\n\nDatasets 1–5Datasets 6–10\n\n\n\nprint_plot(prior_model = \"gam\", names = datasets$name[1:5])\n\n\n\nEstimated scores on the first five datasets: GAM (left), models selected by AUC maximization (middle), and KL divergence minimization relative to prior assumptions (right).\n\n\n\n\n\n\n\n\nprint_plot(prior_model = \"gam\", names = datasets$name[6:10])\n\n\n\nEstimated scores on the first five datasets: GAM (left), models selected by AUC maximization (middle), and KL divergence minimization relative to prior assumptions (right).\n\n\n\n\n\n\n\n\n\n\n\nDatasets 1–5Datasets 6–10\n\n\n\nprint_plot(prior_model = \"gamsel\", names = datasets$name[1:5])\n\n\n\nEstimated scores on the first five datasets: GAMSEL (left), models selected by AUC maximization (middle), and KL divergence minimization relative to prior assumptions (right).\n\n\n\n\n\n\n\n\nprint_plot(prior_model = \"gamsel\", names = datasets$name[6:10])\n\n\n\nEstimated scores on the first five datasets: GAMSEL (left), models selected by AUC maximization (middle), and KL divergence minimization relative to prior assumptions (right).",
    "crumbs": [
      "Real-world Data",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Results</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Austin, Peter C., and Ewout W. Steyerberg. 2019. “The Integrated\nCalibration Index (ICI) and Related Metrics for Quantifying the\nCalibration of Logistic Regression Models.” Statistics in\nMedicine 38 (21): 4051–65. https://doi.org/10.1002/sim.8281.\n\n\nBecker, Barry, and Ronny Kohavi. 1996.\n“Adult.” UCI Machine Learning Repository.\n\n\nBrier, Glenn W. 1950. “Verification of Forecasts Expressed in\nTerms of Probability.” Monthly Weather Review 78 (1):\n1–3.\n\n\nCaffo, Brian S, James G Booth, and AC Davison. 2002. “Empirical\nSupremum Rejection Sampling.” Biometrika 89 (4): 745–54.\n\n\nCandanedo, Luis. 2016. “Occupancy Detection .”\nUCI Machine Learning Repository.\n\n\nChen, Song Xi. 1999. “Beta Kernel Estimators for Density\nFunctions.” Computational Statistics & Data Analysis\n31 (2): 131–45.\n\n\nChouldechova, Alexandra, and Trevor Hastie. 2015. “Generalized\nAdditive Model Selection.” https://arxiv.org/abs/1506.03850.\n\n\nCortez, Paulo, A. Cerdeira, F. Almeida, T. Matos, and J. Reis. 2009.\n“Wine Quality.” UCI Machine Learning\nRepository.\n\n\n“Dry Bean.” 2020. UCI Machine Learning\nRepository.\n\n\nHopkins, Mark, Erik Reeber, George Forman, and Jaap Suermondt. 1999.\n“Spambase.” UCI Machine Learning Repository.\n\n\n“In-Vehicle Coupon Recommendation.” 2020. UCI\nMachine Learning Repository.\n\n\nKullback, S., and R. A. Leibler. 1951. “On Information and\nSufficiency.” The Annals of Mathematical Statistics 22\n(1): 79–86. https://doi.org/10.1214/aoms/1177729694.\n\n\nMoro, S., P. Rita, and P. Cortez. 2012. “Bank\nMarketing.” UCI Machine Learning Repository.\n\n\n“Mushroom.” 1987. UCI Machine Learning\nRepository.\n\n\nNash, Warwick, Tracy Sellers, Simon Talbot, Andrew Cawthorn, and Wes\nFord. 1995. “Abalone.” UCI Machine Learning\nRepository.\n\n\nOjeda, Francisco M., Max L. Jansen, Alexandre Thiéry, Stefan\nBlankenberg, Christian Weimar, Matthias Schmid, and Andreas Ziegler.\n2023. “Calibrating Machine Learning Approaches for Probability\nEstimation: A Comprehensive Comparison.” Statistics in\nMedicine 42 (29): 5451–78. https://doi.org/10.1002/sim.9921.\n\n\nRumbell, Timothy, Jaimit Parikh, James Kozloski, and Viatcheslav Gurev.\n2023. “Novel and Flexible Parameter Estimation Methods for\nData-Consistent Inversion in Mechanistic Modelling.” Royal\nSociety Open Science 10 (11): 230668.\n\n\nYeh, I-Cheng. 2016. “Default of Credit Card\nClients.” UCI Machine Learning Repository.",
    "crumbs": [
      "References"
    ]
  }
]